{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLOps Events Data Cleaning & Deduplication\n",
        "\n",
        "This notebook performs comprehensive data cleaning on the MLOps events CSV dataset. It identifies and removes duplicates based on multiple strategies (talk title, YouTube links, full row), normalizes text fields, extracts canonical YouTube IDs, and produces a clean dataset ready for enrichment and database ingestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Duplicate Analysis\n",
        "\n",
        "### Find Key-Based Duplicates\n",
        "\n",
        "Identify duplicate groups based on the composite key: Talk Title + Full Name + YouTube Link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfV5pdgwZAAT",
        "outputId": "6bd1eea6-b92e-4e62-8504-87cb54363790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Duplicate group: [Talk Title='AI Tools Under Control: Keeping Your Agents Secure and Reliable', Full Name='Bar Chen', YouTube Link='https://youtu.be/poqhv4hPTpA?si=wDHbtOf6DGPVJfvl']\n",
            "Row indices: [61, 413] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Agentic AI: Learning Iteratively, Acting Autonomously', Full Name='Fatma Tarlaci', YouTube Link='https://youtu.be/_k8sPizcqUg?si=nQFRnoSd3voCuvSC']\n",
            "Row indices: [49, 401] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Build with Mistral', Full Name='Sophia  Yang', YouTube Link='https://youtu.be/_IM53bMowlQ?si=TkQ_sbpgcShA9mOD']\n",
            "Row indices: [59, 411] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Building AI Infrastructure for the GenAI Wave', Full Name='Shreya Rajpal', YouTube Link='https://youtu.be/Se9_38V2TPA?si=TSU-oQAASiQ5eAG3']\n",
            "Row indices: [65, 417] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Building Agentic and Multi-Agent Systems with LangGraph (Pt. 2)', Full Name='Greg Loughnane, Chris Alexiuk', YouTube Link='https://youtu.be/uPuoysjaCbw?si=T7bL3rRqQRKc8e2z']\n",
            "Row indices: [51, 403] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Building a Multimodal RAG: A Step-by-Step Guide for AI/ML practitioners', Full Name='Ivan Nardini, Holt Skinner', YouTube Link='https://youtu.be/CPpY4w4m5n0?si=5pPIF8UsKFP5z2P5']\n",
            "Row indices: [53, 405] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Driving GenAI Success in Production: Proven Approaches for Data Quality, Context, and Logging', Full Name='Alison Cossette', YouTube Link='https://youtu.be/FrpAgeJNgbM?si=Zvoem7cYhwIZ_5aT']\n",
            "Row indices: [45, 397] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Era of Multimodal AI & Reasoning', Full Name='Nikita Namjoshi', YouTube Link='https://youtu.be/EmAVZvsopYw?si=B-iLWUqqljdET11T']\n",
            "Row indices: [62, 414] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Evaluating LLM-Judge Evaluations: Best Practices', Full Name='Aishwarya Naresh Reganti', YouTube Link='https://youtu.be/ruwz_OA4XTo?si=mdXg67ss3XL_6UmC']\n",
            "Row indices: [44, 396] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Evolving with AI: Insights from Nyla's Generative AI Journey', Full Name='Nadia Rauch', YouTube Link='https://youtu.be/ktatsqlAjHs?si=Mh62qHSocjw_nnGY']\n",
            "Row indices: [56, 408] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Fast and Reproducible: Taming AI/ML dependencies', Full Name='Savin Goyal', YouTube Link='https://youtu.be/GOHfDHe9dCQ?si=zImiK5EqYSjmP99s']\n",
            "Row indices: [58, 410] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Finding the hidden drivers of AI business value', Full Name='Jakob Frick', YouTube Link='https://youtu.be/JlyYJnFWR7k?si=10fF9lMekeqfP7oi']\n",
            "Row indices: [69, 421] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='GenAI ROI: From Pilot to Profit', Full Name='Ilyas Iyoob', YouTube Link='https://youtu.be/CqlXBAc4oEM?si=8L7ACfJSvGWJqICh']\n",
            "Row indices: [52, 404] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='How to Build your Own LLM User Feedback Loop with Nebuly.', Full Name='Zunair Waseem', YouTube Link='https://youtu.be/OHJtZg9Owns?si=qC5pbEzBMBL4wCeX']\n",
            "Row indices: [72, 424] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Large Language Model Training and Serving at LinkedIn', Full Name='Dre Olgiati', YouTube Link='https://youtu.be/yx_BKcAPoQs?si=QBBhjnkXX8DftCbR']\n",
            "Row indices: [47, 399] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='MLOps for AgenticAI: How to manage Agents in production', Full Name='Eero Laaksonen', YouTube Link='https://youtu.be/Glurm_ADDpU?si=AoIgWcr4kXtyu4xt']\n",
            "Row indices: [48, 400] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Mastering Enterprise-Grade LLM Deployment: Overcoming Production Challenges', Full Name='Jaeman An', YouTube Link='https://youtu.be/dRgwzUk1s-g?si=C5N63MRY6_nHRT4E']\n",
            "Row indices: [64, 416] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Multimodal Agents You Can Deploy Anywhere', Full Name='Michael Thriffiley', YouTube Link='https://youtu.be/hU2mQTKYA80?si=3oa79-tLpkZrlUoG']\n",
            "Row indices: [71, 423] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Multimodal LLMs for product taxonomy at Shopify', Full Name='Kshetrajna Raghavan', YouTube Link='https://youtu.be/DNPLu3qNUN8?si=FkjCPhF2fRflZe9U']\n",
            "Row indices: [55, 407] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Optimized AI Deployment Platform', Full Name='Vasilis Vagias', YouTube Link='https://youtu.be/pNgwVCrpUJU?si=vuLXvV6AT3_twZa-']\n",
            "Row indices: [68, 420] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Optimizing AI/ML Workflows on Kubernetes: Advanced Techniques and Integration', Full Name='Anu Reddy', YouTube Link='https://youtu.be/grCvM9tkS7Q?si=YKB8ojQVTZB5GpWG']\n",
            "Row indices: [46, 398] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Optimizing LLM apps through usage: Implicit feedback, given explicitly', Full Name='Chinar Movsisyan', YouTube Link='https://youtu.be/M7B02XA_COQ?si=14M24AeCcy6Y1fR4']\n",
            "Row indices: [70, 422] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Revolutionizing Cloud Storage: From Petabytes to Intelligence', Full Name='Vinit Dhatrak', YouTube Link='https://youtu.be/cEcGoJrTptA?si=TQ1Pku3I48H9nUXV']\n",
            "Row indices: [60, 412] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Scale Expert Review by 10x to Ship AI Apps at Lightning Speed', Full Name='Niklas Nielsen', YouTube Link='https://youtu.be/Hg_kuDL5Sfs?si=0aVv8TKg-bh-U7ac']\n",
            "Row indices: [67, 419] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Supercharge ML Teams: ZenML's Real World Impact in the MLOps Jungle', Full Name='Adam Probst', YouTube Link='https://youtu.be/J_ItDeB12gY?si=RVjIkdOAxlY1crFB']\n",
            "Row indices: [66, 418] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='The State-of-the-Art in Software Development Agents', Full Name='Graham Neubig', YouTube Link='https://youtu.be/klrhZCxXzB0?si=q16Aitbg9GtboeGk']\n",
            "Row indices: [50, 402] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Toyota's Generative AI Journey', Full Name='Ravi Chandu Ummadisetti, Stephen Ellis, Kordel France, Eric Swei', YouTube Link='https://youtu.be/-zV51vf-u3o?si=yH3UJBaeE_yPmwsI']\n",
            "Row indices: [57, 409] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='Unleashing the Algorithm Genie: AI as the Ultimate Inventor', Full Name='Jepson Taylor', YouTube Link='https://youtu.be/8pZa7Wq53OY?si=mNX5HhbXrJzmP-4A']\n",
            "Row indices: [54, 406] (Total: 2)\n",
            "\n",
            "Duplicate group: [Talk Title='We’re Doing RAG All Wrong—and How We Can Do So Much Better', Full Name='Simba Khadder', YouTube Link='https://youtu.be/DbMv1qhisgQ?si=iaQQKlAN_S2wpBAZ']\n",
            "Row indices: [63, 415] (Total: 2)\n",
            "\n",
            "Number of duplicate groups: 29\n",
            "Total rows involved in duplicates: 58\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "file_path = \"/content/mlops-events.csv\"  # adjust if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Find duplicate groups\n",
        "duplicate_groups = (\n",
        "    df.groupby([\"Talk Title\", \"Full Name\", \"YouTube Link\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "\n",
        "# Keep only duplicates\n",
        "duplicate_groups = duplicate_groups[duplicate_groups[\"Count\"] > 1]\n",
        "\n",
        "# For each duplicate group, get the row indices\n",
        "for _, row in duplicate_groups.iterrows():\n",
        "    talk, name, yt, count = row[\"Talk Title\"], row[\"Full Name\"], row[\"YouTube Link\"], row[\"Count\"]\n",
        "    indices = df[\n",
        "        (df[\"Talk Title\"] == talk) &\n",
        "        (df[\"Full Name\"] == name) &\n",
        "        (df[\"YouTube Link\"] == yt)\n",
        "    ].index.tolist()\n",
        "\n",
        "    print(f\"\\nDuplicate group: [Talk Title='{talk}', Full Name='{name}', YouTube Link='{yt}']\")\n",
        "    print(f\"Row indices: {indices} (Total: {count})\")\n",
        "\n",
        "# Optionally, show summary at the end\n",
        "print(\"\\nNumber of duplicate groups:\", len(duplicate_groups))\n",
        "print(\"Total rows involved in duplicates:\", duplicate_groups[\"Count\"].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Row Duplicates\n",
        "\n",
        "Check for complete duplicates across all columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxlNPXwLZWE5",
        "outputId": "7f3e3b73-49e6-4c2a-e1e6-0e8ec171e231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Full Name, dtype: object)\n",
            "Values: Full Name\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Company Name, dtype: object)\n",
            "Values: Company Name\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Job Title, dtype: object)\n",
            "Values: Job Title\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Talk Title, dtype: object)\n",
            "Values: Talk Title\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Abstract, dtype: object)\n",
            "Values: Abstract\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: What You'll Learn, dtype: object)\n",
            "Values: What You'll Learn\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Prerequiste Knowledge (if required), dtype: object)\n",
            "Values: Prerequiste Knowledge (if required)\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Track, dtype: object)\n",
            "Values: Track\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Technical Level (1-7), dtype: float64)\n",
            "Values: Technical Level (1-7)\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Category 1, dtype: object)\n",
            "Values: Category 1\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Top 3 keywords (in order), dtype: object)\n",
            "Values: Top 3 keywords (in order)\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Bio, dtype: object)\n",
            "Values: Bio\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: YouTube Link, dtype: object)\n",
            "Values: YouTube Link\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Relevant Industries, dtype: object)\n",
            "Values: Relevant Industries\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: What is Unique about your session, dtype: object)\n",
            "Values: What is Unique about your session\n",
            "\n",
            "Duplicate group (all columns identical):\n",
            "Row indices: Series([], Name: Event, dtype: object)\n",
            "Values: Event\n",
            "\n",
            "Number of full-duplicate groups: 0\n",
            "Total rows involved in full duplicates: 103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-913191186.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  grouped = duplicate_rows.groupby(list(df.columns)).apply(lambda x: x.index.tolist())\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "file_path = \"/content/mlops-events.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Find full duplicates (across all columns)\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Group them by all column values\n",
        "grouped = duplicate_rows.groupby(list(df.columns)).apply(lambda x: x.index.tolist())\n",
        "\n",
        "# Print each group with row indices\n",
        "for values, indices in grouped.items():\n",
        "    print(f\"\\nDuplicate group (all columns identical):\")\n",
        "    print(f\"Row indices: {indices}\")\n",
        "    print(f\"Values: {values}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\nNumber of full-duplicate groups:\", len(grouped))\n",
        "print(\"Total rows involved in full duplicates:\", len(duplicate_rows))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect Differing Fields\n",
        "\n",
        "For duplicate groups, identify which fields have different values to understand data inconsistencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8DRa4gqZwyY",
        "outputId": "228d3917-fd19-43a8-d288-6bd8c5d5f15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Duplicate group: [Talk Title='AI Tools Under Control: Keeping Your Agents Secure and Reliable', Full Name='Bar Chen', YouTube Link='https://youtu.be/poqhv4hPTpA?si=wDHbtOf6DGPVJfvl']\n",
            "Row indices: [61, 413] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Agentic AI: Learning Iteratively, Acting Autonomously', Full Name='Fatma Tarlaci', YouTube Link='https://youtu.be/_k8sPizcqUg?si=nQFRnoSd3voCuvSC']\n",
            "Row indices: [49, 401] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Build with Mistral', Full Name='Sophia  Yang', YouTube Link='https://youtu.be/_IM53bMowlQ?si=TkQ_sbpgcShA9mOD']\n",
            "Row indices: [59, 411] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Building AI Infrastructure for the GenAI Wave', Full Name='Shreya Rajpal', YouTube Link='https://youtu.be/Se9_38V2TPA?si=TSU-oQAASiQ5eAG3']\n",
            "Row indices: [65, 417] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Building Agentic and Multi-Agent Systems with LangGraph (Pt. 2)', Full Name='Greg Loughnane, Chris Alexiuk', YouTube Link='https://youtu.be/uPuoysjaCbw?si=T7bL3rRqQRKc8e2z']\n",
            "Row indices: [51, 403] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Building a Multimodal RAG: A Step-by-Step Guide for AI/ML practitioners', Full Name='Ivan Nardini, Holt Skinner', YouTube Link='https://youtu.be/CPpY4w4m5n0?si=5pPIF8UsKFP5z2P5']\n",
            "Row indices: [53, 405] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Driving GenAI Success in Production: Proven Approaches for Data Quality, Context, and Logging', Full Name='Alison Cossette', YouTube Link='https://youtu.be/FrpAgeJNgbM?si=Zvoem7cYhwIZ_5aT']\n",
            "Row indices: [45, 397] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Era of Multimodal AI & Reasoning', Full Name='Nikita Namjoshi', YouTube Link='https://youtu.be/EmAVZvsopYw?si=B-iLWUqqljdET11T']\n",
            "Row indices: [62, 414] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Evaluating LLM-Judge Evaluations: Best Practices', Full Name='Aishwarya Naresh Reganti', YouTube Link='https://youtu.be/ruwz_OA4XTo?si=mdXg67ss3XL_6UmC']\n",
            "Row indices: [44, 396] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Evolving with AI: Insights from Nyla's Generative AI Journey', Full Name='Nadia Rauch', YouTube Link='https://youtu.be/ktatsqlAjHs?si=Mh62qHSocjw_nnGY']\n",
            "Row indices: [56, 408] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Fast and Reproducible: Taming AI/ML dependencies', Full Name='Savin Goyal', YouTube Link='https://youtu.be/GOHfDHe9dCQ?si=zImiK5EqYSjmP99s']\n",
            "Row indices: [58, 410] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Finding the hidden drivers of AI business value', Full Name='Jakob Frick', YouTube Link='https://youtu.be/JlyYJnFWR7k?si=10fF9lMekeqfP7oi']\n",
            "Row indices: [69, 421] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='GenAI ROI: From Pilot to Profit', Full Name='Ilyas Iyoob', YouTube Link='https://youtu.be/CqlXBAc4oEM?si=8L7ACfJSvGWJqICh']\n",
            "Row indices: [52, 404] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='How to Build your Own LLM User Feedback Loop with Nebuly.', Full Name='Zunair Waseem', YouTube Link='https://youtu.be/OHJtZg9Owns?si=qC5pbEzBMBL4wCeX']\n",
            "Row indices: [72, 424] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Large Language Model Training and Serving at LinkedIn', Full Name='Dre Olgiati', YouTube Link='https://youtu.be/yx_BKcAPoQs?si=QBBhjnkXX8DftCbR']\n",
            "Row indices: [47, 399] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='MLOps for AgenticAI: How to manage Agents in production', Full Name='Eero Laaksonen', YouTube Link='https://youtu.be/Glurm_ADDpU?si=AoIgWcr4kXtyu4xt']\n",
            "Row indices: [48, 400] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Mastering Enterprise-Grade LLM Deployment: Overcoming Production Challenges', Full Name='Jaeman An', YouTube Link='https://youtu.be/dRgwzUk1s-g?si=C5N63MRY6_nHRT4E']\n",
            "Row indices: [64, 416] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Multimodal Agents You Can Deploy Anywhere', Full Name='Michael Thriffiley', YouTube Link='https://youtu.be/hU2mQTKYA80?si=3oa79-tLpkZrlUoG']\n",
            "Row indices: [71, 423] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Multimodal LLMs for product taxonomy at Shopify', Full Name='Kshetrajna Raghavan', YouTube Link='https://youtu.be/DNPLu3qNUN8?si=FkjCPhF2fRflZe9U']\n",
            "Row indices: [55, 407] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Optimized AI Deployment Platform', Full Name='Vasilis Vagias', YouTube Link='https://youtu.be/pNgwVCrpUJU?si=vuLXvV6AT3_twZa-']\n",
            "Row indices: [68, 420] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Optimizing AI/ML Workflows on Kubernetes: Advanced Techniques and Integration', Full Name='Anu Reddy', YouTube Link='https://youtu.be/grCvM9tkS7Q?si=YKB8ojQVTZB5GpWG']\n",
            "Row indices: [46, 398] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Optimizing LLM apps through usage: Implicit feedback, given explicitly', Full Name='Chinar Movsisyan', YouTube Link='https://youtu.be/M7B02XA_COQ?si=14M24AeCcy6Y1fR4']\n",
            "Row indices: [70, 422] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Revolutionizing Cloud Storage: From Petabytes to Intelligence', Full Name='Vinit Dhatrak', YouTube Link='https://youtu.be/cEcGoJrTptA?si=TQ1Pku3I48H9nUXV']\n",
            "Row indices: [60, 412] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Scale Expert Review by 10x to Ship AI Apps at Lightning Speed', Full Name='Niklas Nielsen', YouTube Link='https://youtu.be/Hg_kuDL5Sfs?si=0aVv8TKg-bh-U7ac']\n",
            "Row indices: [67, 419] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Supercharge ML Teams: ZenML's Real World Impact in the MLOps Jungle', Full Name='Adam Probst', YouTube Link='https://youtu.be/J_ItDeB12gY?si=RVjIkdOAxlY1crFB']\n",
            "Row indices: [66, 418] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='The State-of-the-Art in Software Development Agents', Full Name='Graham Neubig', YouTube Link='https://youtu.be/klrhZCxXzB0?si=q16Aitbg9GtboeGk']\n",
            "Row indices: [50, 402] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Toyota's Generative AI Journey', Full Name='Ravi Chandu Ummadisetti, Stephen Ellis, Kordel France, Eric Swei', YouTube Link='https://youtu.be/-zV51vf-u3o?si=yH3UJBaeE_yPmwsI']\n",
            "Row indices: [57, 409] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='Unleashing the Algorithm Genie: AI as the Ultimate Inventor', Full Name='Jepson Taylor', YouTube Link='https://youtu.be/8pZa7Wq53OY?si=mNX5HhbXrJzmP-4A']\n",
            "Row indices: [54, 406] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n",
            "\n",
            "Duplicate group: [Talk Title='We’re Doing RAG All Wrong—and How We Can Do So Much Better', Full Name='Simba Khadder', YouTube Link='https://youtu.be/DbMv1qhisgQ?si=iaQQKlAN_S2wpBAZ']\n",
            "Row indices: [63, 415] (Total: 2)\n",
            "  No differing fields — rows are exact duplicates.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "file_path = \"/content/mlops-events.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Find duplicate groups\n",
        "duplicate_groups = (\n",
        "    df.groupby([\"Talk Title\", \"Full Name\", \"YouTube Link\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "duplicate_groups = duplicate_groups[duplicate_groups[\"Count\"] > 1]\n",
        "\n",
        "# Inspect duplicates for differences\n",
        "for _, row in duplicate_groups.iterrows():\n",
        "    talk, name, yt = row[\"Talk Title\"], row[\"Full Name\"], row[\"YouTube Link\"]\n",
        "    dup_rows = df[\n",
        "        (df[\"Talk Title\"] == talk) &\n",
        "        (df[\"Full Name\"] == name) &\n",
        "        (df[\"YouTube Link\"] == yt)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\nDuplicate group: [Talk Title='{talk}', Full Name='{name}', YouTube Link='{yt}']\")\n",
        "    print(f\"Row indices: {dup_rows.index.tolist()} (Total: {len(dup_rows)})\")\n",
        "\n",
        "    # Compare values column by column\n",
        "    differences = {}\n",
        "    for col in df.columns:\n",
        "        if len(dup_rows[col].unique()) > 1:  # if column has more than one unique value in this group\n",
        "            differences[col] = dup_rows[col].tolist()\n",
        "\n",
        "    if differences:\n",
        "        print(\"Fields that differ:\")\n",
        "        for col, vals in differences.items():\n",
        "            print(f\"  {col}: {vals}\")\n",
        "    else:\n",
        "        print(\"  No differing fields — rows are exact duplicates.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Normalization & Diagnosis\n",
        "\n",
        "### Normalize Text for Fair Comparison\n",
        "\n",
        "Create cleaned views of data with normalized whitespace, Unicode characters (NFKC), and hidden characters exposed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRJYhrHsZ-0u",
        "outputId": "43e0c67c-5d10-42c1-9034-ce143d877d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact duplicate rows (raw): 103\n",
            "Exact duplicate rows (cleaned): 103\n",
            "Duplicate key groups (same trio): 30\n",
            "Total rows in key-duplicate groups: 113\n",
            "Key-duplicate groups that are full-row identical (cleaned view): 29\n",
            "Key-duplicate groups with some differing fields: 1\n",
            "\n",
            "=== Group ===\n",
            "Talk Title: AI Tools Under Control: Keeping Your Agents Secure and Reliable\n",
            "Full Name:  Bar Chen\n",
            "YouTube:    https://youtu.be/poqhv4hPTpA?si=wDHbtOf6DGPVJfvl\n",
            "Row indices (raw):   [61, 413]\n",
            "Row indices (clean): [61, 413]\n",
            "No differing fields in cleaned view (full-row identical after normalization).\n",
            "\n",
            "=== Group ===\n",
            "Talk Title: Agentic AI: Learning Iteratively, Acting Autonomously\n",
            "Full Name:  Fatma Tarlaci\n",
            "YouTube:    https://youtu.be/_k8sPizcqUg?si=nQFRnoSd3voCuvSC\n",
            "Row indices (raw):   [49, 401]\n",
            "Row indices (clean): [49, 401]\n",
            "No differing fields in cleaned view (full-row identical after normalization).\n",
            "\n",
            "=== Group ===\n",
            "Talk Title: Build with Mistral\n",
            "Full Name:  Sophia Yang\n",
            "YouTube:    https://youtu.be/_IM53bMowlQ?si=TkQ_sbpgcShA9mOD\n",
            "Row indices (raw):   []\n",
            "Row indices (clean): [59, 411]\n",
            "No differing fields in cleaned view (full-row identical after normalization).\n",
            "\n",
            "=== Group ===\n",
            "Talk Title: nan\n",
            "Full Name:  nan\n",
            "YouTube:    nan\n",
            "Row indices (raw):   []\n",
            "Row indices (clean): [76, 78, 79, 81, 83, 86, 89, 96, 99, 102, 106, 108, 110, 112, 116, 119, 121, 123, 133, 135, 139, 142, 144, 147, 150, 157, 160, 162, 185, 196, 199, 207, 210, 213, 215, 220, 223, 228, 230, 248, 249, 269, 275, 279, 293, 296, 300, 302, 306, 309, 313, 314, 316, 389, 391]\n",
            "Differing fields (cleaned view):\n",
            "  - Company Name: [\"'MLOps & GenAI World 2023'\", \"'I will talk through some specific war stories from applying these techniques at Twitter and Abnormal Security'\", \"'It is important to test beyond accuracy in your NLP system. This is because the business requirements for the system include robustness, reliability, fairness, toxicity, efficiency, lack of bias, lack of data leakage, and safety. Therefore, your test suites should reflect these requirements. A comprehensive review of definitions and metrics for these terms in different contexts is provided in the Holistic Evaluation of Language Models [Liang et. al 2022], which is well worth reading. However, you will need to write your own tests to determine what inclusiveness means for your specific application.'\", \"'nan'\", \"'It is based on real use cases that are in production at Chick-fil-A today. It will also provide insight into how the machine learning engineering team at Chick-fil-A approaches horizontal scaling, and how we are enabling data scientists to easily scale their exiting solutions without needing to worry about the details of Apache Spark, or other distributed computing platforms.'\", \"'This can’t be found online, because it is specific to Ford’s journey in building its AI/ML ecosystem.'\", '\"We will be able to go much deeper and properly cover the tools, including emergent ones (e.g., LangSmith), in a way that we weren\\'t able to in just 1 hour: https://www.youtube.com/live/Azfc-TjG9Tg?feature=share\"', \"'This provides an inside look at how an ML practitioner is addressing this problem in a large-scale, multi-tenant environment within a regulated industry like finance.'\", \"'A lot of talks are on what systems have been built, or how to build a software product. Very little content exists on how to start a platform and build one out over time.'\", \"'TMLS 2022'\"]\n",
            "  - Job Title: [\"'nan'\", \"'MLOps & GenAI World 2023'\", \"'TMLS 2023'\", \"'TMLS 2024'\"]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === Load ===\n",
        "file_path = \"/content/mlops-events.csv\"  # adjust as needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# === Create two views ===\n",
        "# raw_df: untouched; clean_df: normalized strings for fair comparison\n",
        "raw_df = df.copy()\n",
        "\n",
        "clean_df = df.copy()\n",
        "for c in clean_df.select_dtypes(include=[\"object\"]).columns:\n",
        "    # normalize strings: strip, collapse internal whitespace, normalize case for urls/titles/names as desired\n",
        "    clean_df[c] = clean_df[c].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "# === 1) Whole-row duplicates (exact) ===\n",
        "whole_row_dupe_mask_raw   = raw_df.duplicated(keep=False)\n",
        "whole_row_dupe_mask_clean = clean_df.duplicated(keep=False)\n",
        "\n",
        "print(\"Exact duplicate rows (raw):\", int(whole_row_dupe_mask_raw.sum()))\n",
        "print(\"Exact duplicate rows (cleaned):\", int(whole_row_dupe_mask_clean.sum()))\n",
        "\n",
        "# === 2) Key-duplicates on the trio ===\n",
        "KEY = [\"Talk Title\", \"Full Name\", \"YouTube Link\"]\n",
        "missing_keys = [k for k in KEY if k not in df.columns]\n",
        "if missing_keys:\n",
        "    raise ValueError(f\"Missing expected columns: {missing_keys}\")\n",
        "\n",
        "key_dupe_counts = clean_df.groupby(KEY).size().reset_index(name=\"Count\")\n",
        "key_dupe_groups = key_dupe_counts[key_dupe_counts[\"Count\"] > 1]\n",
        "\n",
        "print(\"Duplicate key groups (same trio):\", len(key_dupe_groups))\n",
        "print(\"Total rows in key-duplicate groups:\", int(key_dupe_groups[\"Count\"].sum()))\n",
        "\n",
        "# === 3) For each key-duplicate group, check if rows are fully identical (across ALL columns) ===\n",
        "def row_signature(row):\n",
        "    # tuple of all columns in order; NaN-safe\n",
        "    return tuple((None if pd.isna(v) else v) for v in row.tolist())\n",
        "\n",
        "fully_identical_groups = []\n",
        "partly_different_groups = []\n",
        "\n",
        "for _, grp_key in key_dupe_groups[KEY].iterrows():\n",
        "    mask = (clean_df[KEY] == grp_key.values).all(axis=1)\n",
        "    grp = clean_df.loc[mask]\n",
        "\n",
        "    # Build signatures across ALL columns for each row\n",
        "    sigs = grp.apply(row_signature, axis=1)\n",
        "    unique_sigs = sigs.drop_duplicates()\n",
        "\n",
        "    if len(unique_sigs) == 1:\n",
        "        fully_identical_groups.append(tuple(grp_key.values))\n",
        "    else:\n",
        "        partly_different_groups.append(tuple(grp_key.values))\n",
        "\n",
        "print(\"Key-duplicate groups that are full-row identical (cleaned view):\", len(fully_identical_groups))\n",
        "print(\"Key-duplicate groups with some differing fields:\", len(partly_different_groups))\n",
        "\n",
        "# === 4) Show diagnostics for differences (including hidden chars) ===\n",
        "def show_hidden(s):\n",
        "    if pd.isna(s):\n",
        "        return \"<NaN>\"\n",
        "    return repr(str(s))  # exposes escaped whitespace like '\\xa0', '\\n', etc.\n",
        "\n",
        "def explain_group(triple, max_rows=10):\n",
        "    talk, name, yt = triple\n",
        "    mask_raw   = (raw_df[\"Talk Title\"] == talk) & (raw_df[\"Full Name\"] == name) & (raw_df[\"YouTube Link\"] == yt)\n",
        "    mask_clean = (clean_df[\"Talk Title\"] == talk) & (clean_df[\"Full Name\"] == name) & (clean_df[\"YouTube Link\"] == yt)\n",
        "\n",
        "    raw_rows   = raw_df.loc[mask_raw]\n",
        "    clean_rows = clean_df.loc[mask_clean]\n",
        "\n",
        "    print(\"\\n=== Group ===\")\n",
        "    print(f\"Talk Title: {talk}\")\n",
        "    print(f\"Full Name:  {name}\")\n",
        "    print(f\"YouTube:    {yt}\")\n",
        "    print(\"Row indices (raw):  \", raw_rows.index.tolist())\n",
        "    print(\"Row indices (clean):\", clean_rows.index.tolist())\n",
        "\n",
        "    # Column-by-column unique values (cleaned)\n",
        "    diffs = {}\n",
        "    for c in clean_df.columns:\n",
        "        vals = clean_rows[c]\n",
        "        uniq = pd.unique(vals)\n",
        "        if len(uniq) > 1:\n",
        "            diffs[c] = [show_hidden(x) for x in uniq[:max_rows]]\n",
        "\n",
        "    if diffs:\n",
        "        print(\"Differing fields (cleaned view):\")\n",
        "        for c, vals in diffs.items():\n",
        "            print(f\"  - {c}: {vals}\")\n",
        "    else:\n",
        "        print(\"No differing fields in cleaned view (full-row identical after normalization).\")\n",
        "\n",
        "# Example: print details for a few groups (both categories)\n",
        "for triple in fully_identical_groups[:3]:\n",
        "    explain_group(triple)\n",
        "\n",
        "for triple in partly_different_groups[:3]:\n",
        "    explain_group(triple)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deterministic Deduplication\n",
        "\n",
        "### Build Canonical Unique Keys\n",
        "\n",
        "Create stable unique keys using talk title + speaker name + YouTube ID, then implement scoring-based deduplication (prefer rows with more data, YouTube ID present, longer abstracts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKBMeFkbagpg",
        "outputId": "a80d5a9f-d903-41db-8d03-10bfe3060650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate groups by key: 31\n",
            "Showing up to 5 clashing groups (with differing fields):\n",
            "\n",
            "key=0b5bab7e69fc0f337894 | size=2 | row_indices=[57, 409]\n",
            "  No differing fields (fully identical rows).\n",
            "--------------------------------------------------------------------------------\n",
            "key=0f8ca83c95553d53a797 | size=2 | row_indices=[68, 420]\n",
            "  No differing fields (fully identical rows).\n",
            "--------------------------------------------------------------------------------\n",
            "key=1785ea18584c46433ddb | size=2 | row_indices=[53, 405]\n",
            "  No differing fields (fully identical rows).\n",
            "--------------------------------------------------------------------------------\n",
            "key=18d41ecec98bfca7de3a | size=2 | row_indices=[49, 401]\n",
            "  No differing fields (fully identical rows).\n",
            "--------------------------------------------------------------------------------\n",
            "key=18db96b2224bd2f868c1 | size=2 | row_indices=[61, 413]\n",
            "  No differing fields (fully identical rows).\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original rows: 425\n",
            "Unique rows after dedupe: 341 (removed 84)\n",
            "Built 341 objects.\n",
            "Wrote: /content/mlops-events.cleaned.csv and /content/mlops-events.objects.json\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready cell for your schema (uses \"YouTube Link\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from hashlib import blake2b\n",
        "import json\n",
        "\n",
        "# ===== 0) Load CSV =====\n",
        "CSV_PATH = \"/content/mlops-events.csv\"  # change if needed\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "expected_cols = [\n",
        "    \"Full Name\", \"Company Name\", \"Job Title\", \"Talk Title\", \"Abstract\",\n",
        "    \"What You'll Learn\", \"Prerequiste Knowledge (if required)\", \"Track\",\n",
        "    \"Technical Level (1-7)\", \"Category 1\", \"Top 3 keywords (in order)\",\n",
        "    \"Bio\", \"YouTube Link\", \"Relevant Industries\",\n",
        "    \"What is Unique about your session\", \"Event\"\n",
        "]\n",
        "missing = [c for c in expected_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "# ===== 1) Normalization helpers =====\n",
        "def norm_text(s, lower=True):\n",
        "    \"\"\"Unicode NFKC, strip, collapse whitespace; optional lowercase.\"\"\"\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s.lower() if lower else s\n",
        "\n",
        "def extract_youtube_id(url):\n",
        "    \"\"\"Extract a YouTube video ID from common URL formats.\"\"\"\n",
        "    if pd.isna(url) or str(url).strip() == \"\":\n",
        "        return None\n",
        "    url = unicodedata.normalize(\"NFKC\", str(url)).strip()\n",
        "\n",
        "    # youtu.be/<id>\n",
        "    m = re.match(r\"^https?://(?:www\\.)?youtu\\.be/([A-Za-z0-9_-]{6,})\", url)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "\n",
        "    # youtube.com variants\n",
        "    try:\n",
        "        p = urlparse(url)\n",
        "        if \"youtube.com\" in p.netloc or \"m.youtube.com\" in p.netloc:\n",
        "            q = parse_qs(p.query)\n",
        "            if \"v\" in q and len(q[\"v\"]) > 0:\n",
        "                return q[\"v\"][0]\n",
        "            # /shorts/<id>\n",
        "            m = re.match(r\"^/shorts/([A-Za-z0-9_-]{6,})\", p.path)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "            # /embed/<id>\n",
        "            m = re.match(r\"^/embed/([A-Za-z0-9_-]{6,})\", p.path)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: guess an 11-char token\n",
        "    m = re.search(r\"([A-Za-z0-9_-]{11})(?:\\b|$)\", url)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# ===== 2) Canonical fields for uniqueness =====\n",
        "df[\"_talk_norm\"] = df[\"Talk Title\"].apply(lambda x: norm_text(x, lower=True))\n",
        "df[\"_name_norm\"] = df[\"Full Name\"].apply(lambda x: norm_text(x, lower=True))\n",
        "df[\"_yt_id\"]     = df[\"YouTube Link\"].apply(extract_youtube_id)\n",
        "\n",
        "# ===== 3) Stable unique key (talk|name|ytid) =====\n",
        "def make_key(talk, name, ytid):\n",
        "    triple = f\"{talk or ''}||{name or ''}||{ytid or ''}\"\n",
        "    return blake2b(triple.encode(\"utf-8\"), digest_size=10).hexdigest()\n",
        "\n",
        "df[\"_uniq_key\"] = [make_key(t, n, y) for t, n, y in zip(df[\"_talk_norm\"], df[\"_name_norm\"], df[\"_yt_id\"])]\n",
        "\n",
        "# ===== 4) Diagnose what’s “wrong” (clashing groups + differing fields) =====\n",
        "key_counts = df.groupby(\"_uniq_key\").size().rename(\"Count\")\n",
        "clashing_keys = key_counts[key_counts > 1].index.tolist()\n",
        "\n",
        "def differing_fields(group):\n",
        "    \"\"\"Columns whose values differ within given group.\"\"\"\n",
        "    diffs = {}\n",
        "    for c in df.columns:\n",
        "        vals = group[c]\n",
        "        uniq = {repr(x) for x in vals}  # repr exposes hidden whitespace/characters\n",
        "        if len(uniq) > 1:\n",
        "            diffs[c] = list(uniq)\n",
        "    return diffs\n",
        "\n",
        "print(f\"Duplicate groups by key: {len(clashing_keys)}\")\n",
        "if clashing_keys:\n",
        "    print(\"Showing up to 5 clashing groups (with differing fields):\\n\")\n",
        "    for k in clashing_keys[:5]:\n",
        "        grp = df[df[\"_uniq_key\"] == k]\n",
        "        diffs = differing_fields(grp)\n",
        "        print(f\"key={k} | size={len(grp)} | row_indices={grp.index.tolist()}\")\n",
        "        if diffs:\n",
        "            print(\"  Differing fields:\")\n",
        "            for col, reps in diffs.items():\n",
        "                # truncate long reps to keep console readable\n",
        "                preview = [r if len(r) < 160 else r[:157] + \"...'\" for r in reps]\n",
        "                print(f\"   - {col}: {preview}\")\n",
        "        else:\n",
        "            print(\"  No differing fields (fully identical rows).\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "# ===== 5) Deterministic dedupe policy =====\n",
        "# Prefer rows with: (a) most non-null fields, (b) longest Abstract, (c) YouTube ID present, (d) first seen\n",
        "non_null_counts = df.notna().sum(axis=1)\n",
        "has_ytid = df[\"_yt_id\"].notna().astype(int)\n",
        "abstract_len = df[\"Abstract\"].fillna(\"\").str.len()\n",
        "\n",
        "# Higher score is better\n",
        "df[\"_score\"] = (\n",
        "    non_null_counts * 1.0\n",
        "    + abstract_len * 1e-6         # tiny tie-breaker\n",
        "    + has_ytid * 0.1              # prefer rows that resolve to a YT id\n",
        ")\n",
        "\n",
        "# Keep the highest-score row per key (break ties by earliest index)\n",
        "dedup = (\n",
        "    df.sort_values([\"_uniq_key\", \"_score\", df.index.name if df.index.name else \"Full Name\"], ascending=[True, False, True])\n",
        "      .groupby(\"_uniq_key\", as_index=False)\n",
        "      .head(1)\n",
        "      .copy()\n",
        ")\n",
        "\n",
        "print(f\"\\nOriginal rows: {len(df)}\")\n",
        "print(f\"Unique rows after dedupe: {len(dedup)} (removed {len(df) - len(dedup)})\")\n",
        "\n",
        "# ===== 6) Build your per-row objects =====\n",
        "def build_object(row):\n",
        "    return {\n",
        "        \"full_name\": row[\"Full Name\"],\n",
        "        \"company_name\": row[\"Company Name\"],\n",
        "        \"job_title\": row[\"Job Title\"],\n",
        "        \"talk_title\": row[\"Talk Title\"],\n",
        "        \"abstract\": row[\"Abstract\"],\n",
        "        \"what_youll_learn\": row[\"What You'll Learn\"],\n",
        "        \"prerequisite_knowledge\": row[\"Prerequiste Knowledge (if required)\"],\n",
        "        \"track\": row[\"Track\"],\n",
        "        \"technical_level\": row[\"Technical Level (1-7)\"],\n",
        "        \"category_1\": row[\"Category 1\"],\n",
        "        \"top_3_keywords\": row[\"Top 3 keywords (in order)\"],\n",
        "        \"bio\": row[\"Bio\"],\n",
        "        \"youtube_link\": row[\"YouTube Link\"],\n",
        "        \"youtube_id\": row[\"_yt_id\"],\n",
        "        \"relevant_industries\": row[\"Relevant Industries\"],\n",
        "        \"unique_session\": row[\"What is Unique about your session\"],\n",
        "        \"event\": row[\"Event\"],\n",
        "        \"unique_key\": row[\"_uniq_key\"],\n",
        "    }\n",
        "\n",
        "objects = [build_object(r) for _, r in dedup.iterrows()]\n",
        "print(f\"Built {len(objects)} objects.\")\n",
        "\n",
        "# ===== 7) (Optional) Save cleaned CSV and JSON =====\n",
        "OUT_CSV  = \"/content/mlops-events.cleaned.csv\"\n",
        "OUT_JSON = \"/content/mlops-events.objects.json\"\n",
        "\n",
        "# Drop helper columns before writing cleaned CSV\n",
        "helper_cols = [c for c in dedup.columns if c.startswith(\"_\")]\n",
        "dedup_clean = dedup.drop(columns=helper_cols)\n",
        "dedup_clean.to_csv(OUT_CSV, index=False)\n",
        "\n",
        "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(objects, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Wrote:\", OUT_CSV, \"and\", OUT_JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Checks\n",
        "\n",
        "### Count Unique Values\n",
        "\n",
        "Verify uniqueness across key fields after initial deduplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMKgiiVGboTD",
        "outputId": "9c258262-9e33-429f-8b0a-a96059e70777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Full Names   : 311\n",
            "Unique Talk Titles  : 340\n",
            "Unique YouTube Links: 280\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned file you wrote in the previous step\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "# Count uniques\n",
        "unique_full_names   = df[\"Full Name\"].nunique()\n",
        "unique_talk_titles  = df[\"Talk Title\"].nunique()\n",
        "unique_youtube_urls = df[\"YouTube Link\"].nunique()\n",
        "\n",
        "print(\"Unique Full Names   :\", unique_full_names)\n",
        "print(\"Unique Talk Titles  :\", unique_talk_titles)\n",
        "print(\"Unique YouTube Links:\", unique_youtube_urls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove Talk Title Duplicates\n",
        "\n",
        "Drop duplicate talk titles, keeping only the first occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTs1-XDEcHdY",
        "outputId": "01fe6257-0ebb-4675-dbe6-5908f101e774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows before: 341\n",
            "Rows after : 341\n",
            "Unique Talk Titles: 340\n",
            "Overwritten file saved at /content/mlops-events.cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned file\n",
        "path = \"/content/mlops-events.cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Drop duplicates by Talk Title, keeping the first occurrence\n",
        "df_dedup = df.drop_duplicates(subset=[\"Talk Title\"], keep=\"first\")\n",
        "\n",
        "print(\"Rows before:\", len(df))\n",
        "print(\"Rows after :\", len(df_dedup))\n",
        "print(\"Unique Talk Titles:\", df_dedup[\"Talk Title\"].nunique())\n",
        "\n",
        "# Overwrite the same file\n",
        "df_dedup.to_csv(path, index=False)\n",
        "print(f\"Overwritten file saved at {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show remaining duplicate talk titles (if any) for manual review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTk-FLs5cYnR",
        "outputId": "7cbc324a-e36f-44ef-aa38-40437dc07b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Full Name, Talk Title, YouTube Link]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Find titles that occur more than once\n",
        "dupe_titles = df[\"Talk Title\"][df[\"Talk Title\"].duplicated(keep=False)]\n",
        "\n",
        "# Show all rows with those titles\n",
        "dupe_rows = df[df[\"Talk Title\"].isin(dupe_titles)]\n",
        "print(dupe_rows[[\"Full Name\", \"Talk Title\", \"YouTube Link\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## YouTube URL Normalization\n",
        "\n",
        "### Identify YouTube Link Duplicates\n",
        "\n",
        "Find duplicate YouTube URLs before normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZYdsp0gcisp",
        "outputId": "46ee39e9-4adb-4ba4-ae22-2053e148c878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate YouTube URLs: 0\n",
            "Total rows involved in duplicates: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned file\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "# Find duplicates in YouTube Link column\n",
        "duplicate_urls = (\n",
        "    df.groupby(\"YouTube Link\")\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        "    .query(\"Count > 1\")\n",
        ")\n",
        "\n",
        "print(\"Number of duplicate YouTube URLs:\", len(duplicate_urls))\n",
        "print(\"Total rows involved in duplicates:\", duplicate_urls[\"Count\"].sum())\n",
        "\n",
        "# Show details for each duplicate group\n",
        "for _, row in duplicate_urls.iterrows():\n",
        "    url = row[\"YouTube Link\"]\n",
        "    count = row[\"Count\"]\n",
        "    indices = df[df[\"YouTube Link\"] == url].index.tolist()\n",
        "    print(f\"\\nYouTube URL: {url}\")\n",
        "    print(f\"Appears {count} times\")\n",
        "    print(f\"Row indices: {indices}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check potential duplicates after basic normalization (lowercase, strip whitespace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvdpKpdafFh-",
        "outputId": "e38a1a79-59ab-4d58-bfac-0d4410cd5c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 341\n",
            "Unique YouTube Links: 280\n",
            "Potential duplicates after normalization: 61\n",
            "Empty DataFrame\n",
            "Columns: [YouTube Link, Full Name, Talk Title]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Check how many rows vs uniques again\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Unique YouTube Links:\", df[\"YouTube Link\"].nunique())\n",
        "\n",
        "# Show potential duplicates (ignoring case and spaces)\n",
        "df[\"YouTube_norm\"] = df[\"YouTube Link\"].str.strip().str.lower()\n",
        "\n",
        "dupes = df[df.duplicated(subset=[\"YouTube_norm\"], keep=False)]\n",
        "print(\"Potential duplicates after normalization:\", len(dupes))\n",
        "\n",
        "# Group and show\n",
        "dupes_grouped = dupes.groupby(\"YouTube_norm\").agg({\n",
        "    \"YouTube Link\": list,\n",
        "    \"Full Name\": list,\n",
        "    \"Talk Title\": list\n",
        "})\n",
        "print(dupes_grouped.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract & Canonicalize YouTube IDs\n",
        "\n",
        "Robust extraction of YouTube video IDs from various URL formats (youtu.be, youtube.com, shorts, embed). Create canonical links in standard format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk1Z4cSTfW78",
        "outputId": "39d5cf0f-4d7d-4d91-db05-0e3f83b61143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique YouTube Links (BEFORE): 280\n",
            "Unique YouTube Links (AFTER): 280\n",
            "Duplicate YouTube URLs (AFTER normalization): 0\n",
            "Overwritten: /content/mlops-events.cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re, unicodedata\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# --- 1) Load ---\n",
        "path = \"/content/mlops-events.cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# --- 2) Robust extractor for YouTube video IDs ---\n",
        "def extract_youtube_id(url):\n",
        "    if pd.isna(url) or str(url).strip() == \"\":\n",
        "        return None\n",
        "    url = unicodedata.normalize(\"NFKC\", str(url)).strip()\n",
        "\n",
        "    # youtu.be/<id>\n",
        "    m = re.match(r\"^https?://(?:www\\.)?youtu\\.be/([A-Za-z0-9_-]{6,})\", url)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "\n",
        "    # youtube.com variants\n",
        "    try:\n",
        "        p = urlparse(url)\n",
        "        host = (p.netloc or \"\").lower()\n",
        "        if \"youtube.com\" in host or \"m.youtube.com\" in host:\n",
        "            q = parse_qs(p.query)\n",
        "            if \"v\" in q and len(q[\"v\"]) > 0:\n",
        "                return q[\"v\"][0]\n",
        "            # /shorts/<id>\n",
        "            m = re.match(r\"^/shorts/([A-Za-z0-9_-]{6,})\", p.path)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "            # /embed/<id>\n",
        "            m = re.match(r\"^/embed/([A-Za-z0-9_-]{6,})\", p.path)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: guess an 11-char token\n",
        "    m = re.search(r\"([A-Za-z0-9_-]{11})(?:\\b|$)\", url)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# --- 3) BEFORE stats ---\n",
        "before_unique = df[\"YouTube Link\"].nunique()\n",
        "print(\"Unique YouTube Links (BEFORE):\", before_unique)\n",
        "\n",
        "# --- 4) Compute IDs and canonicalize the link column ---\n",
        "yt_ids = df[\"YouTube Link\"].apply(extract_youtube_id)\n",
        "df[\"YouTube ID\"] = yt_ids\n",
        "\n",
        "# Build canonical link only when we have an ID; otherwise keep original\n",
        "def canonical_link(vid, raw):\n",
        "    if pd.isna(vid) or not vid:\n",
        "        return raw  # preserve whatever was there if we can't parse\n",
        "    return f\"https://www.youtube.com/watch?v={vid}\"\n",
        "\n",
        "df[\"YouTube Link\"] = [canonical_link(v, r) for v, r in zip(df[\"YouTube ID\"], df[\"YouTube Link\"])]\n",
        "\n",
        "# --- 5) AFTER stats + quick sanity check on duplicates ---\n",
        "after_unique = df[\"YouTube Link\"].nunique()\n",
        "print(\"Unique YouTube Links (AFTER):\", after_unique)\n",
        "\n",
        "# Show duplicate URLs (should now truly group together)\n",
        "dupe_urls = (\n",
        "    df.groupby(\"YouTube Link\")\n",
        "      .size()\n",
        "      .reset_index(name=\"Count\")\n",
        "      .query(\"Count > 1\")\n",
        "      .sort_values(\"Count\", ascending=False)\n",
        ")\n",
        "print(\"Duplicate YouTube URLs (AFTER normalization):\", len(dupe_urls))\n",
        "if not dupe_urls.empty:\n",
        "    for _, row in dupe_urls.head(10).iterrows():  # print up to 10 groups for brevity\n",
        "        url = row[\"YouTube Link\"]\n",
        "        idxs = df.index[df[\"YouTube Link\"] == url].tolist()\n",
        "        print(f\"{url} -> {row['Count']} rows | indices: {idxs}\")\n",
        "\n",
        "# --- 6) Overwrite the cleaned file ---\n",
        "df.to_csv(path, index=False)\n",
        "print(f\"Overwritten: {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify duplicate URLs after normalization (should be properly grouped now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6px_o-_3frsM",
        "outputId": "9d27400c-e86e-4884-ba7d-9360f0efc58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate YouTube URLs: 0\n",
            "Total rows involved in duplicates: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned file\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "# Find duplicates in YouTube Link column\n",
        "duplicate_urls = (\n",
        "    df.groupby(\"YouTube Link\")\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        "    .query(\"Count > 1\")\n",
        ")\n",
        "\n",
        "print(\"Number of duplicate YouTube URLs:\", len(duplicate_urls))\n",
        "print(\"Total rows involved in duplicates:\", duplicate_urls[\"Count\"].sum())\n",
        "\n",
        "# Show details for each duplicate group\n",
        "for _, row in duplicate_urls.iterrows():\n",
        "    url = row[\"YouTube Link\"]\n",
        "    count = row[\"Count\"]\n",
        "    indices = df[df[\"YouTube Link\"] == url].index.tolist()\n",
        "    print(f\"\\nYouTube URL: {url}\")\n",
        "    print(f\"Appears {count} times\")\n",
        "    print(f\"Row indices: {indices}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Statistics\n",
        "\n",
        "### Comprehensive Link Analysis\n",
        "\n",
        "Calculate total rows, null/non-null YouTube links, unique counts, and identify remaining duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ir0VMbZf4GZ",
        "outputId": "068d3f5c-d811-4a10-d1a0-7c3a981bcc39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 341\n",
            "Non-null YouTube Links: 280\n",
            "Null (missing) YouTube Links: 61\n",
            "Unique YouTube Links (non-null only): 280\n",
            "Unique YouTube Links (counting NaN as a category): 281\n",
            "\n",
            "Duplicate non-null YouTube URLs: 0\n",
            "\n",
            "Groups with Count>1 when treating NaN as '<NA>':\n",
            "  _YouTube_Link_with_NA  Count\n",
            "0                  <NA>     61\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "total_rows = len(df)\n",
        "non_null    = df[\"YouTube Link\"].notna().sum()\n",
        "nulls       = df[\"YouTube Link\"].isna().sum()\n",
        "uniques     = df[\"YouTube Link\"].nunique()                # dropna=True (default)\n",
        "uniques_all = df[\"YouTube Link\"].nunique(dropna=False)    # counts NaN as a distinct value\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"Non-null YouTube Links: {non_null}\")\n",
        "print(f\"Null (missing) YouTube Links: {nulls}\")\n",
        "print(f\"Unique YouTube Links (non-null only): {uniques}\")\n",
        "print(f\"Unique YouTube Links (counting NaN as a category): {uniques_all}\")\n",
        "\n",
        "# Real duplicate URLs among NON-NULL values\n",
        "dupe_urls = (\n",
        "    df[df[\"YouTube Link\"].notna()]\n",
        "      .groupby(\"YouTube Link\")\n",
        "      .size()\n",
        "      .reset_index(name=\"Count\")\n",
        "      .query(\"Count > 1\")\n",
        "      .sort_values(\"Count\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nDuplicate non-null YouTube URLs:\", len(dupe_urls))\n",
        "if not dupe_urls.empty:\n",
        "    for _, row in dupe_urls.iterrows():\n",
        "        url = row[\"YouTube Link\"]\n",
        "        idxs = df.index[df[\"YouTube Link\"] == url].tolist()\n",
        "        print(f\"{url} -> {row['Count']} rows | indices: {idxs}\")\n",
        "\n",
        "# OPTIONAL: if you want to also consider missing links as a single \"duplicate group\"\n",
        "# (this is unusual, but sometimes useful), fill NaN with a sentinel and group:\n",
        "df[\"_YouTube_Link_with_NA\"] = df[\"YouTube Link\"].fillna(\"<NA>\")\n",
        "dupe_including_na = (\n",
        "    df.groupby(\"_YouTube_Link_with_NA\")\n",
        "      .size()\n",
        "      .reset_index(name=\"Count\")\n",
        "      .query(\"Count > 1\")\n",
        "      .sort_values(\"Count\", ascending=False)\n",
        ")\n",
        "print(\"\\nGroups with Count>1 when treating NaN as '<NA>':\")\n",
        "print(dupe_including_na.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove Missing YouTube Links\n",
        "\n",
        "Drop rows where YouTube Link is missing or empty to ensure all records have valid video data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND7MGxQUgX_F",
        "outputId": "631771a1-39b6-4e3d-f243-4b8f9e48ea92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows before: 341\n",
            "Rows after : 280\n",
            "Unique YouTube Links: 280\n",
            "File overwritten at /content/mlops-events.cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to your cleaned file\n",
        "path = \"/content/mlops-events.cleaned.csv\"\n",
        "\n",
        "# Load the file\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Drop rows where YouTube Link is missing/empty\n",
        "df_no_empty = df.dropna(subset=[\"YouTube Link\"])\n",
        "\n",
        "print(\"Rows before:\", len(df))\n",
        "print(\"Rows after :\", len(df_no_empty))\n",
        "print(\"Unique YouTube Links:\", df_no_empty[\"YouTube Link\"].nunique())\n",
        "\n",
        "# Overwrite the cleaned file\n",
        "df_no_empty.to_csv(path, index=False)\n",
        "print(f\"File overwritten at {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Validation\n",
        "\n",
        "### Unique Talk Titles Count\n",
        "\n",
        "Confirm final count of unique talk titles in cleaned dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0VYxw-BgyOq",
        "outputId": "2a011801-2acd-47c6-e797-2c0fb3a81143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Talk Titles: 280\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated cleaned file\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "# Count unique talk titles\n",
        "unique_talk_titles = df[\"Talk Title\"].nunique()\n",
        "\n",
        "print(\"Unique Talk Titles:\", unique_talk_titles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unique Speakers Count\n",
        "\n",
        "Confirm final count of unique speakers in cleaned dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT3eILjng7on",
        "outputId": "5a41ceef-d745-49e3-c6dd-4024886bfe99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Full Names: 263\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated cleaned file\n",
        "df = pd.read_csv(\"/content/mlops-events.cleaned.csv\")\n",
        "\n",
        "# Count unique full names\n",
        "unique_full_names = df[\"Full Name\"].nunique()\n",
        "\n",
        "print(\"Unique Full Names:\", unique_full_names)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
