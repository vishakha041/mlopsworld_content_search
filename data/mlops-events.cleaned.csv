Full Name,Company Name,Job Title,Talk Title,Abstract,What You'll Learn,Prerequiste Knowledge (if required),Track,Technical Level (1-7),Category 1,Top 3 keywords (in order),Bio,YouTube Link,Relevant Industries,What is Unique about your session,Event,YouTube ID
"Jakub Witkowski, Dariusz Adamczyk",Roche,Roche Informatics - IT Expert,From ML Repository to ML Production Pipeline,"In the pRED MLOps team, we collaborate closely with research scientists to transition their machine learning models into a production environment seamlessly. Through our efforts, we have developed a robust framework that standardises and scales this process effectively. In this talk, we will provide an in-depth look at our framework, the tools we leverage, and the challenges we overcome in this journey."," - How to create framework for moving ML code to production
- What can be automated in this process (role of containerisation, CI/CD, building reusable components for repeating tasks)
- What tools are important for dev team
- What are most important challenges to tackle in this process",,Virtual Talk,4.0,Deployment and integration,"ML production pipeline, Automation, Framework","Jakub Witkowski, PhD is a data scientist and MLOps engineer with experience spanning various industries, including consulting, media, and pharmaceuticals. At Roche, he focuses on understanding the needs of data scientists to help them make their work and models production-ready. He achieves this by providing comprehensive frameworks and upskilling opportunities.

Dariusz is a DevOps and MLOps engineer. He has experience in various industries such as public cloud computing, telecommunications, and pharmaceuticals. At Roche, he focuses on infrastructure and the process of deploying machine learning models into production.",https://www.youtube.com/watch?v=fqzqDK-ZXcQ,,We will share the experience of MLOps team in Roche on how to cooperate with data scientists to successfully deploy their machine learning models to production.,MLOps & GenAI World 2024,fqzqDK-ZXcQ
David Rosenberg,Bloomberg,"Head of ML Strategy, Office of the CTO",BloombergGPT: How we built a 50 billion parameter financial language model,"We will present BloombergGPT, a 50 billion parameter language model, purpose-built for finance and trained on a uniquely balanced mix of standard general-purpose datasets and a diverse array of financial documents from the Bloomberg archives. Building a large language model (LLM) is a costly and time-intensive endeavor. To reduce risk, we adhered closely to model designs and training strategies from recent successful models, such as OPT and BLOOM. Nevertheless, we faced numerous challenges during the training process, including loss spikes, unexpected parameter drifts, and performance plateaus.

In this talk, we will discuss these hurdles and our responses, which included a complete training restart after weeks of effort. Our persistence paid off: BloombergGPT ultimately outperformed existing models on financial tasks by significant margins, while maintaining competitive performance on general LLM benchmarks. We will also provide several examples illustrating how BloombergGPT stands apart from general-purpose models.

Our goal is to provide valuable insights into the specific challenges encountered when building LLMs and to offer guidance for those debating whether to embark on their own LLM journey, as well as for those who are already determined to do so.","You'll learn about the challenges that are often faced when designing and training LLMs, as well as some approaches to address these challenges. You'll also see how domain-specific datasets can benefit LLMs.",,Case Study,5.0,,,"David Rosenberg leads the Machine Learning Strategy team in the Office of the CTO at Bloomberg. He is also an adjunct associate professor at the Center for Data Science at New York University, where he has repeatedly received NYU’s Center for Data Science “Professor of the Year” award. Previously, he was Chief Scientist at Sense Networks, a location data analytics and mobile advertising company, and served as scientific adviser to Discovereads, a book recommendation company first acquired by Goodreads and later Amazon. He received his Ph.D. in statistics from UC Berkeley, where he worked on statistical learning theory and natural language processing. David received a Master of Science in applied mathematics, with a focus on computer science, from Harvard University, and a Bachelor of Science in mathematics from Yale University. He is currently based in Toronto.",https://www.youtube.com/watch?v=m2Scj2SO85Y,"Banking & Financial Services
","I was part of the team that built BloombergGPT and co-authored the paper
",TMLS 2023,m2Scj2SO85Y
Intae Rhyoo,VESSL AI,Co-founder & CPO,"LLMs, from Playgrounds to Production-ready Pipelines","Despite the onset of commercially viable open-source Large Langauge Models, companies are struggling to leverage cutting-edge models like Llama2 and Mistral 7B for production-ready applications. Creating a simple demo page on a personal laptop and training, fine-tuning, and serving multi-billion parameter LLMs on HPC-scale infrastructure - with proprietary enterprise data - involves an entirely different engineering challenge. In this session, Intae, who co-founded and now leads product development at VESSL AI, will explore how companies can leverage MLOps infrastructure to go from a simple model playground to deploying enterprise-scale pipelines for LLMs.",Building CI/CD pipelines for LLMs,,Workshop,5.0,Deployment and integration,"LLMs, MLOps infrastructure, Enterprise-scale pipelines
","As the Chief Product Officer at VESSL AI, Intae leverages his expertise in the intersection of software engineering and machine learning to help customers scale their workloads on VESSL AI. Being the founding ML engineer at VESSL AI, he oversees product development, managing areas from developer experiences to Kubernetes-backends. Before co-founding VESSL AI, he was a research assistant at the Personal Autonomous Robotics Lab (PeARL) at UT Austin and later joined high-growth AI startups as an ML engineer.",https://www.youtube.com/watch?v=HWXPBJAhNlY,,"This talk provides a unique opportunity to see behind the scenes how MLOps strategies are implemented in real-world business settings. By giving concrete, varied examples across different industries, I aim to illustrate the versatility and impact of MLOps. What sets this talk apart is our commitment to providing insights into these transformative strategies and making them accessible and actionable for executive business leaders. My goal is to demystify MLOps, empowering attendees to envision its practical potential within their organizations.
",MLOps & GenAI World 2023,HWXPBJAhNlY
Nathan Beach,Google,Product Manager,Leverage Kubernetes To Optimize the Utilization of Your AI Accelerators,"When training or serving AI models, the cost of AI accelerators such as GPUs can dwarf the cost of all other compute resources combined.

This session provides techniques to leverage Kubernetes to analyze the current utilization of AI accelerators and provides numerous tactics to optimize AI accelerator utilization for training, inference, notebooks, and other AI workloads.",,,Research or Advanced Technical,3.0,Performance optimization and efficiency,"Kubernetes, AI accelerators, Optimization","Nathan Beach is a Product Manager at Google, working on Google Kubernetes Engine. He received his MBA from Harvard Business School and, prior to Google, led his own startup. He is a builder and creator passionate about making products that superbly meet user needs. He enjoys career coaching and mentoring, and he is eager to help others transition into product management and excel in their careers.",https://www.youtube.com/watch?v=5jdZksHaJ_Q,,,MLOps & GenAI World 2024,5jdZksHaJ_Q
Maxime Labonne,Liquid AI,Senior Staff Machine Learning Scientist,Everything You Need to Know about Fine-Tuning," Fine-tuning LLMs is a fundamental technique for companies to customize models for their specific needs. In this talk, we will introduce fine-tuning and best practices associated with it. We'll explore how to create a high-quality data generation pipeline, discuss fine-tuning techniques using popular libraries, explain how model merging works, and present the best ways to evaluate LLMs."," Best practices for fine-tuning, creating a high-quality data generation pipeline, fine-tuning techniques, best fine-tuning libraries, how to do model merging, and evaluation methods for fine-tuned models.",,Applied Case Studies,5.0,"Model dev, training, arch.","Fine-tuning, Data generation pipeline, Model evaluation","Maxime Labonne is a Senior Staff Machine Learning Scientist at Liquid AI, serving as the head of post-training. He holds a Ph.D. in Machine Learning from the Polytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML.

An active blogger, he has made significant contributions to the open-source community, including the LLM Course on GitHub, tools such as LLM AutoEval, and several state-of-the-art models like NeuralBeagle and Phixtral. He is the author of the best-selling book “Hands-On Graph Neural Networks Using Python,” published by Packt.
",https://www.youtube.com/watch?v=ijQ_8awMFBk,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Hospital & Health Care, Information Technology Services, Insurance, Marketing & Advertising, Telecommunications","Lots of experience releasing open-source models, datasets, tools, and articles, and working in this industry for years, tips are battle-tested and come from real-world experience.",MLOps & GenAI World 2024,ijQ_8awMFBk
Eric Hart,Anheuser-Busch,Staff Data Scientist,Optimal Beer Pricing: An Optimization Layer for Price Elasticities,"At Anheuser-Busch, we’re obsessed with price elasticities. When the price of beer changes, how will that affect the volume of beer that we sell? These questions (yes, this is more than one question) have implications all over the business, from price setting to procurement to financial planning. We’ve worked hard to make sure our answers to these questions are as data driven as possible. But once we have a model to produce (and predict) these elasticities, how do we make business decisions based on that? And how do we make sure those business decisions are also as data driven as possible? 

In this talk we’ll discuss an optimal pricing layer for beer elasticities. We’ll cover how to use mathematical optimization to make specific price change suggestions at a variety of granularities to help achieve specific business objectives. We’ll consider what objective we actually want to optimize (Profit? Revenue? Market Share?) and see how to use constraints to help smooth the trade-off between these objectives. Finally, we’ll investigate how to ensure our price suggestions stay within the regions where the underlying elasticities models make sense.

Ever wanted to see a real-world example of levelling up your analytics from predictive- to prescriptive-, and do so in the context of price setting (or beer drinking)? Now’s your chance!
",How to add an optimization layer to ml models.,,Case Study,2.0,Business and stakeholder alignment,,"Eric is a Staff Data Scientist with more than 7 years of experience working at Altair Engineering and Anheuser-Busch. He has a PhD in probability from the University of Toronto, and a masters degree in Applied Math and an undergraduate degree in Engineering from Queen's university. He's also a world champion Blokus player.",https://www.youtube.com/watch?v=5J8g-EHbDSc,,"I would argue the whole topic is fairly unique (optimization layers for predictive models are not widely used or discussed). In addition, the specifics of trying to work around the realities of the beer industry (especially varying laws about beer pricing across different geographies) add an extra layer of complexity to this already deep problem.
",TMLS 2022,5J8g-EHbDSc
Avin Regmi,Engineering Manager ML,Spotify,Compute Strategies for Generative AI,"Distributing computing efforts strategically to maximize resource utilization and minimize wastage. Investing in Spotify's Hendrix ML Platform, which streamlines AI training and serving processes for models with over 70 billion parameters. ",,,,,Performance optimization and efficiency,"Compute Strategies for Generative AI, Resource Utilization, Hendrix ML Platform
","Avin is an Engineering Manager at Spotify, leading the ML training and compute team for the Hendrix ML Platform. His areas of expertise include training and serving ML models at scale, ML infrastructure, and growing high-performing teams. The Hendrix ML Platform is now integral to Spotify's core functions, such as search, ranking, and recommendations. Prior to joining Spotify, Avin led the ML Platform team at Bell. In this role, he focused on distributed training and serving LLMs. Additionally, Avin is the founder of Panini AI, which is a cloud solution that serves ML models at low latency using adaptive distributed batching. Outside of work, Avin practices yoga and meditation and enjoys high-altitude alpine climbing and hiking.",https://www.youtube.com/watch?v=0OEcXIL-piE,"Information Technology & Service, Telecommunications, Computer Software
",,TMLS 2024,0OEcXIL-piE
Jineet Doshi,Intuit,Staff Data Scientist/AI Lead,Measuring the Minds of Machines: Evaluating Generative AI Systems,"Evaluating LLMs is essential in establishing trust before deploying them to production. Even post deployment, evaluation is essential to ensure LLM outputs meet expectations, making it a foundational part of LLMOps. However, evaluating LLMs remains an open problem. Unlike traditional machine learning models, LLMs can perform a wide variety of tasks such as writing poems, Q&A, summarization etc. This leads to the question how do you evaluate a system with such broad intelligence capabilities? This talk covers the various approaches for evaluating LLMs along with the pros and cons of each. It also covers evaluating LLMs for safety and security and the need to have a holistic approach for evaluating these very capable models."," The audience will learn why evaluating GenAI systems is fundamental yet it remains an open problem, a broad overview of different techniques for evaluating GenAI systems (including some state of the art ones) along with pros and cons of each, how other ML Practicioners are doing LLM evals and techniques for evaluating for safety and security",,case study & advanced technical,3.0,Performance optimization and efficiency,"LLM evaluation, Safety and security, Generative AI"," Jineet Doshi is an award winning AI Lead and Engineer with over 7 years of experience. He has a proven track record of leading successful AI projects and building machine learning models from design to production across various domains, which have impacted millions of customers and have significantly improved business metrics, leading to millions of dollars of impact. He is currently an AI Lead at Intuit where he is one of the architects of their Generative AI platform which was featured on Forbes and Wall Street.

Jineet has also delivered guest lectures at Stanford University and UCLA on Applied AI. He is on the Advisory Board of University of San Francisco's AI Program. He holds multiple patents in the field, has advised numerous AI startups and has also co chaired workshops at top AI conferences like KDD.",https://www.youtube.com/watch?v=wruiUGF9X_A,"Automotive, Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology Services, Insurance, Marketing & Advertising, Telecommunications","This talk is created based on learnings from developing Intuit Assist, Intuit's GenAI CoPilot deployed to 100 Million customers across the world, covering multiple product lines like TurboTax, Quickbooks, Credit Karma and MailChimp. It covers a good breadth of techniques including some state of the art ones. We also conducted a survey recently in the MLOps Community of 100+ ML Praciticioners on how they are doing LLM evals. This talk includes insights from that survey also. An older version of this talk was also delivered at Stanford University as part of their Building LLM Applications class",MLOps & GenAI World 2024,wruiUGF9X_A
Mandy Gu,Wealthsimple,Senior Software Development Manager,How GenAI is Being Used for Productivity at Wealthsimple,"At Wealthsimple, we leverage GenAI internally to improve operational efficiency and streamline monotonous tasks. Our GenAI stack is a blend of tools we developed in house and third party solutions.

Today, roughly half of the company utilizes these tools in their day to day work. These are the lessons we learned in adoption, user behaviour and how to effectively leverage these tools to improve productivity."," - Impact of GenAI on internal productivity 
- Strategies to drive adoption of GenAI tools (what worked, what didn't work)",,Business Strategy or Ethics,2.0,Business and stakeholder alignment,"GenAI, Internal Productivity, Adoption Strategies
","Mandy is a Senior Software Development Manager at Wealthsimple, where she leads Machine Learning & Data Engineering. These teams provide a simple and reliable platform to empower the rest of the company to iterate quickly on machine learning applications, GenAI tools and leverage data assets to make better decisions. Previously, Mandy worked in the NLP space and as a data scientist.",https://www.youtube.com/watch?v=h_Q1g_dZKeE,"Banking & Financial Services
",,TMLS 2024,h_Q1g_dZKeE
"Bhuvana Adur Kannan, Yoyo Yang",Voiceflow,"Lead - Agent Performance & ML Platform, Machine Learning Engineer",Building and Evaluating Prompts on Production Grade Datasets,"Constructing prompts per task can be challenging given the many unknowns of running them in production. In this talk, we'll cover how we created several production style datasets for two types of LLM tasks to productize prompt based features. We'll walk through the methodology, techniques and lessons learned from developing and shipping prompt based features. The techniques in this talk will be widely applicable, but focused on conversational AI.","How to approach dataset creation, iterate on prompts and measure success of releases in production.",,Applied Case Studies,6.0,"Model dev, training, arch.","Production Datasets, Prompt Engineering, Conversational AI
","Bhuvana heads the Conversational Agent performance and ML platform at Voiceflow, aiming to improve conversational agent performance for customers. She has prior experience working on enterprise data systems for major Canadian banks and financial institutions.

Yoyo is a Machine Learning Engineer at Voiceflow, a conversational AI company. She works on various facets of machine learning systems, from model training and prompt tuning to backend architecture and real-time inference. Yoyo has been working in ML and Data Science for the past five years. She is committed to transforming ideas into robust, scalable solutions and continually pushing the boundaries of what’s possible.",https://www.youtube.com/watch?v=eCvoogEV3uU,,,TMLS 2024,eCvoogEV3uU
Urmish Thakker,SambaNova Systems,Director of Machine Learning,Building state-of-the-art chat bot using open source models and composite systems,"Open source LLMs like LLAMA2 and BLOOM have enabled widespread development of enterprise LLM Applications. As the models adoption has matured over-time, we have seen a rise in LLMs specialized to solve narrow domains, tasks or modalities. By adopting such specialization, these models are able to outperform far larger proprietary or open models. For example, 7-70B llama experts like UniNER, TabLLAMA, NexusRaven, SambaCoder-nsql-llama2 etc can outperform GPT-4 on NER, Function Calling, Tabular data and Text2SQL tasks. Many more such examples exist and can be found in open source. However, one unique feature that larger proprietary models offer is a single end-point that takes an user query and provides a response. These responses can sometimes also include a chain of tasks that was solved to get to such a response. 

The question we try to answer in this research is whether we can build a composite LLM system using open source checkpoints that can effectively provide this same usability as a larger proprietary model. This includes taking a user request and mapping it to a single checkpoint or a group of checkpoints that can solve the request and serve the user. Our work indicates that such a composite is indeed possible. We show this by building a new state-of-the-art model based on various adaptations of the mistral 7b model. Using unique ensembling methods, this composite model outperforms Gemma-7B, Mixtral-8x7B, llama2-70B,, Qwen-72B, Falcon-180B and BLOOM-176B at an effective inference cost of <10B parameter model (https://sambanova.ai/blog/samba-coe-v01-composition-of-experts)",,,Advanced Technical/Research,4.0,"Model dev, training, arch.","Composite LLM systems, Open-source models, Ensembling methods","Urmish leads the LLM Team at SambaNova Systems. The LLM team at SambaNova focuses on understanding how to train and evaluate HHH aligned large language models, adapting LLMs to enterprise use-cases and HW-SW co-design of LLMs to enable efficient training and inference. Before SambaNova, he was in various engineering and research roles at Arm, AMD and Texas Instruments. He also helped drive the TinyML Performance Working Group in MLPerf, contributing to the development of key benchmarks for IoT ML. Urmish has 35+ publications and patents focussing on efficient deep learning and LLMs. His papers have been published at top ML and HW conferences like NeurIPS, ICLR and ISCA and has been an invited speaker at various top universities and industry academia summits. He completed his masters at the University of Wisconsin Madison and his bachelors from Birla Institute of Technology and Science.",https://www.youtube.com/watch?v=yYlBiRLBss0,"Information Technology Services, Banking & Financial Services, Computer Software, Hospital & Health Care",,MLOps & GenAI World 2024,yYlBiRLBss0
Emerson Taymor,InfoBeans,"SVP, Design",GenAI: A New Renaissance in Product Development,"Drawing inspiration from the Renaissance, a time of explosive cultural and economic growth, we explore how GenAI is poised to revolutionize product development. I’ll explore its groundbreaking impact on global collaboration, user research, and product development process. I will also highlight potential caveats and pitfalls. This talk promises an action-packed look at GenAI's role in shaping a new era of human-centric and economically impactful product development, ensuring you're equipped to experiment in this modern renaissance."," * Understand quickly why GenAI is going to have such a profound impact on the world
* Learn specific ways that you can leverage GenAI when working with global teams
* Gain tool kits for integrating GenAI in your user research process to help you go faster
* Discover new tools that can improve all stages of the product development process to maximize speed
* Understand specific caveats and pitfalls that should be avoided when thinking about GenAI in the product process",,Business Strategy or Ethics,1.0,Business and stakeholder alignment,"Generative AI, Product Development, Global Collaboration
","Emerson Taymor is a serial entrepreneur, investor, and currently the SVP of Design at InfoBeans as well as the creator of multiple hospitality brands in New York City. He co-founded the digital product studio, Philosophie, where he worked with enterprise executives and startup founders to launch over 300 digital products before it was acquired by InfoBeans in 2019.

In the digital world, Emerson works with leaders to help them unstick their big digital ideas through rapid experimentation and making.

In the physical world, he helped create one of the hottest speakeasy concepts in the East Village and one of the top rated cocktail bars in Brooklyn.

He loves weaving together these two worlds, exploring new parts of the world and being a sports fanatic.

https://linkedin.com/in/etaymor",https://www.youtube.com/watch?v=ddkjHTGJyHA,"Automotive, Banking & Financial Services, Computer Software, Food & Beverages, Information Technology & Service, Environmental Services, Hospital & Health Care, Insurance, Telecommunications, Marketing & Advertising, Other
",,TMLS 2024,ddkjHTGJyHA
Jose Nicholas Francisco,Deepgram,ML Developer Advocate,"LLMs, Big Data, and Audio: Breaching an Untapped Gold Mine","Large language models like those in the GPT and Llama series are primarily trained on massive amounts of *text* data. However, the vast majority of language and communication doesn't take place over text, but rather through voice. Cues in vocal tone carry information that the plaintext cannot convey—think about the last time you've witnessed or experienced a miscommunication over text/email/Slack. Thus, in this talk, I argue that training language models on audio data is the next step to improving them. Then, I'll propose a way of integrating audio data with text data in a larger dataset that can then be used for training various LLMs.",,,Ignite Lightning Talk,,"Model dev, training, arch.","Audio Data Integration, LLM Training, Communication Enhancement","Jose is a Developer Advocate at Deepgram, aiming to demystify the inner workings of AI. He has a background in software engineering, with projects focused on fraud detection and prevention. Jose earned a bachelor's and master's degree in computer science—with a specialization on AI and NLP—from Stanford University. He currently lives in San Francisco with his friends.",https://www.youtube.com/watch?v=5Uju4DP7XLY,,"Because audio data is such a neglected domain, there is very little online about its importance and impact. It's difficult to find a presentation that showcases the utility and diversity of applications fueled by speech-to-text models. This talk gathers information that would otherwise be hopelessly spread out throughout the internet and showcases them in a single space.
",MLOps & GenAI World 2023,5Uju4DP7XLY
Jeff Clune,University of British Columbia,"Professor, Computer Science, University of British Columbia; CIFAR AI Chair, Vector; Senior Research Advisor, DeepMind",Open-Ended and AI-Generating Algorithms in the Era of Foundation Models,"Open-Ended and AI-Generating Algorithms in the Era of Foundation Models

Foundation models (e.g. large language models) create exciting new opportunities in our longstanding quests to produce open-ended and AI-generating algorithms, wherein agents can truly keep innovating and learning forever. In this talk I will share some of our recent work harnessing the power of foundation models to make progress in these areas. I will cover our recent work on OMNI (Open-endedness via Models of human Notions of Interestingness), Video Pre-Training (VPT), Thought Cloning, Automatically Designing Agentic Systems, and The AI Scientist.",,,Virtual Workshop,3.0,Future trends,"Foundation models, Open-ended algorithms, Agentic systems","Jeff Clune is a Professor of computer science at the University of British Columbia, a Canada CIFAR AI Chair at the Vector Institute, and a Senior Research Advisor at DeepMind. Jeff focuses on deep learning, including deep reinforcement learning. Previously he was a research manager at OpenAI, a Senior Research Manager and founding member of Uber AI Labs (formed after Uber acquired a startup he helped lead), the Harris Associate Professor in Computer Science at the University of Wyoming, and a Research Scientist at Cornell University. He received degrees from Michigan State University (PhD, master’s) and the University of Michigan (bachelor’s). More on Jeff’s research can be found at JeffClune.com or on Twitter (@jeffclune). Since 2015, he won the Presidential Early Career Award for Scientists and Engineers from the White House, had two papers in Nature and one in PNAS, won an NSF CAREER award, received Outstanding Paper of the Decade and Distinguished Young Investigator awards, received two test of time awards, and had best paper awards, oral presentations, and invited talks at the top machine learning conferences (NeurIPS, CVPR, ICLR, and ICML). His research is regularly covered in the press, including the New York Times, NPR, the New Yorker, CNN, NBC, Wired, the BBC, the Economist, Science, Nature, National Geographic, the Atlantic, and the New Scientist.",https://www.youtube.com/watch?v=N8EqPWRp5cg,,,MLOps & GenAI World 2024,N8EqPWRp5cg
Nassim Tayari,IBM Canada,watsonx Canada Leader,"AI Governance: Accelerate Responsible, Transparent, and Explainable AI Workflows","The Hype around AI and the value it can offer and the concerns around how it can be implemented has reached a fever pitch in recent months. AI governance is not just a “nice to have” in today’s AI environment. It provides a level of organizational rigor and human oversight into how AI models are created and deployed. While it doesn’t replace the MLOps processes that organizations have, it complements them with activities intended to strike the appropriate balance between the benefits and risks of AI. The focus of this talk is on a comprehensive platform agnostic end to end solution for managing life cycles and risk for both traditional predictive machine learning and new generative AI models. watsonx.governance is an AI-driven, highly scalable governance, risk and compliance platform that can centralize siloed risk management functions within a single environment.","As part of this talk the audience would learn about importance of AI governance and how to ensure that these advanced technologies are used ethically, responsibly, and in a manner that benefits society as a whole. The learners will be introduced into a governance framework that outlines the principles, policies, and practices for governing the development, deployment, and use of generative artificial intelligence (AI) systems.",,Applied Case Studies,4.0,"Ethics, governance compliance ","AI Governance, Responsible AI, Generative AI
","Nassim Tayari is a distinguished technology leader with over 15 years of experience in Data science and engineering management. Currently, she serves as watsonx Canada leader at IBM, overseeing a large cross functional team of AI engineers and Solutions Architects assist Canadian clients in adopting trusted generative AI within their organizations.

Prior to this role, Nassim held various leadership positions at Borealis AI and Royal bank of Canada. Her background also includes hands-on experience as a Data scientist. Nassim is a visionary technologist with a passion for harnessing the power of bleeding-edge technology to revolutionize businesses.

Nassim is passionate about creating innovative and impactful solutions that leverage the power of Data and AI, prioritizing teamwork, diversity, excellence, and strive to make a positive difference in the world through her work.
Nassim Tayari holds PhD in the applications of Machine Learning in Medical Imaging and have multiple publications and certifications in the field.",https://www.youtube.com/watch?v=aQFUaSB5LME,,,TMLS 2024,aQFUaSB5LME
"Ravi Chandu Ummadisetti, Stephen Ellis, Kordel France, Eric Swei",Toyota,"Generative AI Architect, Technical Generative AI Product Manager, AI Architect, AI Architect",Toyota's Generative AI Journey,"Team Toyota will delve into their innovative journey with generative AI in automotive design, with the talk exploring how the Toyota research integrates traditional engineering constraints with state-of-the-art generative AI techniques, enhancing designers' capabilities while ensuring safety and performance considerations.","1. Toyota's Innovation Legacy
2. Leveraging LLMs in Automotive - battery, vehicle, manufacturing, etc
3. Failures in Generative AI projects
4. Education to business stakeholders",,case study & business strategy,2.0,"Model dev, training, arch.","Generative AI, Automotive design, Stakeholder education","Ravi Chandu Bio (Generative AI Architect): Ravi Chandu Ummadisetti is a distinguished Generative AI Architect with over a decade of experience, known for his pivotal role in advancing AI initiatives at Toyota Motor North America. His expertise in AI/ML methodologies has driven significant improvements across Toyota's operations, including a 75% reduction in production downtime and the development of secure, AI-powered applications. Ravi's work at Toyota, spanning manufacturing optimization, legal automation, and corporate AI solutions, showcases his ability to deliver impactful, data-driven strategies that enhance efficiency and drive innovation. His technical proficiency and leadership have earned him recognition as a key contributor to Toyota's AI success.

Stephen Ellis Bio (Technical Generative AI Product Manager): 10 years of experience in research strategy and the application of emerging technologies for companies as small as startups to Fortune 50 Enterprises. Former Director of the North Texas Blockchain Alliance where leading the cultivation of the Blockchain and Cryptocurrency competencies among software developers, C-level executives, and private investment advisors. Formerly the CTO of Plymouth Artificial Intelligence which was researching and developing future applications of AI. In this capacity advised companies on building platforms that seek to leverage emerging technologies for new business cases. Currently Technical Product Manager at Toyota Motors North America focused on enabling generative AI solutions for various group across the enterprise to drive transformation in developing new mobility solutions and enterprise operations.

Kordel France Bio (AI Architect): Kordel brings a diverse background of experiences in robotics and AI from both academia and industry. He has multiple patents in advanced sensor design and spent much of the past few years founding and building a successful sensor startup that enables the sense of smell for robotics. He is on the board of multiple startups and continues to further his AI knowledge as an AI Architect at Toyota.

Eric Swei Bio (Senior Generative AI Architect): Boasting an impressive career spanning over two decades, Eric Swei is an accomplished polymath in the tech arena, with deep-seated expertise as a full stack developer, system architect, integration architect, and specialist in computer vision, alongside his profound knowledge in generative AI, data science, IoT, and cognitive technologies.

At the forefront as the Generative AI Architect at Toyota, Eric leads a formidable team in harnessing the power of generative AI. Their innovative endeavors are not only enhancing Toyota's technological prowess but also redefining the future of automotive solutions with cutting-edge AI integration.",https://www.youtube.com/watch?v=-zV51vf-u3o,Automotive,"In this presentation, Stephen Ellis  will provide an overview of the Enterprise AI Team, detailing their role and initiatives within Toyota. Eric Swei will discuss Toyota's innovation history, highlighting the company's long-standing commitment to technological advancement. Ravi Chandu Ummadisetti will speak about the inception of Generative AI at Toyota, marking a significant milestone in the company's AI journey. Kordel France will cover the transition from Toyota Connected North America to Toyota Motors North America, emphasizing the strategic shifts and integration efforts.
 
Following these introductions, the panel will share lessons learned on vendor management and the evaluation of incoming generative AI proposals by technology partners, stressing the importance of aligning with Toyota's standards and expectations. Discussing the use of Large Language Models (LLMs) to judge, score, and quantify generative AI outputs, ensuring they meet Toyota's quality benchmarks in addition to lessons learned on collaborative requirement discovery and outcome defined capability selection to ensure appropriate application of resources to drive adoption and continued Enterprise leadership engagement.",MLOps & GenAI World 2024,-zV51vf-u3o
Guanghua Shu,Instacart,Staff Machine Learning Engineer,Lessons Learned: The Journey to Real-Time Machine Learning at Instacart,"Instacart incorporates machine learning extensively to improve the quality of experience for all actors in our “four-sided marketplace” — customers who place orders on Instacart apps to get deliveries in as fast as 30 minutes, shoppers who can go online at anytime to fulfill customer orders, retailers that sell their products and can make updates to their catalog in real time, and the brand partners that participate in auctions on the Instacart Advertising platform to promote their products.
A typical shopping journey at Instacart is powered by hundreds of machine learning models.  Many decisions/actions happen in real time, which means leveraging machine learning in real-time can provide significant value to the business. One of the major changes we have gone through is transitioning many of our batch-oriented ML systems into real-time. In this talk, we describe ML platform at Instacart with a focus on the journey of real-time ML. We will discuss both fundamental infrastructures and important use cases, review main challenges and decisions, and draw important lessons that could help others learn from our experience.","Key problems to consider for real-time ML.
Important foundations to support real-time ML in a e-commerce platform.
Avoid pitfalls and take aways good lessons in building real-time ML from our experience.",-,,4.0,Deployment and integration,"Real-time ML, ML platform, E-commerce
","Guanghua Shu is a Staff Machine Learning Engineer at Instacart, where he focuses on building end-to-end machine learning solutions to gain actionable insights from data in the e-commerce domain. Guanghua has spent over six years in applied machine learning, and worked on recommender systems for product recommendation, leveraging ML to improve cloud security, and Data/AI platforms.   

Guanghua holds a PhD in ECE from University of Illinois at Urbana-Champaign. He has published over 30 research papers and received over 10 patents. Through academia and industry experience, Guanghua has explored different abstract levels of the technology stack, ranging from ASIC design, computer architecture, distributed software systems, and applied machine learning.  He believes in the power of technology and its outsized impact on business, society and beyond. 
",https://www.youtube.com/watch?v=a9aoRDL4gJ0,"Computer Software, Food & Beverages, Information Technology & Service
",The talk will include more insights and latest update on MLOps at Instacart..,,a9aoRDL4gJ0
Arash Taheri-Dezfouli,Groq,Compiler Engineer,Extending PyTorch for Custom Compiler Targets,"Groq has delivered the world’s first LPUs, focused on LLM inference and deep learning acceleration. However, to support inference specific accelerators, metadata on the PyTorch graph, such as custom or unsupported data types, are often required to improve performance of the model. This metadata is non-trivial to get through PyTorch’s graph export systems (e.g. TorchScript + ONNX, torch.compile, torch.export). In order to maximize inference efficiency for custom hardware targets, we present a generalizable technique that allows users to annotate PyTorch code at different granularities with arbitrary information. PyTorch model annotations can be a simple and powerful means to adjust the mapping of the workload to accelerators, yet maintain the semantics of the PyTorch inference graph. Our technique allows easy injection of information into a PyTorch graph at the Python level and easy recovery of the information and semantics during downstream ingestion. We demonstrate how to use this technique to modify existing PyTorch models in place to enable custom data-types and persist precision information through PyTorch into our compiler.",A generalizable technique that allows users to annotate PyTorch code at different granularities with arbitrary information and modify existing PyTorch models in place to enable custom data-types and persist precision information through PyTorch into Groq compiler.,,Research or Advanced Technical,5.0,"Model dev, training, arch.","Custom Compiler Targets, PyTorch Annotations, Model Optimization
","Arash is a technical lead on Groq's Compiler team, focusing on front-end integrations with PyTorch, ONNX, Jax and other ML/HPC frameworks. He received his masters and undergraduate degrees from the University of Toronto and has worked on Compiler technologies for Machine Learning/AI workloads for the last 7 years. ",https://www.youtube.com/watch?v=Bn62gQ9385Y,,,TMLS 2024,Bn62gQ9385Y
Ryan McClelland,NASA Goddard Space Flight Center,Research Engineer,Evolved Structures: Using AI and Robots to build spaceflight structures at NASA,"Come get a first hand account of how NASA is leveraging Generative Design to reduce the cost and increase the performance of spaceflight missions. Also, how the concept of AI Prompt Engineering can be practically applied to diverse fields, such as structures development.",,,Case Study,4.0,Performance optimization and efficiency,"Generative Design, Spaceflight Structures, AI Prompt Engineering
","From a young age, Ryan McClelland has been captivated by futurism and technology, aspiring to contribute to a brighter future. As a Research Engineer in NASA GSFC's Instrument Systems and Technology Division, he pursues the development and implementation of digital engineering technologies for space-flight mission. Ryan is particularly excited about the potential of Artificial Intelligence, Virtual Reality, Generative Design, and Digital Manufacturing to accelerate space systems development.

With a diverse background in technology development, Ryan's previous research encompasses lightweight X-ray optics, aluminum foam core optical systems, and the investigation of non-linear effects in kinematic mechanisms. In addition to his research, Ryan has played a significant role in various flight missions, including designs currently on orbit aboard the Hubble Space Telescope and International Space Station. Recently, he served as the Roman Space Telescope Instrument Carrier Manager. Ryan holds a B.S. in Mechanical Engineering, summa cum laude, from the University of Maryland.",https://www.youtube.com/watch?v=kXo6SZXWaXQ,Space,,,kXo6SZXWaXQ
"Alex Olsen, Somayeh Sadat","CARTE, University of Toronto","Senior Research Associate, Assistant Director","Demystifying Large Language Models: ChatGPT, GPT-4, and the Future of AI Communication","The rapid advancements in artificial intelligence (AI) have given rise to a new era of large language models (LLMs) such as ChatGPT and GPT-4. These sophisticated AI systems are capable of understanding and generating human-like text, making them invaluable tools for communication and problem-solving. However, the complexity of these models often leaves the general public feeling overwhelmed and unsure of their implications. This talk aims to demystify LLMs, explaining their inner workings in an accessible manner, while highlighting their potential impact on our society.

The presentation will begin by offering an introduction to the concept of LLMs, touching on their history and development. We will explore the key principles behind these AI systems, such as neural networks and natural language processing, in a way that is easy to grasp for a non-technical audience. The primary focus will be on ChatGPT and GPT-4, two of the most advanced and widely-discussed models currently available.

Next, we will delve into the practical applications of these LLMs, demonstrating their versatility in tasks ranging from content creation to customer support. By showcasing real-world examples, attendees will gain a better understanding of how these AI models can transform various industries and streamline everyday tasks.

Finally, we will address some of the ethical and societal implications brought about by the increasing adoption of LLMs. Topics such as data privacy, algorithmic bias, and the potential for AI-generated misinformation will be discussed, providing a balanced perspective on the benefits and challenges of these revolutionary technologies.

By the end of this talk, attendees will have a clearer understanding of large language models and their impact on our lives. Join us for an enlightening journey into the world of AI communication, where we will explore the potential of ChatGPT, GPT-4, and other LLMs to shape our future interactions with technology.","I have experience giving this talk to a completely non-technical audience, for which I've had great feedback. The aim of this talk is for people to walk away feeling like they actually understand the functionality of LLMs, without needing a tchnical background.",,Business strategy,1.0,Future trends,"Large Language Models, ChatGPT, GPT-4





","Alex - At CARTE, Alex collaborates with faculty, industry partners, and students to bring the latest advances in AI to a broad audience. He has contributed to machine learning-based research in a wide variety of fields, and runs training for learners at every level.

Somayeh Sadat is the assistant director at the Centre for Analytics and Artificial Intelligence Engineering (CARTE), University of Toronto. In this role, she facilitates and oversees partnerships with industry and government to collaboratively conduct state-of-the-art research and translate and commercialize effective solutions in analytics and artificial intelligence, where applicable with leveraged funds. She also helps nurture the next generation of engineering talent through facilitating experiential learning opportunities with external partners. Somayeh holds a Ph.D. from the University of Toronto, and has over fifteen years of experience in research and education, business development, and consulting.",https://www.youtube.com/watch?v=fJBomhGZc3Y,,I have experience explaining complex AI concepts to a completely non-technical audience.,TMLS 2023,fJBomhGZc3Y
Arthur  Berrill,RBC,CTO of Data and Analytics,Simulation and Prediction - exploring the Digital Twin Technology stack.,"The digital twin technology stack encompasses machine learning models as active agents federated together, geospatial intelligence, ontological constraints (as in neural symbolic AI), ontological learning (where an ontology reasoner responds to a stimulus and generates or updates data) all based on a knowledge graph. This talk explains the deployment of the technology stack and its use to solve a complex prediction use case in agriculture and climate change management.",North America has missed the rapid adoption profile of geospatial digital twin technology evident overseas despite the fact that it is a branch of AI. This talk is to galvanize awareness of the technology and its potential.,,Case study,5.0,"Model dev, training, arch.","Digital Twin Technology, Geospatial Intelligence, Knowledge Graphs





","Arthur Berrill is CTO for the Royal Bank of Canada Data and Analytics team. He is a technology leader and voice for RBC technology and innovation with commercial partners, government, open source communities and academic researchers, in domains relevant to RBC’s vision and strategy.
In service of this responsibility, Arthur is involved in most of the data science disciplines including location intelligence, data content, artificial intelligence, ontology, graph analytics and climate change studies. In particular, Arthur has a long history in the location intelligence field.

Arthur is an RBC Distinguished Engineer.
",https://www.youtube.com/watch?v=tl3RGFfYV4A,"Banking & Financial Services, Computer Software, Environmental Services, Information Technology & Service, Insurance, Telecommunications
",,TMLS 2023,tl3RGFfYV4A
Vasilis Vagias,CentML,Head of AI Solutions,Optimized AI Deployment Platform,Showcasing CentMLs ability to streamline the process of deploying and optimizing LLMs in production.,,,Furture of AI,,Deployment and integration,"CentML, LLM deployment, Optimization","Vasilis is a Solutions Architect at CentML, where he focuses on developing cutting-edge strategies for optimizing and deploying large language models (LLMs) and AI/ML solutions. With a background in AI architecture at Intel and a passion for app development, Vasilis has designed and deployed applications that bring LLM capabilities directly to mobile users. His expertise spans across various technologies, including PyTorch, SwiftUI, Docker, and cloud-based AI/ML infrastructure.

At CentML, he leads efforts in designing solutions that accelerate AI/ML operations, focusing on efficiency at both the hardware and software levels. This includes optimizing inference engines and managing large-scale deployments on diverse hardware, such as A100 GPUs. His work involves a deep understanding of how AI/ML computations can be optimized at the chip level, ensuring that our solutions provide significant performance gains and cost reductions for enterprise customers.

In addition to Vasilis' work at CentML, he has developed and published iOS applications, incorporating features like text, image, and audio generation. One of his current projects is an app that leverages AI to create interactive experiences for users, from alphabet books to sentiment analysis tools. He has integrated in-app purchases, Core Data management, and SwiftUI-based modern UIs into these projects, showcasing his ability to merge technical skills with a user-centric approach.

Vasilis holds a degree in Physics, which provides a unique perspective on problem-solving and deep technical understanding. He is currently expanding his knowledge in reinforcement learning and open-source LLM model deployment, staying up-to-date with the latest practices and trends in AI. Vasilis approach blends practical expertise in AI infrastructure with a keen sense of market strategy, making me adept at positioning innovative solutions to meet the needs of enterprise clients.",https://www.youtube.com/watch?v=pNgwVCrpUJU,,,MLOps & GenAI World 2024,pNgwVCrpUJU
Emmanuel Turlay,Sematic,CEO,Oracles and Worker Bees: The future of AI models,"Today, most of the community's attention is on very large language models (7B+ parameters). These perform impressively, but they remain wild luxuries. They are difficult and expensive to run.
In this talk, I show how the future will be made of many smaller models fine-tuned on synthetic data generated by large models.",,,Future of AI Talk,,Future trends,"Future of AI models, Smaller models, Synthetic data","Emmanuel started his career doing particle physics research at CERN before moving into tech in 2011. In 2014, he joined Instacart where he built out payment and ordering systems. In 2018, he joined robotaxi company Cruise to establish the ML Infrastructure team. In 2022, drawing from his experiences, he founded Sematic to share insights into ML Infrastructure with the industry in an open-source manner.",https://www.youtube.com/watch?v=9-J_QW5zgzM,,"Most online content overfit to one particular use case or type of models, and most authors do not have the experience that we have building tooling for organizations of 100+ ML Engineers shipping mission-critical models for self-driving cars.
",MLOps & GenAI World 2023,9-J_QW5zgzM
"Quoc Tien Au, Aysha Cotterill ",Manifest Climate,"Data Scientist, Data Analyst",Assessing Alignment of Climate Disclosures Using NLP for the Financial Markets,"Climate-related disclosure is increasing in importance as companies and stakeholders alike aim to reduce their environmental impact and exposure to climate-induced risk. Companies primarily disclose this information in annual or other lengthy documents where climate information is not the sole focus. To assess the quality of a company's climate-related disclosure, these documents, often hundreds of pages long, must be reviewed manually by climate experts. We propose a more efficient approach to assessing climate-related financial information. We construct a model leveraging TF-IDF, sentence transformers and multi-label k nearest neighbors (kNN). The developed model is capable of assessing alignment of climate disclosures at scale, with a level of granularity and transparency that will support decision-making in the financial markets with relevant climate information.","How an early-stage startup runs machine learning experiments ; takes decisions balancing model performance, model explainability, resource constraints and added business value ; uses deep language models to create the most valuable business opportunities.",,Case Study,5.0,Performance optimization and efficiency,,"I am a data scientist at Manifest Climate, working on applying machine learning and natural language processing to climate disclosures. Extracting information at scale is paramount to increase transparency in financial markets, so that we can improve decision-making with data-driven climate information.",https://www.youtube.com/watch?v=Gfv2xA8ogq4,,"How to creatively solve business problems using machine learning and NLP.
Choose the models that align the most with the business needs and can evolve as the business grows.
Machine learning projects are only possible after a certain data maturity is reached.
How to successfully use machine learning to unlock untapped business opportunities.",TMLS 2022,Gfv2xA8ogq4
Iddo Avneri,LakeFS,VP Customer Success,Building Reproducible ML Processes with an Open Source Stack,"Machine learning experiments consist of Data + Code + Environment. While MLFlow Projects are a great way to ensure reproducibility of Data Science code, it cannot ensure the reproducibility of the input data used by that code.
In this talk, we'll go over the trifecta required for truly reproducible experiments: Code (MLFlow and Git), Data (lakeFS) and Environment (Infrastructure-as-code).
This talk will include a hands-on code demonstration of reproducing an experiment, while ensuring we use the exact same input data, code and processing environment as used by a previous run. We will demonstrate programmatic ways to tie all moving parts together: from creating commits that snapshot the input data, to tagging and traversing the history of both code and data in tandem."," This talk will demonstrate the progress we have made to actually making ML Processes, workflows, and data reproducibility truly possible through open source tooling.",,MLOps & Infrastructure,4.0,Introduction to MLOps and GenAI,"MLOps, Reproducible ML Processes, Open Source Tooling","Iddo has a strong software development background. He started his career in the army, where he served for 8 years, eventually heading the main software development school. Following his service, Iddo built technical teams for several startups in the Observability, Cloud and data spaces. 
Prior to joining the lakeFS team Iddo built the technical enterprise field team at Turbonomic, from the ground up, as well as served as the Field CTO, and was the account executive for some of the company’s largest customers; up to the $1.9B IBM acquisition in 2021. At Treeverse, the company behind lakeFS, Iddo runs all customer engagements from sales to customer success.",https://www.youtube.com/watch?v=PtaNIMBaWoQ,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Telecommunications
",,TMLS 2024,PtaNIMBaWoQ
Andrea Ruotolo,Rockwell Automation,"Global Head, Sustainability / ESG ",AI & Sustainability: A $50 trillion opportunity,"
The financial and business community have already caught on to the essential importance of sustainability. Investors now call for better practices and reporting on Environment, Social, and Governance, or ESG performance metrics. According to Bloomberg Intelligence, growth in ESG investing is fast becoming the new norm, with ESG investments projected to exceed USD 50 trillion by 2025 – more than 1/3 of all global assets under management. This movement of funds to ESG represents a massive, once-in-a-generation transition to an entirely new economy.

Sustainability is incredibly complex, involving billions of moving parts and decisions. It starts at the edge, where exabytes of data are flowing from real-time sensors and controls in factories and power plants, which aggregate up to the top-level decision makers in companies, which aggregate up to the massive funds that hold portfolios of those companies, and to government regulators and policymakers. AI is critical in analyzing those exabytes of data and enables closed-loop optimization to reduce energy, water, and waste.

In this session, we'll explore the top 3 needs and opportunities for AI to catalyze change toward more sustainable companies, economies, and societies. ",,,Ignite Lighting Talk,,"Ethics, governance compliance ","AI, Sustainability, ESG
","Andrea is the Global Head of Sustainability/ESG at Rockwell Automation, the world’s largest industrial automation company, with responsibility for advancing innovation in sustainability for Rockwell’s customers, which include Fortune 100 companies in energy and manufacturing, representing millions of employees and hundreds of billions of dollars in annual revenues. Andrea is a passionate evangelist for the role AI/ML can play in dramatically improving the sustainability of the industrial sector.

Across her nearly two decades of experience in leading technology innovation, in a career spanning Europe, Asia, and the Americas, Andrea has held multiple senior executive roles focused on applying advanced technologies to solve the sustainability challenge. She has served as co-founder and entrepreneur in smart grid consulting, global lead in the world’s largest engineering services firm in the energy sector, and senior director at a major utility.

As well as her Fulbright Doctorate in an ESG analysis of sustainable energy systems, Andrea holds a B.A. from the University of la Plata in Argentina, a M.Sci. in Aeronautical and Aerospace Engineering from Madrid Polytechnic, and certification in Digital Business Strategy and AI from MIT Sloan School of Management.",https://www.youtube.com/watch?v=6DzQu5KyhoA,,,TMLS 2022,6DzQu5KyhoA
Alistair Johnson,SickKids,Scientist,Do we still need clinical large language models?,"It remains unclear whether large language models (LLMs) trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text. Recent results have suggested that LLMs encode a surprising amount of medical knowledge. To investigate whether smaller domain-specific LLMs retain utility, we conduct an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks. We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when fine-tuned on limited annotated data. Further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text. We release our trained models used under the PhysioNet Credentialed Health Data license and data use agreement.",In what cases fine-tuning an LLM to specialized domains (like clinical text) is worthwhile,,Advanced Technical,5.0,"Model dev, training, arch.","Clinical Language Models, Model Fine-Tuning, Specialized Domains





","Dr. Johnson is a Scientist at the Hospital for Sick Children. He received his Bachelor of Biomedical and Electrical Engineering at McMaster University and successfully read for a DPhil at the University of Oxford. Dr. Johnson strives to overcome barriers in data sharing, and his work on the MIMIC databases demonstrates the immense potential of publicly available healthcare data. His research focuses on the development of new data structures, algorithms for deidentification, and new machine learning methodologies for medical data.",https://www.youtube.com/watch?v=nK9sJSeuNVE,Hospital & Health Care,,,nK9sJSeuNVE
Aniket Maurya,Lightning AI,Research Engineer,Serving GenAI Workload At Scale With LitServe," Learn about serving AI models with high throughput at scale. Dynamic batching, autoscaling and serve LLMs based complex workloads."," - Model serving in production
- Dynamic batching for high throughput
- Autoscaling
- Logging and monitoring in production",Python and machine learning,Workshop,5.0,Deployment and integration,"Model serving, Dynamic batching, Autoscaling","I’m Aniket, a Machine Learning - Software Engineer with with over 4 years of experience, demonstrating a strong track record in developing and deploying machine learning models to production.",https://www.youtube.com/watch?v=ClA0OwE4Zs0,Computer Software,This sessions covers serving ML models at high throughput in pure Python.,MLOps & GenAI World 2024,ClA0OwE4Zs0
Annie En-Shiun Lee,University of Toronto,Assistant Professor (Teaching Stream) for the Computer Science Department,"Pre-Trained Multilingual Sequence-to-Sequence Models for NMT: Tips, Tricks and Challenges","Neural Machine Translation (NMT) has seen a tremendous spurt of growth in less than ten years, and has already entered a mature phase. Pre-trained multilingual sequence-to-sequence (PMSS) models, such as mBART and mT5, are pre-trained on large general data, then fine-tuned to deliver impressive results for natural language inference, question answering, text simplification and neural machine translation. This tutorial presents 1) An Introduction to Sequence-to-Sequence Pre-trained Models, 2) How to adapt pre-trained models for NMT, 3) Tips and Tricks for NMT training and evaluation, 4) Challenges/Problems faced when using these models. This tutorial will be useful for those interested in NMT, from a research as well as industry point of view.","This tutorial will give an overview of Pre-trained Sequence-to-Sequence Multilingual Models, tips, tricks and frameworks that can be used to adapt these models for NMT especially for low resource languages and the challenges faced while using these models and how to overcome them.",,Workshop,5.0,Introduction to MLOps and GenAI,,Annie En-Shiun Lee is an Assistant Professor (Teaching Stream) for the Computer Science Department at the University of Toronto. She received her PhD from the University of Waterloo in 2014 under the supervision of Professor Andrew K. C. Wong and Daniel Stashuk from the Centre of Pattern Intelligence and Machine Intelligence. She has also been a visiting researcher at the Fields Institute (invited by Nancy Reid) and CUHK (invited by K. S. Leung and M. H. Wong) as well as a research scientist at VerticalScope and Stradigi AI.,https://www.youtube.com/watch?v=40HhnYSj0uI,,,TMLS 2022,40HhnYSj0uI
Richie Frost,Dropbox,Software Engineer,From Prototype to Product: Rapid iteration and ML model deployment at Dropbox,"As AI rapidly evolves, organizations must keep pace by iterating on ML models quickly. In the MLOps field, we face several challenges that make it hard to prototype and deploy quickly. In this presentation, we share how we leverage a mix of open-source tools and our own solutions at Dropbox to achieve faster iteration speeds, reducing prototyping and deployment times from weeks to under an hour.

The session will explore how to establish an end-to-end system to achieve rapid prototyping and deployment while maintaining security best practices. Attendees will gain insights into the integration of open source frameworks, such as TorchServe, Triton, KServe, and Kubernetes, along with internal tools to accelerate the process. The discussion will also delve into the challenges of iterating on ML models and their deployments and explore strategies for optimizing resource allocation, managing dependencies, and automating deployment processes. "," How to develop a system for easily prototyping and deploying models in production, including LLMs",,Case Study,4.0,Deployment and integration,"Rapid Prototyping, ML Model Deployment, Open Source Tools
","Richie Frost is a software engineer on the ML Foundations team at Dropbox, specializing in rapid iteration on ML inference platforms. Previously, he worked as a data scientist and software engineer at Microsoft, where he built and analyzed innovative AI-based solutions.",https://www.youtube.com/watch?v=nIIBQXOz7dA,"Computer Software, Information Technology & Service
",,MLOps & GenAI World 2023,nIIBQXOz7dA
Sri  Raghu Malireddi,Grammarly,Senior Machine Learning Engineer,On-Device ML for LLMs: Post-training Optimization Techniques with T5 and Beyond," This session explores the practical aspects of implementing Large Language Models (LLMs) on devices, focusing on models such as T5 and its modern variations. Deploying ML models on devices presents significant challenges due to limited computational resources and power constraints. However, On-Device ML is crucial as it reduces dependency on cloud services, enhances privacy, and lowers latency. 

Optimizing LLMs for on-device deployment requires advanced techniques to balance performance and efficiency. Grammarly is at the forefront of On-Device ML, continuously innovating to deliver high-quality language tools. This presentation offers valuable insights for anyone interested in the practical implementation of on-device machine learning using LLMs, drawing on Grammarly's industry application insights.

The topics that will be covered as part of this talk are - 
- Techniques for optimizing performance and reducing inference latency in LLMs - Quantization, Pruning, Layer Fusion, etc. 
- Methods to develop efficient and scalable AI solutions on edge devices.
- Addressing common challenges in deploying LLMs to edge devices - over-the-air updates, logging, and debugging issues in production.",,,case study & advanced technical,4.0,Performance optimization and efficiency,"On-device ML, Optimization techniques, LLM deployment"," Sri Raghu Malireddi is a Senior Machine Learning Engineer at Grammarly, working on the On-Device Machine Learning. He specializes in deploying and optimizing Large Language Models (LLMs) on-device, focusing on improving system performance and algorithm efficiency. He has played a key role in the on-device personalization of the Grammarly Keyboard. Before joining Grammarly, he was a Senior Software Engineer and Tech Lead at Microsoft, working on several key initiatives for deploying machine learning models in Microsoft Office products.",https://www.youtube.com/watch?v=QRPvr1td4s0,Computer Software,,MLOps & GenAI World 2024,QRPvr1td4s0
Dan Adamson,Armilla AI,CEO and Co-founder,"How do you make GenAI safe, trustworthy and useful for enterprises?","We discuss the risks of using generative AI in enterprise settings, as well as new alignment techniques to test and iteratively improve generative AI models' safety, fairness, robustness, and fit-for-purpose capability.",,,Ignite Lighting Talk,,"Ethics, governance compliance ","GenAI Risks, Safety Alignment, Enterprise Applications","Dan Adamson is the Co-Founder and CEO of Armilla.AI, a company helping institutions testing and building more fair, safe, trustworthy, and useful AI. Previously, he founded and served as OutsideIQ’s CEO, since its inception in 2010 until its acquisition in 2017 by Exiger, where he remained as their President overseeing product and cognitive computing research. Dan also previously served as Chief Architect at Medstory, a vertical search start-up acquired by Microsoft.  He is an expert on vertical search, investigative use of big data and cognitive computing with more than 15 years in the industry. He holds several search algorithm and cognitive computing patents, has been named among the most influential “must-see” thought leaders in AI and FinTech, in addition to being a recipient of numerous academic awards and holding a Master of Science degree from U.C. Berkeley.  ",https://www.youtube.com/watch?v=vGoZN2vffBo,"Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology & Service, Marketing & Advertising
",,TMLS 2023,vGoZN2vffBo
"Ivan Nardini, Holt Skinner",Google Cloud,"Developer Relation Engineer, AI/ML, Developer Advocate",Building a Multimodal RAG: A Step-by-Step Guide for AI/ML practitioners,"Learn to build a multimodal Retrieval Augmented Generation (RAG) system that goes beyond text, incorporating images, video, and audio. This hands-on session dives deep into the architecture and foundational principles of multimodal RAG, enabling you to leverage diverse data sources for enhanced information retrieval and extraction.","Foundational principles of multimodal RAG systems
How to design and implement key RAG components (data ingestion, parsing, chunking, retrieval, ranking, etc.) for multimodal RAG.
Best practices for evaluating RAG performance and mitigating hallucinations
Hands-on experience building a multimodal RAG system on Vertex AI (with provided cloud credits!)
Strategies for scaling your RAG system from prototype to MVP and beyond.
Gain insights into scaling Multimodal RAG for real-world applications.",,,5.0,"Model dev, training, arch.","Multimodal RAG, System architecture, Scaling"," Ivan Nardini is a Developer Relation Engineer on Google’s Cloud team, focusing on Artificial Intelligence and Machine Learning. He enables developers to build innovative AI and ML applications using their preferred libraries, models, and tools on Vertex AI, through code samples, online content, and events. Ivan has a master degree in Economics and Social Sciences from Università Bocconi and he attended a specialized training in Data Science from Barcelona Graduate School of Economics.

Holt Skinner is a Developer Advocate at Google Cloud AI, dedicated to helping developers harness the full potential of cutting-edge tools like Gemini and Vertex AI Search. He has led the creation of numerous educational series on the Google Cloud Tech YouTube channel and manages the Google Cloud Generative AI GitHub repository, delivering practical resources and expert insights. With a passion for simplifying developer workflows, Holt actively engages with the global developer community to gather feedback and drive continuous improvement in AI technologies. Beyond his work in tech, Holt is also an accomplished classical singer, having performed in concerts across the United States and Europe.",https://www.youtube.com/watch?v=CPpY4w4m5n0,"Banking & Financial Services, Insurance, Other","True Multimodal Focus: Most RAG sessions focus solely on text; this session dives deep into handling diverse data types (image, video, audio) for real-world applications.
Architecture-First Approach: We emphasize foundational principles and system design, enabling you to adapt your knowledge to any model (open/close) or ecosystem (LangChain, LlamaIndex, etc.).
Hands-on with Vertex AI: Gain practical experience with an enterprise-ready platform for data security and scaling.
Evaluation and Refinement: Learn to effectively evaluate your RAG system and reduce hallucinations for improved accuracy.
Future-Proof Your Skills: Learn core principles applicable to any multimodal model or framework, keeping you at the forefront of AI development.
Build More Robust AI Applications: Develop RAG systems capable of handling complex, real-world data with greater accuracy and efficiency.",MLOps & GenAI World 2024,CPpY4w4m5n0
"Everaldo Aguiar, Wendy Foster, Margaret Wu, Christopher Parisien","PagerDuty, Shopify, CIBC, NVIDIA","Senior Engineering Manager, Data Products Leader, Senior Data Scientist, Advanced Analytics and AI, Senior Manager, Applied Research",Panel: RAGs in Production: Delivering Impact Safely and Efficiently," ""Urgent"" and ""unplanned"" are among the least favorite words in any productive team's dictionaries. Unexpected issues disrupt roadmaps, delay important work, lead to burnout, and hurt customer trust.

Here at PagerDuty we’ve been leveraging AI to help our customers experience fewer incidents and resolve the ones they do have faster. This often involves giving them streamlined access to information they need about our product, their individual setups, and an efficient way for them to get answers to complex answers on the fly.

As technologies evolved and we rolled out our generative AI infrastructure, RAGs became an excellent candidate for those use-cases. They allow for an easy-to-automate process of building ""knowledge bases"" and using those to power powerful chat-like applications, but productionalizing them in a safe manner is often more challenging than building these RAG systems themselves.

In this panel we'll discuss some of these challenges, how we've been tackling them, as well as existing areas of open research we're excited to pursue in the coming months.",Attendees will learn how to tackle some common (and uncommon) challenges that come with bundling RAG models into their own products. We'll cover a few corner cases that were completely unexpected as well as automation processes that we designed to ensure that complex parts of our systems could be maintained with minimal engineering effort.,,Panel,4.0,Deployment and integration,"RAG (Retrieval Augmented Generation), Productionalizing, Challenges
","Everaldo: Everaldo started his Data Science journey as a Data Science for Social Good Fellow at the Center for Data Science and Public Policy at UChicago. Today he is a Senior Engineering Manager at PagerDuty where he leads both the Data Science and Data Engineering teams, and a faculty member at the Khoury College at Northeastern University. Prior to that he was a Data Science Lead at Shopify's Growth organization. Everaldo is originally from Brazil and Seattle has been home to him for 6 years.

Wendy: With over 10 years of experience leading data organizations at scale, Wendy Foster divides her time between data start-up advising and applied data science education; supporting the next wave of data leaders and innovation in this rapidly evolving space.

Margaret: Margaret is a Senior Data Scientist at CIBC, specializing in leveraging machine learning and artificial intelligence techniques to address a variety of business challenges across the enterprise. Her expertise spans NLP, time series forecasting and LLM RAG. She holds a Master’s degree in Data Science from the University of British Columbia and Bachelor’s degree in Mathematics from University of Waterloo.

Christopher: Christopher Parisien is a Senior Manager of Applied Research at NVIDIA, leading the development of NeMo Guardrails, a toolkit for safety and security in Large Language Models. Chris holds a PhD in Computational Linguistics from the University of Toronto, where he used AI models to explain the strange ways that children learn language. During his time in industry, he helped build the first generation of mainstream chatbots, developed systems to understand medical records, and served as Chief Technology Officer at NexJ Health, a patient-centred health platform. His current focus at NVIDIA is to bring trustworthy language models to large enterprises.",https://www.youtube.com/watch?v=zBlZOs8ljY4,"Other
",,TMLS 2024,zBlZOs8ljY4
Amit  Kesarwani,lakeFS,"Director, Solution Engineering",Fast Data Loading for Deep Learning Workloads with lakeFS Mount,"Working with large datasets locally allows for a lot more control in your executions and workflows mainly for AI and Deep Learning workloads.

However, this can present a number of tradeoffs that lakeFS Mount helps solve:
• 𝗚𝗶𝘁 𝗶𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗶𝗼𝗻 – Mounting a path in a Git repo automatically tracks the data version, linking it with your code. When checking older code versions, you get the corresponding data version, preventing local-only successes.

• 𝗦𝗽𝗲𝗲𝗱 – Data consistency and performance are guaranteed. lakeFS prefetches commit metadata into a local cache in sub-milliseconds, allowing you to work immediately without having to wait for large dataset downloads.
Intelligent – lakeFS Mount efficiently uses cache, accurately predicting which objects will be accessed. This enables granular pre-fetching for metadata and data files before processing starts.

• 𝗖𝗼𝗻𝘀𝗶𝘀𝘁𝗲𝗻𝗰𝘆 – Working locally risks using outdated or incorrect data versions. With Mount, you can work with consistent, immutable versions, ensuring you know exactly what data version you’re using."," With lakeFS Mount, you can transparently mount an object store reference as a local directory (yes, even at petabyte-scale), while avoiding the common pitfalls typically associated with trying to access an object store as a filesystem.

In this talk, you will learn about lakeFS Mount and you will also see a demonstration of:
• Training a TensorFlow predictive model on data mounted using lakeFS Mount
• Integration with Git to version code and data together
• Reproducibility of code as well as data",,Virtual Talk,6.0,"Model dev, training, arch.","lakeFS Mount, Data versioning, Reproducibility ","Amit heads the solution architecture group at Treeverse, the company behind lakeFS, an open-source platform that delivers a Git-like experience to object-storage based data lakes.
Amit has 30+ years of experience as a technologist working with Fortune 100 companies as well as start-ups. Designing and implementing technical solutions for complicated business problems.
As an entrepreneur, he launched a cloud offering to provide Data Warehouse as a Service. Amit holds a Master’s certificate in Project Management from George Washington University and a bachelor’s degree in Computer Science and Technology from Indian Institute of Technology (IIT), India. He is the inventor of the patent: System and Method for Managing and Controlling Data",https://www.youtube.com/watch?v=xhojSxbD7Qg,"Other, Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Information Technology Services, Insurance, Telecommunications","Learn how to allow for best-in-class I/O performance, while simultaneously making it extremely easy to use - Just read from a local directory. No integration or custom SDKs required. All this while ensuring full reproducibility and enjoying the benefits of version control such as branching, rolling back, tagging and more",MLOps & GenAI World 2024,xhojSxbD7Qg
Fatma Tarlaci,Rastegar Capital,CTO,"Agentic AI: Learning Iteratively, Acting Autonomously","AI agents are advanced software systems capable of autonomous actions and decision-making to achieve specific goals. Built on sophisticated machine learning models, they can process and respond to dynamic data inputs in real-time. Unlike traditional large language models (LLMs) that generate outputs in a single attempt (zero-shot mode), agentic AI introduces an iterative, dynamic workflow. These workflows often involve multiple stages—planning, data gathering, drafting, assessment, and revision—significantly improving the quality of outcomes. This process mirrors human learning, where continual refinement leads to better results. Notably, iterative agentic workflows have recently shown impressive performance in tasks like coding, outperforming standard models on benchmarks such as HumanEval.

This presentation will provide an in-depth analysis of the architectures that underpin agentic AI, explore the cutting-edge technologies enabling their capabilities, and delve into their practical applications. It will also address key challenges in the field, such as scalability and ethical considerations, while exploring future directions. Attendees will gain a thorough understanding of AI agents, their current uses, and their potential to transform industries."," Attendees will learn about the fundamental architectures and technologies underpinning AI agents, their real-world applications, and the challenges they face, including ethical issues and scalability. The session will also explore future trends in AI development and its potential impact across various sectors.",,Advanced Technical/Research,3.0,"Model dev, training, arch.","Agentic AI architectures, Iterative workflows, Scalability and ethics","Dr. Fatma Tarlaci is a distinguished engineering leader with a wealth of experience in artificial intelligence, specializing in natural language processing. As the Chief Technology Officer at Rastegar Capital, she possesses a unique combination of technical expertise and leadership skills that distinguish her in the industry. She applies advanced AI solutions in her work that enhance business strategies and operational efficiencies through the integration of cutting-edge technologies. Her approach combines her deep expertise in AI with practical applications to solve complex challenges in the industry. Before entering the industry, she conducted research at OpenAI and taught in academia. Demonstrating her commitment to education, she currently also teaches as an Adjunct Assistant Professor of Computer Science at UT Austin. Her interdisciplinary background provides her with a refined ability to navigate diverse professional environments effectively. She mentors at several organizations and serves as an advisor to startups.",https://www.youtube.com/watch?v=_k8sPizcqUg,"Banking & Financial Services, Information Technology Services, Computer Software, Hospital & Health Care, Marketing & Advertising","This session uniquely combines a detailed examination of AI agent architectures with practical insights into their iterative agentic workflows, which demonstrates substantial potential and improvements over traditional models. It offers a blend of technical depth and strategic foresight into the evolving role of AI agents in industry and research.",MLOps & GenAI World 2024,_k8sPizcqUg
Bar Chen,Aporia,Senior software engineer,AI Tools Under Control: Keeping Your Agents Secure and Reliable,"This session focuses on AI tools and the importance of keeping them secure and reliable. We’ll discuss the main security challenges these tools face and share simple, practical solutions to address them. You’ll discover how using best practices can help protect your AI systems, reduce risks, and maximize their effectiveness.",,,Ignite Lighting Talk,,Security and Privacy,"AI security, Risk reduction, Best practices","I’m a senior software engineer and product manager at Aporia, where I’ve been working for the past two years. I’ve spent nearly eight years in the tech industry, focusing on both cybersecurity and AI.",https://www.youtube.com/watch?v=poqhv4hPTpA,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Information Technology Services, Insurance, Marketing & Advertising, Telecommunications",The session focuses not just on the power of AI tools but on the critical need to secure and guide them properly.,MLOps & GenAI World 2024,poqhv4hPTpA
Tristan Zajonc,Continual,Co-founder,Automating Knowledge Work with Generative AI,"With the emergence of large generative AI models such as GTP3, DallE2, and Stable Diffusion, generative AI is set to revolutionize knowledge work over the next few years.  However applying these models to solve real world business problems remains a challenge due to the need to align models with human preferences, orchestrate models to address complex use cases, and augment models with human feedback and control.  This workshop will provide an overview of the current state of generative AI and a hands on experience using generative AI to automate knowledge work.","- An overview of the state-of-the-art of generative AI
- A hands on experience using generative AI to automate knowledge work.",,Workshop,4.0,Introduction to MLOps and GenAI,,"Tristan is the cofounder of Continual, a startup focused on enabling pervasive operational AI within the enterprise.  He was previously the CTO for Machine Learning at Cloudera.",https://www.youtube.com/watch?v=DZGLC_4ZknQ,,"There have not been many presentations for this type of topic before. Most talks in this vein are squarely within the context of MLOps, which I believe is a proper subset of operational AI. 
",TMLS 2022,DZGLC_4ZknQ
"Stefan Krawczyk, Hugo Bowne-Anderson",DAGWorks Inc.,"Co-founder & CEO, Independent Data and AI Scientist","Building GenAI-Powered Apps: A Workshop for Software Engineers
","This workshop is designed to equip software engineers with the skills to build and iterate on generative AI-powered applications. Participants will explore key components of the AI software development lifecycle through first principles thinking, including prompt engineering, monitoring, evaluations, and handling non-determinism. The session focuses on using multimodal AI models to build applications, such as querying PDFs, while providing insights into the engineering challenges unique to AI systems. By the end of the workshop, participants will know how to build a PDF-querying app, but all techniques learned will be generalizable for building a variety of generative AI applications.

If you're a data scientist, machine learning practitioner, or AI enthusiast, this workshop can also be valuable for learning about the software engineering aspects of AI applications, such as lifecycle management, iterative development, and monitoring, which are critical for production-level AI systems.
","- How to integrate AI models and APIs into a practical application.
- Techniques to manage non-determinism and optimize outputs through prompt engineering.
- How to monitor, log, and evaluate AI systems to ensure reliability.
- The importance of handling structured outputs and using function calling in AI models.
- The software engineering side of building AI systems, including iterative development, debugging, and performance monitoring.
-Practical experience in building an app to query PDFs using multimodal models.
",,Workshop,4.0,"Model dev, training, arch.","Generative AI applications, Multimodal models, Prompt engineering","Stefan hails from New Zealand, speaks Polish, and completed his Masters at Stanford specializing in AI. He has spent over 15 years working across many parts of the stack, but has focused primarily on data and machine learning / AI related systems and their connection to building product applications. He has built many 0 to 1 and 1 to 3 versions of these systems at places like Stanford, Honda Research, LinkedIn, Nextdoor, Idibon, and Stitch Fix.

A regular conference speaker, Stefan has guest lectured at Stanford’s Machine Learning Systems Design course & Apps with LLMs Inside Course and is an author of two popular open source frameworks called Hamilton and Burr.

Stefan is currently co-founder and CEO of DAGWorks, where he’s building for the composable AI future that spans pipelines & agents with Hamilton & Burr.

------

Hugo is an independent data and AI scientist, consultant, writer, educator & podcaster. His interests include promoting data & AI literacy/fluency, helping to spread data skills through organizations and society and lowering the barrier to entry for data science, analysis, and machine learning. Previously, he was Head of Developer Relations at Outerbounds, a company committed to building infrastructure that provides a solid foundation for machine learning applications of all shapes and sizes. He is also the host of the industry podcast Vanishing Gradients. He was previously Head of Marketing and Data Science Evangelism at Coiled, building solutions for scalable data science and machine learning in Python. Before this, he was at DataCamp, a data science training company educating over 4 million learners worldwide through interactive courses on the use of Python, R, SQL, Git, Bash and Spreadsheets in a data science context. While there, he spearheaded the development of over 25 courses in DataCamp’s Python curriculum, impacting over 250,000 learners worldwide through my own courses. He hosted and produced the data science podcast DataFramed, in which he used long-format interviews with working data scientists to delve into what actually happens in the space and what impact it can and does have.",https://www.youtube.com/watch?v=tSIpREFVMXs,"Telecommunications, Marketing & Advertising, Insurance, Information Technology Services, Hospital & Health Care, Food & Beverage, Computer Software, Enviromental Services, Banking & Financial Services, Automotive, Other","Most workshops focus on building demoware. We won't be.
We'll show that building production grade agents are possible in a workshop, given the right principles/concepts and open source libraries.",MLOps & GenAI World 2024,tSIpREFVMXs
"Chinmay Sheth, Colleen Gilhuly, Haleh Shahzad",Royal Bank of Canada (RBC),"Senior Machine Learning Engineer, Data Scientist, Director, Data Science",Building an end-to-end web application integrated with Microsoft's Semantic Kernel and a Large Language Model,"Large language models (LLMs) such as ChatGPT are able to interpret text input and generate human-like responses. Many individuals and companies are excited to use this technology, but integration remains a question
 mark. Applications using LLMs are also limited by their tendency to invent information and give unpredictable answers.
 
Microsoft’s Semantic Kernel is an open source, lightweight SDK which enables fast integration of LLMs into a wide range of applications. It also enhances the power of LLMs with a structured approach to responses
 and the ability to refer to external sources of truth. 

In this workshop, we will give a crash-course on Microsoft Semantic Kernel and demonstrate how to create a simple web app that harnesses the power of LLMs from OpenAI. This workshop will be hands on so please come
 prepared with an OpenAI API key. ","- Learn the basics of LLMs, and their applications/impacts in industry
- Hands-on experience with Semantic Kernel’s skills, planner, memories, and chains
- You will be able to build your own web app with an LLM integrated into the backend",,Workshop,5.0,Deployment and integration,"Large Language Models, Microsoft Semantic Kernel, Web Application Integration","Chinmay Sheth is a Senior Machine Learning Engineer at the Royal Bank of Canada and is completing his MSc in Computer Science at McMaster University. His responsibilities include providing support for MLOps tooling, promoting ML models to production, and developing production-grade data pipelines.

Colleen - Colleen completed her PhD in Astronomy & Astrophysics at the University of Toronto. She then turned to data science in order to combine her favourite aspects of research with a non-academic career. She joined the Chief Data Office at RBC in July 2022, working primarily on projects in NLP.


Haleh is a Director, Data Science at RBC with 9+ years of experience in AI (Deep learning, machine learning), software development and advanced analytics.

She is currently leading AI initiatives for cutting-edge solutions to maximize the impact of data across organization at RBC. She is also an instructor in the school of continuing studies at York University.

Haleh has a Ph.D. degree in Electrical and Computer Engineering from McMaster university where she was also part of the sessional faculty in the Electrical and computer engineering department.",https://www.youtube.com/watch?v=iV5QEooEfS8,"Banking & Financial Services, Computer Software, Information Technology & Service
",,TMLS 2023,iV5QEooEfS8
Anouk Dutrée,UbiOps,Product Owner,Deploying Generative AI Models: Best Practices and an Interactive Example,"Generative AI models are all the hype nowadays, but how do you actually deploy them in a scalable way? In this talk we will discuss best practices when moving models to production, as well as show an interactive example of how to deploy one using UbiOps. UbiOps is a serverless and cloud agnostic platform for AI & ML models, built to help data science teams run and scale models in production. We will pay special attention to typical hurdles encountered in deploying (generative) AI models at scale. Python knowledge is all you need for following along!","Deployment at scale doesn't have to be difficult. Participants will learn how to deploy a generative AI model to the cloud themselves, and how to select the right hardware for your use case (CPU,GPU,IPU etc.).",Python knowledge and a basic understanding of AI/ML models,Workshop,4.0,Deployment and integration,"Generative AI, Deployment at scale, UbiOps","Anouk is the Product Owner at UbiOps. She studied Nanobiology and Computer Science at the Delft University of Technology, and did a Master’s in Game Development at Falmouth University, which spiked her interest in Machine Learning. Next to her role at UbiOps, she frequently writes for Towards Data Science about various MLOps topics and she co-hosts the biggest Dutch data podcast, de Dataloog. Her efforts in tech have been awarded twice with the T500 award, in both 2020 and 2021",https://www.youtube.com/watch?v=BvA9b39a_ag,,Experiences and use cases from our users,,BvA9b39a_ag
Madhav Singhal,Replit,AI Engineer and Researcher,Transitioning from LLMs to Autonomous Agents in Programming and Software Engineering,A technical talk discussing the evolution of LLMs into Agents as applied to programming and software engineering.,"How LLMs for programming and software development have evolved into Agent systems and how they are built.

Will cover:
- The evolution of technical approaches from completion to FIM to tools to action models to end-to-end in-context learning driven agents
- Technical details on data, post-training and in-context learning for turning LLMs into agents for software engineering. (will cover https://blog.replit.com/code-repair as a case study)
- Evaluation of programming and software engineering Agents and use cases
- Insights and learnings from training, post-training, and building Agents in production at Replit for 20M+ users.",,Research or Advanced Technical,5.0,"Model dev, training, arch.","Evolution of LLMs, Autonomous Agents, In-Context Learning
","Madhav has been a founding member of AI Team at Replit, researching LLMs for code and designing AI agents and systems for Replit's 20M+ users.",https://www.youtube.com/watch?v=PmFZpVgF4zo,"Information Technology & Service, Computer Software
",,TMLS 2024,PmFZpVgF4zo
Vinay Kumar Sankarapu,Arya.ai,Founder and CEO,From Black Box to Mission Critical: Implementing Advanced AI Explainability and Alignment in FSIs,"In highly regulated industries like FSIs, there are more stringent policies regarding the use of 'ML Models' in production. To gain acceptance from all stakeholders, multiple additional criteria are required in addition to model performance.

This workshop will discuss the challenges of deploying ML and the stakeholders' requirements in FSIs. We will review the sample setup in use cases like claim fraud monitoring and health claim processing, along with the case study details of model performance and MLOps architecture iterations.

The workshop will also discuss the AryaXAI MLObservability competition specifications and launch details"," In this workshop, you will gain a comprehensive understanding of the expectations of FSIs while deploying machine learning models. We'll explore the additional criteria beyond model performance essential for gaining acceptance from various stakeholders, including compliance officers, risk managers, and business leaders. We'll delve into how AI explainability outputs must be iterated for multiple stakeholders and how alignment is implemented through real-world case studies in claim fraud monitoring and health claim processing. You'll also gain insights into why the iterative process of developing MLOps architectures is needed to meet performance and compliance requirements.",,Virtual Workshop,4.0,"Ethics, governance compliance ","AI explainability, Compliance, MLOps architecture","Vinay Kumar Sankarapu is the Founder and CEO of Arya.ai. He did his Bachelor's and Master's in Mechanical Engineering at IIT Bombay with research in Deep Learning and published his thesis on CNNs in manufacturing. He started Arya.ai in 2013, one of the first deep learning startups, along with Deekshith, while finishing his Master's at IIT Bombay.

He co-authored a patent for designing a new explainability technique for deep learning and implementing it in underwriting in FSIs. He also authored a paper on AI technical debt in FSIs. He wrote multiple guest articles on ‘Responsible AI’, ‘AI usage risks in FSIs’. He presented multiple technical and industry presentations globally - Nvidia GTC (SF & Mumbai), ReWork (SF & London), Cypher (Bangalore), Nasscom(Bangalore), TEDx (Mumbai) etc. He was the youngest member of ‘AI task force’ set up by the Indian Commerce and Ministry in 2017 to provide inputs on policy and to support AI adoption as part of Industry 4.0. He was listed in Forbes Asia 30-Under-30 under the technology section.",https://www.youtube.com/watch?v=s-MdybrfNhw,"Banking & Financial Services, Insurance","This session provides targeted insights for professionals in this sector by providing direct input on AI solutions, specifically on the regulatory landscape of FSIs. Unlike general knowledge, we take a stakeholder-centric approach, emphasizing the importance of meeting the diverse requirements of multiple stakeholders—a critical but often overlooked aspect of machine learning deployment in regulated industries. By examining detailed case studies in claim fraud monitoring and health claim processing, we offer practical knowledge you can apply directly to your work. 

You'll also learn how a production ML Solution could fail and how to manage model risk. You'll leave with concrete strategies and best practices for deploying compliant, explainable, and effective ML models in your organization. Moreover, the session encourages interactive discussions about common challenges and solutions, fostering a collaborative environment for knowledge exchange among professionals facing similar obstacles.",MLOps & GenAI World 2024,s-MdybrfNhw
Norm Zhou,"Stealthmode Startup
Meta (No longer working here)","Founding engineer, Engineering Manager",Beyond the Kaggle Paradigm: Future of End-to-End ML Platforms,ML platforms help enable intelligent data-driven applications and maintain them with limited engineering effort. However the current approaches to ML system building is limited by the “Kaggle Paradigm” which focuses on the data to model transformation and the operationalizing the deployment of models into applications. This model centric view limits further increase engineering productivity for future ML Systems. We propose an alternative policy centric view as an alternative to model centric view. This policy centric view involves two major additions to model centric view. First is a fully managed unified data collection system extending upstream to establish a “full chain of data custody”. Second we propose downstream extension to A/B testing systems which will bridge online offline mismatch typically experienced in many ML practitioners. Together these approaches enable a fully end-to-end automation allowing for a future ML platform to directly improve business metrics and more fluently address changing business needs.,Connecting ML to Business Impact. Short comings of a model first approach and an proposed alternative.,,Business Strategy,4.0,Deployment and integration,"ML platforms, policy-centric approach, end-to-end automation","As a Founding Engineer at an early-stage AI startup currently in stealth mode, I focus on creating innovative technological solutions. I earned a Master's Degree in Computer Architecture from the University of California, Berkeley. Before taking on my current role, I was an Engineering Manager at Meta, where I led teams developing key AI platforms such as Looper, FBLearner Flow, and AutoML. My earlier experiences include working on LinkedIn's Ads team and contributing to semiconductor startups Tabula and Ambarella, where I worked on chip architectures. Throughout my career, I've been committed to lowering the barriers to technology access for developers at all levels. Guided by first principles thinking, I approach complex challenges holistically, from applied research to delivering real business impact.",https://www.youtube.com/watch?v=0TdJrBfxI6E,,,MLOps & GenAI World 2023,0TdJrBfxI6E
Vinit Dhatrak,DocuSign,Lead Software Engineer,Revolutionizing Cloud Storage: From Petabytes to Intelligence," In an era driven by exponential data growth and the need for intelligent insights, cloud storage solutions must evolve to meet the dynamic demands of modern enterprises. This talk delves into the intricate process of migrating traditional on-premises blob storage systems to cutting-edge cloud platforms like Azure while integrating AI-powered insights to enhance cloud software offerings.

We will explore the architecture and implementation challenges faced while leading the Blob Storage team at DocuSign, focusing on how AI technologies were harnessed to transform data management within the Intelligent Agreement Management (IAM) platform. This initiative did not just facilitate a seamless transition but also pioneered a new category in cloud software, significantly bolstering market leadership.

Attendees will gain insights into optimizing resource utilization, achieving cost efficiency, and ensuring scalability in cloud migrations, drawing from a successful case implementing an intelligence-enabled cloud ecosystem. Furthermore, the talk will illuminate how AI and machine learning models were leveraged to provide actionable insights, assisting in strategic decision-making and enhancing user engagement.

The session will cover critical lessons learned, including identity and data security in cloud transformations, effective use of REST APIs for integration, and the deployment of microservices for agile and scalable services. Participants will leave equipped with advanced strategies to align their cloud migration efforts with organizational goals, optimize resources, and drive innovation through AI. Join us as we explore the convergence of cloud and artificial intelligence, unlocking new potentials in data storage solutions."," You'll learn how to migrate on-premises blob storage to cloud platforms like Azure, focusing on DocuSign's experience. We'll explore the architectural and implementation challenges, highlighting how AI-powered insights were integrated into the Intelligent Agreement Management (IAM) platform. The talk will cover optimizing resource utilization and achieving cost efficiency during cloud migrations, using successful case studies. you'll gain practical strategies for aligning cloud migrations with organizational goals, fostering innovation through AI-driven insights, and maximizing user engagement.",,Advanced Technical/Research,6.0,Deployment and integration,"Cloud migration, AI-powered insights, Blob storage","Vinit is a seasoned software engineer with a demonstrated history of building on-premise and cloud-native distributed systems at scale. Currently, Vinit serves as a Lead Software Engineer at DocuSign, contributing to the Docusign's Storage team. With expertise encompassing cloud storage, distributed systems, and virtualization technologies such as Kubernetes, Docker, and the Linux Kernel, Vinit stands out as a thought leader in the tech industry. Throughout his career, Vinit has held pivotal roles at notable companies like Google, Box, Commvault, and Marvell, where he played an instrumental role in developing highly scalable and distributed cloud storage solutions. His proficiency in object-oriented design and systems programming, coupled with his capability to scale infrastructures to handle concurrent requests and planet-scale storage, positions him as a true expert in his field. Vinit is an alumnus of the Georgia Institute of Technology, where he earned a Master's degree in Computer Science. His technical acumen and leadership capabilities are evident in his ability to mentor peers and collaborate effectively with industry leaders. Recognized for his impactful contributions, Vinit frequently engages with the tech community, participating in conferences. His dedication to advancing technological solutions is not only evident in his professional experiences but also in his commitment to ongoing learning and development exemplified by his educational achievements and professional accomplishments. Stay connected with Vinit through his LinkedIn profile to gain insights from his extensive knowledge of scalable design and distributed systems, as he continues to innovate and lead in the ever-evolving landscape of technology. (https://www.linkedin.com/in/vinit-dhatrak/)",https://www.youtube.com/watch?v=cEcGoJrTptA,Information Technology Services,"My session is unique because it's a real-world case study of migrating blob storage to the cloud, specifically integrating AI-powered insights into the process. This isn't just theoretical; it's a practical example of how DocuSign transformed their data management and achieved significant market leadership through this migration. We'll delve into the specific challenges and solutions encountered, offering actionable strategies applicable to your own cloud migration projects. This practical, case-study-driven approach sets it apart from theoretical presentations.",MLOps & GenAI World 2024,cEcGoJrTptA
"Ville Tuulos, Eddie Mattia",Outerbounds,"Co-Founder, Data scientist",Building a Production-Grade Document Understanding System with LLMs,"LLMs can be used to process troves of unstructured text automatically, e.g. to discover patterns, summarize and classify content, and enhance existing ML models through embeddings.

In this workshop, we will build a realistic document understanding system that reads live, large-scale data continuously from a data warehouse, queries state-of-the-art LLMs (cost-) efficiently, and uses the results to power various use cases.

The system is powered by open-source Metaflow and open models, so you can apply the blueprint easily in your own environment.",You will learn how to build and operate a realistic document understanding system powered by state-of-the-art LLMs.,Basic knowledge of Python,Workshop,3.0,"Model dev, training, arch.","Document understanding system, large language models (LLMs), Metaflow.






","Ville: Ville Tuulos is a co-founder and CEO of Outerbounds, a developer-friendly ML/AI platform. He has been developing infrastructure for ML and AI for over two decades in academia and as a leader at a number of companies. At Netflix, he led the ML infrastructure team that created Metaflow, a popular open-source, human-centric foundation for ML/AI systems. He is also the author of a book, Effective Data Science Infrastructure, published by Manning.

Eddie Mattia is a data scientist with a background in applied math, and experience working in a variety of customer-facing and R&D roles. He currently works at Outerbounds to help customers and open-source practitioners build machine-learning systems and products.

Eddie: Building AI developer tools and many applications on top of them!
",https://www.youtube.com/watch?v=-7VqpHcffYo,"Banking & Financial Services, Hospital & Health Care, Insurance, Marketing & Advertising, Environmental Services
",,TMLS 2024,-7VqpHcffYo
"En-Shiun Annie Lee, David Anugraha, Kosei Uemura, Jeremy Bradbury",OntarioTech University and University of Toronto,"Assistant Professor (Annie), Professor, Ontario Tech University (Jeremy), Undergraduate at the University of Toronto (David & Kosei)",ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models,"Performance prediction is a method to estimate the performance of Language Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating computational costs associated with model capacity and data for fine-tuning. Our paper introduces ProxyLM, a scalable framework for predicting LM performance using proxy models in multilingual tasks. These proxy models act as surrogates, approximating the performance of the LM of interest. By leveraging proxy models, ProxyLM significantly reduces computational overhead on task evaluations, achieving up to a 37.08× speedup compared to traditional methods, even with our smallest proxy models. Additionally, our methodology showcases adaptability to previously unseen languages in pre-trained LMs, outperforming the state-of-the-art performance by 1.89× as measured by root-mean-square error (RMSE). This framework streamlines model selection, enabling efficient deployment and iterative LM enhancements without extensive computational resources.",It may be worth exploring the usage of smaller and cheaper to fine-tune language models to gauge the performance of bigger and more expensive language models.,,Research or Advanced Technical,3.0,"Model dev, training, arch.","ProxyLM, Performance Prediction, Multilingual Tasks.
","Annie En-Shiun Lee is an assistant professor at OntarioTech University and the University of Toronto (status-only). Her goal is to make language technology as inclusive and accessible to as many people as possible. She runs the Lee Langauge Lab (L^3) with research focusing on language diversity and multilinguiality. Professor Lee’s research has been published in Nature Digital Medicine, ACM Computing Survey, ACL, SIGCSE, IEEE TKDE, and Bioinformatics. She serves as the demo co-chair for NAACL and has extensive experience transferring technology to industry. Previously she was an assistant professor (teaching stream) at the University of Toronto. She received her PhD from the University of Waterloo and was a visiting researcher at the Fields Institute and Chinese University of Hong Kong as well as worked as a research scientist in industry at VerticalScope and Stradigi AI.

David Anugraha is an undergraduate researcher at the University of Toronto, where he has focused on developing efficient methods for low-resource languages and multilinguality under the guidance of Assistant Professor En-Shiun Annie Lee. He has also assisted  Assistant Professor Maryam Dehnavi in investigating methods around large language models compression.

Kosei Uemura is an NLP student researcher at the University of Toronto, specializing in low-resource languages and cross-lingual transfer learning. He has developed state-of-the-art models in African languages with a model size of 7b. With experience in training LLMs from scratch at UTokyo's Matsuo-Iwasawa Lab and fine-tuning large language models at Spiral.AI, specializing in personality injection, Kosei is dedicated to advancing AI capabilities.
",https://www.youtube.com/watch?v=4YTa1jPxYUM,Other,,,4YTa1jPxYUM
Krishnachaitanya Gogineni,Observe.AI,Principal ML Engineer,Generative AI Design Patterns,"In this presentation, we delve into the expansive world of generative AI design patterns, selecting five pivotal examples to explore in depth: Retrieval Augmented Generation (RAG), Cluster Pulse, State Based Agents, Guard Rails, and Auto-Prompting. These patterns represent a subset of the broader spectrum of generative AI techniques, each offering unique insights into how we can enhance the capabilities and safety of AI systems. RAG provides a method for enriching AI responses with external data, Cluster Pulse fosters creativity in AI outputs, State Based Agents ensure AI actions are aligned with specific objectives, Guard Rails establish boundaries for AI behavior, and Auto-Prompting facilitates more dynamic and context-aware interactions with AI models.

The application of these patterns is demonstrated through the development of the Personalized K-8 Tutor, a project that showcases the synergistic potential of combining multiple generative AI design patterns. This educational tool leverages the strengths of each pattern to create a customized learning experience that adapts to the unique needs and preferences of individual students. By focusing on these five patterns, the presentation aims to provide attendees with a clear understanding of how generative AI can be harnessed to create innovative and impactful solutions, while also highlighting the vast array of other patterns waiting to be explored in the field of generative AI."," Understanding of three critical generative AI design patterns: Retrieval Augmented Generation (RAG) for enhancing AI responses with external information, State Based Agent for managing AI behavior, and Cluster Pulse for fostering AI creativity.
Insight into the practical application of these design patterns in building intelligent and adaptive AI systems.
Hands-on experience in integrating these patterns into a comprehensive project, the Personalized K-8 Tutor, showcasing their potential to revolutionize educational technology.
Appreciation of the importance of design patterns in structuring and optimizing generative AI solutions for real-world challenges.
Knowledge of how to leverage generative AI to create innovative, user-centric applications that push the boundaries of traditional software engineering.",,Research or Advanced Technical,3.0,Introduction to MLOps and GenAI,"Generative AI, design patterns, Retrieval Augmented Generation (RAG).






","Krishna Gogineni is a Principal Engineer at Observe.AI, leading the company's Generative AI stack. He specializes in integrating and productionizing large language models and other advanced architectures to solve product use cases, expertly balancing accuracy/quality with cost/latency. With a solid background in platform engineering and machine learning, Krishna excels in applying state-of-the-art research to industry use cases at scale, ensuring economic viability. Outside of work, he enjoys writing, attending local hackathons and startup events.",https://www.youtube.com/watch?v=mXki_lLKogM,,,TMLS 2024,mXki_lLKogM
Sophia Yang,Anaconda,Senior Data Scientist,PyScript for data science,"Are you a data scientist or a developer who mostly uses Python? Are you jealous of developers who write Javascript code and build fancy websites in a browser? How nice would it be if we can write websites in Python? PyScript makes it possible! PyScript allows users to write Python in the browser. In this talk, I will introduce PyScript and discuss what does PyScript mean for data scientists, how PyScript might change the way data scientists work, and how PyScript can be incorporated into the data science workflow.",Use PyScript to run Python in Your HTML,,Technical,4.0,Introduction to MLOps and GenAI,,"Sophia Yang is a Senior Data Scientist at Anaconda, Inc., where she uses data science to facilitate decision-making for various departments across the company. She volunteers as a Project Incubator at NumFOCUS to help Open Source Scientific projects grow. She is also the author of multiple Python open-source libraries such as condastats, cranlogs, PyPowerUp, intake-stripe, and intake-salesforce. She holds an M.S. in Statistics and Ph.D. in Educational Psychology from The University of Texas at Austin.",https://www.youtube.com/watch?v=dcKwdZ6Aijs,Information Technology & Service,,,dcKwdZ6Aijs
Aniket Maurya,Lightning AI,Developer Advocate,AI Agents with Function Calling/Tool Use,Learn about Agentic workflows with LLM tool use. Generate structured JSON output and execute external tools/functions.,By the end of this workshop you will learn how to build AI Agents and make use of function calling with OpenAI and open-source LLMs,Python and LLM fundamentals,Workshop,4.0,"Model dev, training, arch.","AI agents, function calling, LLM tool use.","I’m Aniket, a Machine Learning - Software Engineer with with over 4 years of experience, demonstrating a strong track record in developing and deploying machine learning models to production.",https://www.youtube.com/watch?v=46x7WbaSgqs,"Computer Software
",,TMLS 2024,46x7WbaSgqs
Marcelo Litovsky,Aporia,Director of Sales Engineering,The Missing Piece of MLOps,"MLOps is bringing a lot of attention to the business impact of Machine Learning. It also introduces new challenges that cannot be efficiently addressed with DevOps. What are these challenges, and what makes MLOps so different from DevOps? They both deal with the life cycle of an application, so what is the difference? Most software applications have a pre-defined behavior. We know the data going in, and we know the data going out. Anything not matching a predefined format or schema is a problem. Machine Learning models follow the same pattern to operate, but their value diminishes as the content of the data changes. We are looking at the schema, format, and patterns describing a change in the data. This is the big difference between DevOps and MLOps, observing the data. 

Most organizations have focused on the simplification, automation, and scalability of Machine Learning applications. Observability has taken a back seat. This session will explore the steps you can take to prepare for ML observability. We will also discuss how observability helps data scientists and MLOps practitioners showcase the business value of the applications they deploy and get recognition for their hard work.
",This session will explore the steps you can take to prepare for ML observability. We will also discuss how observability helps data scientists and MLOps practitioners showcase the business value of the applications they deploy and get recognition for their hard work.,,Workshop,4.0,Performance optimization and efficiency,,"Marcelo Litovsky is an experienced Information Technology professional with 30 years of diverse background in Enterprise Architecture, AI, Systems and Database Management, and Programming. He has worked in multiple industries: Financial Services, Entertainment, and Information Technology in his career. Today, he serves as Director of Sales Engineering at Aporia, bringing his expertise to help Data Scientists, Machine Learning Engineers, and Business Users work together to unlock and promote the business value of their machine learning models. You can find him at the gym, preparing healthy vegan meals when he is not talking to customers or writing Python code.",https://www.youtube.com/watch?v=LCQy1Y4f6pw,,,TMLS 2022,LCQy1Y4f6pw
Anish  Shah,Weights & Biases,ML Engineer,Investigating the Evolution of Evaluation from Model Training to GenAI inference,"This session explores the evolution of evaluation techniques in machine learning, from traditional model training through fine-tuning to the current challenges of assessing large language models (LLMs) and generative AI systems. We'll trace the progression from simple metrics like accuracy and F1 score to sophisticated automated evaluation systems that can generate criteria and assertions. The session will culminate in an in-depth look at cutting-edge approaches like EvalGen, which use LLMs to assist in creating aligned evaluation criteria while addressing phenomena like criteria drift."," Attendees will gain a comprehensive understanding of evaluation techniques across different ML paradigms, from cross-validation in traditional training to the nuances of evaluating fine-tuned models and LLMs. You'll learn practical approaches for automating evaluation criteria and assertions, strategies for aligning these automated evaluations with human judgments, and techniques for handling the unique challenges posed by generative AI, such as criteria drift and the balance between human oversight and AI-assisted evaluation.",,Advanced Technical/Research,2.0,Deployment and integration,"Evaluation techniques, Generative AI, Large Language Models (LLMs)","Anish Shah is a leading expert in AI and ML at Weights & Biases, specializing in the optimization of large language models for complex applications. With extensive experience in fine-tuning and model evaluation, Anish has contributed to significant advancements in the field of AI research and presented at many conferences and events",https://www.youtube.com/watch?v=0pznirh2Ko4,"Computer Software, Information Technology Services","This session offers a rare, holistic view of AI evaluation, bridging the gap between traditional machine learning and cutting-edge generative AI. By examining the progression of evaluation techniques, attendees will gain insights into both the historical context and future directions of AI assessment. The discussion of automated evaluation systems like EvalGen provides practical, forward-looking knowledge that can be applied to real-world LLM projects, while the exploration of phenomena like criteria drift offers a nuanced understanding of the ongoing challenges in this rapidly evolving field.",MLOps & GenAI World 2024,0pznirh2Ko4
Rajiv  Shah,Hugging Face,Machine Learning Engineer,Evaluation Techniques for Large Language Models,"Large language models (LLMs) represent an exciting trend in AI, with many new commercial and open-source models released recently. However, selecting the right LLM for your needs has become increasingly complex. This tutorial provides data scientists and machine learning engineers with practical tools and best practices for evaluating and choosing LLMs.

The tutorial will cover the existing research on the capabilities of LLMs versus small traditional ML models. If an LLM is the best solution, the tutorial covers several techniques, including evaluation suites like the EleutherAI Harness, head-to-head competition approaches, and using LLMs for evaluating other LLMs. The tutorial will also touch on subtle factors that affect evaluation, including role of prompts, tokenization, and requirements for factual accuracy. Finally, a discussion of model bias and ethics will be integrated into the working examples.

Attendees will gain an in-depth understanding of LLM evaluation tradeoffs and methods. Jupyter Notebooks will provide reusable code for each technique discussed.",Ways to quickly start evaluating models,,Advanced Technical/Research,5.0,"Model dev, training, arch.","Large language models (LLMs), Model evaluation techniques, Prompt engineering
","Rajiv Shah is a machine learning engineer at Hugging Face who focuses on enabling enterprise teams to succeed with AI. Rajiv is a leading expert in the practical application of AI. Previously, he led data science enablement efforts across hundreds of data scientists at DataRobot. He was also a part of data science teams at Snorkel AI, Caterpillar, and State Farm.

Rajiv is a widely recognized speaker on AI, published over 20 research papers, been cited over 1000 times, and received over 20 patents. His recent work in AI covers topics such as sports analytics, deep learning, and interpretability.

Rajiv holds a PhD in Communications and a Juris Doctor from the University of Illinois at Urbana Champaign. While earning his degrees, he received a fellowship in Digital Government from the John F. Kennedy School of Government at Harvard University. He has recently started making short videos, @rajistics, with several million views.",https://www.youtube.com/watch?v=vqO7ks7DFZw,"Banking & Financial Services, Insurance
",,MLOps & GenAI World 2023,vqO7ks7DFZw
Eero Laaksonen,Valohai,CEO & Founder,MLOps for AgenticAI: How to manage Agents in production,"AI Agents are taking over (and for a good reason). There's infinite yet untapped potential for everyone, from enterprises to startups, working on everything, from supporting internal operations to shipping user-facing features. New players emerge specifically to offer AI Agents as a service, often catering to specific industries.  

However, very few have succeeded in getting their AI Agents to production and generating value from them. One of the main reasons is the complex infrastructure and MLOps best practices that must be in place from day one.

What You'll Learn: 
- How to build the foundation for future-proofing the success of proprietary AI Agents
- The trade offs in MLOps stacks and AI infrastructure for in the AgenticAI space
- How to manage AI Agents in production and maximize the Return-on-Investment",,,Business Strategy,3.0,Deployment and integration,"AI Agents in production, MLOps infrastructure, Return-on-Investment (ROI)",Serial entrepreneur hippie with a keen interest in making the world a better place. I believe people should work less and enjoy life more. Currently escalating the adoption of machine learning in enterprises around the world with Valohai.,https://www.youtube.com/watch?v=Glurm_ADDpU,,,MLOps & GenAI World 2024,Glurm_ADDpU
Mike  Taylor,Saxifrage,Owner,Making ChatGPT funny with Prompt Optimization,"A recent study found ChatGPT repeated the same 25 jokes 90% of the time. As is often the case with popular narratives about the limits of AI, ChatGPT is capable of so much more... you just have to know how to ask! Using principles of prompt engineering, I try to get ChatGPT to make you laugh, while arming you with techniques for overcoming similar supposed limitations, when working with AI.","AI is capable of a lot more than people realize, and they'd get better results if they learned prompt engineering.",-,,2.0,Introduction to MLOps and GenAI,"Prompt optimization, ChatGPT, AI humor","I’m a data-driven, technical marketer who built a 50 person marketing agency (Ladder), and 300k people have taken my online courses (LinkedIn, Udemy, Vexpower). I now work freelance on generative AI projects, and I’m writing a book on Prompt Engineering for (O’Reilly Media).",https://www.youtube.com/watch?v=lo6OOTlSS6A,"Banking & Financial Services, Hospital & Health Care, Information Technology & Service, Marketing & Advertising, Telecommunications
",,MLOps & GenAI World 2023,lo6OOTlSS6A
Qingyun Wu,Agmax Inc,Assistant Professor at Penn State University; Creator of AutoGen,AutoGen: Enabling Next-Gen AI Applications via Multi-Agent Conversation,"AutoGen is an open-source programming framework for agentic AI. It enables the development of AI agentic applications using multiple agents that can converse with each other to solve tasks. In this session, the speaker will provide a deep dive into the key concepts of AutoGen, demonstrate diverse applications enabled by AutoGen, and share the latest updates and ongoing efforts spanning across key directions such as evaluation, interfaces, learning/optimization/teaching, and seamless integration with existing AI technologies.","- Agentic AI and the core concepts of AutoGen as an open-source programming framework for agentic AI
- How AutoGen enables the development of AI applications using multiple conversing agents
- The architecture and key components of the AutoGen framework
- Various applications and use cases made possible by AutoGen
Recent updates and ongoing developments in the AutoGen project
- Key areas of focus in AutoGen's development.

You will gain insights into how AutoGen can be used to create advanced AI applications that leverage multi-agent conversations to solve complex tasks. You will also get a glimpse of the future directions and potential impact of this technology in the field of AI.",,case study & advanced technical,1.0,Deployment and integration,"AutoGen, Agentic AI, Multi-agent conversation
","Dr. Qingyun Wu is an Assistant Professor at the College of Information Science and Technology at Penn State University. She is a post-doc researcher in the NYC Lab of Microsoft Research 2020-2021. She got her Ph.D. in Computer Science from the University of Virginia in 2020. Qingyun received the 2019 SIGIR Best Paper Award and ICLR 2024 LLM agent workshop Best Paper Award. Qingyun is the creator and one of the core maintainers of AutoGen, a leading programming framework for agentic AI applications.",https://www.youtube.com/watch?v=WLKStss2WXQ,"Banking & Financial Services, Computer Software, Information Technology Services, Insurance, Telecommunications, Marketing & Advertising, Hospital & Health Care, Food & Beverage, Enviromental Services, Automotive","This session on AutoGen offers a unique perspective on the emerging trend of agentic AI in business applications:

- Multi-agent framework for business solutions: While many discussions on agentic AI focus on single agents, AutoGen introduces a framework for creating multi-agent systems. This approach is particularly relevant for complex business scenarios that require collaboration and diverse expertise.
- Practical implementation of a trending concept: As agentic AI gains traction in various industries, this session provides concrete tools and methods to implement these concepts using the AutoGen framework. It bridges the gap between theoretical trends and practical business applications.
- Open-source advantage for businesses: AutoGen's open-source nature offers a unique opportunity for businesses to adopt, customize, and scale agentic AI solutions without being locked into proprietary systems. This can be especially appealing for companies looking to innovate while controlling costs and maintaining flexibility.
- Real-world use cases: The session likely showcases diverse business applications of AutoGen, demonstrating how multi-agent AI can solve real-world problems across different industries. This practical focus sets it apart from more theoretical discussions on agentic AI.
- Integration with existing business technologies: By addressing how AutoGen can be integrated with current AI technologies, the session offers insights on how businesses can evolve their existing AI infrastructure towards more advanced agentic systems.
- Future-proofing business AI strategies: With its focus on the latest developments and future directions, this session provides valuable insights for businesses looking to stay ahead in the rapidly evolving AI landscape.

This unique combination of practical tools, business-relevant applications, and forward-looking insights makes the session particularly valuable for attendees interested in leveraging the trending concept of agentic AI for business innovation and problem-solving.",MLOps & GenAI World 2024,WLKStss2WXQ
Bo Chang,Google Brain,Software Engineer,Latent User Intent Modeling in Recommender Systems,"The current sequential recommender systems mainly rely on users’ item-level interaction history to capture topical interests and lacks a high-level understanding of user intent. It is challenging to explicitly define and enumerate all possible user intents. We propose to use latent variable models to capture user intents as latent variables through encoding and decoding user behavior signals, with an application to a large industrial recommender system.",How to better model user intent in recommender systems using a latent variable model.,,Advanced Technical/ Research,7.0,"Model dev, training, arch.",,"Bo Chang is a software engineer at Google Brain, based in Toronto, Canada. Prior to that, he was a machine learning researcher at Borealis AI. He finished his Ph.D. in statistics at the University of British Columbia.",https://www.youtube.com/watch?v=PMUoygO5S3Q,,Noval ML techniqies,,PMUoygO5S3Q
Chip Huyen,Claypot AI,CEO,Real-time Machine Learning: Architecture and Challenges,Fresh data beats stale data for machine learning applications. This talk discusses the value of fresh data as well as different types of architecture and challenges of online prediction.,Fresh data beats stale data for machine learning applications,,Technical,5.0,"Model dev, training, arch.",,"Chip Huyen is a co-founder of Claypot AI, a platform for real-time machine learning. Previously, she was with Snorkel AI and NVIDIA. She teaches CS 329S: Machine Learning Systems Design at Stanford. She’s the author of Designing Machine Learning Systems, an Amazon bestseller in AI. She has also written four bestselling Vietnamese books.",https://www.youtube.com/watch?v=S4A8QWN1G7s,,,TMLS 2022,S4A8QWN1G7s
Tibor Mach,DVC,Machine Learning Solutions Engineer,Applying GitOps principles at every step of an E2E MLOps project - an interactive workshop,"With the emergence of IaC (infrastructure as code) tools, we have seen GitOps become an increasingly popular DevOps pattern that facilitates automation, reproducibility, and security. While hugely beneficial, applying the same principles in MLOps is not straightforward due to the specific aspects of the field such as the need to work with large amounts of data and the experimental nature of ML development. In this talk, we will see how we can bridge these gaps by using tools such as DVC. Step by step, we will create an end-to-end MLOps pipeline which is centered around the git repository as its single source of truth.","In this largely interactive workshop you can learn how you can use your git repositories to keep track of your ML experiments, version data and models, maintain a model registry and handle model deployment",Basics of working with git and conceptual understanding of GitHub Actions or GitLab CI.,Workshop,4.0,Deployment and integration,"GitOps, MLOps, DVC





",Tibor Mach is a Machine Learning Solutions Engineer at Iterative.ai He has been working in ML and MLOps in the past 5 years. Tibor has a Ph.D in mathematics from the University of Göttingen and had published papers in the field of probability theory prior to refocusing to ML.,https://www.youtube.com/watch?v=J3vUMwG8dks,,"The talk brings together unique insights from creators of popular data versioning tool DVC with first-hand experience in navigating the challenges of data versioning in generative AI. We will focus on basic principles and user stories instead of DVC or other tools. Moreover, the talk will foster dialogue about recent changes in the field, providing fresh perspectives on the transformations generative AI has introduced to data versioning.
",MLOps & GenAI World 2023,J3vUMwG8dks
Arpita Vats,LinkedIn,Senior AI Engineer,LLMs in Vision Models,"The integration of Large Language Models (LLMs) in vision-based AI systems has sparked a new frontier in multimedia understanding. Traditional vision models, while powerful, often lack the ability to comprehend contextual information beyond visual features. By incorporating LLMs, vision models can process both visual and textual information, creating a more holistic and interpretable understanding of multimedia content. This presentation will explore the convergence of LLMs with vision models, highlighting their application in image captioning, object recognition, and multimodal recommendation systems."," By attending this presentation, the audience will learn how Large Language Models (LLMs) can enhance the capabilities of vision-based AI systems, creating more context-aware and interpretable multimedia models. Attendees will gain insights into the architecture and integration techniques used to combine vision and language models, practical industry applications, and the challenges and solutions associated with building these advanced systems. They will leave with a deeper understanding of how LLMs in vision models are transforming multimedia analysis, enabling more accurate, scalable, and personalized AI-driven solutions.",,Workshop,5.0,"Model dev, training, arch.","LLMs in vision, Multimodal integration, Context-aware AI","I am a Senior AI Engineer at LinkedIn with expertise in AI, Deep Learning, NLP, and Computer Vision. I have experience from Meta and Amazon, where I focused on LLM and Generative AI. I have published papers and led projects enhancing recommendation algorithms and multimedia models for various industry applications.",https://www.youtube.com/watch?v=4uF0ugpSmpk,"Computer Software, Hospital & Health Care","This session will focus on the practical integration of Large Language Models (LLMs) with vision-based AI systems, bridging the gap between theory and real-world application. Unlike traditional sessions that may focus solely on either vision or language models, this session dives into the convergence of both, demonstrating how the fusion of these technologies can create powerful, context-aware multimedia models. I will present real-world case studies and architectures that show how this integration improves AI-driven tasks such as image captioning, object recognition, and personalized recommendations. Attendees will gain actionable insights and strategies for implementing these advanced models in industry settings, making the session both technically enriching and highly applicable.",MLOps & GenAI World 2024,4uF0ugpSmpk
Greg Kuhlmann,Sumatra,CEO,Learning from Extremes: What Fraud-Fighting at Scale Can Teach Us About MLOps Across Domains,"The engineers behind large-scale anti-fraud platforms, faced with extreme demands for low-latency inference, feature freshness, and agile redeployment, have been the quiet pioneers at the cutting edge of MLOps. One might assume the architectures and practices developed for these intense problems would be overkill in less operationally-demanding domains. However, we will challenge this assumption, and discuss how the real-time-first approach taken by these systems actually simplifies architectures by eliminating many complex pipelines. Further, we'll show how the observability and replay technlogies developed to respond quickly to unpredictable attacks can be applied broadly to make ML teams more agile across the board.","- The canonical architecture for modern, large-scale, real-time fraud prevention systems
- A comparison of the ""real-time-first"" vs. ""make-batch-faster"" approaches
- How log-time denormalization, unified online/offline feature transformation engines, and backfill on demand, are the keys to rapidly deploying model improvements in non-stationary domains",-,,5.0,Deployment and integration,"Fraud prevention systems, Real-time MLOps, Observability and replay technologies
","Greg Kuhlmann is Co-founder and CEO of Sumatra, a realtime customer data platform that helps growth teams optimize conversions through on-site personalization and recommendations. He formerly led data science teams for the App Store and Apple Pay. He holds a PhD in machine learning from UT Austin.",https://www.youtube.com/watch?v=6gm7h0wS_qg,"Banking & Financial Services, Information Technology & Service, Insurance, Marketing & Advertising
","During my 8+ years at Apple, I helped build a realtime ML platform that was conceived for fraud alone, across the app store, online store, retail, and more. Today, that platform powers everything from web personalization to logistics. Now at my startup, we're applying the same approach to recommender systems, funnel optimization, and more non-fraud applications. I believe my experience bridging the fraud to non-fraud gap offers a unique perspective.
",MLOps & GenAI World 2023,6gm7h0wS_qg
Gon Rappaport,Aporia,Solution Architect,LLMidas' Touch; Safely adopting GenAI for production use-cases,"During the session, we'll explore the challenges of adopting GenAI in production use-cases. Through focus on the goal of using language models to solve more dynamic problems, we'll address the dangers of ""No-man's-prod"" and provide insights into safe and successful adoption. This presentation is designed for engineers, product managers and stakeholders and aims to provide a roadmap to release the first GenAI applications safely and successfully to production."," * Become familiar with the potential issues of using generative AI in production applications
* Learn how to mitigate the dangers of AI applications
* Learn how to measure the performance of different AI application types",,Virtual Workshop,3.0,Deployment and integration,"GenAI adoption, Production use-cases, Safety"," I'm a solution architect at Aporia. I joined just over two years ago. I've spent over eight years in the tech industry, starting from low-level programming and cybersecurity and transitioning to AI&ML.",https://www.youtube.com/watch?v=A3KschpEU_g,"Computer Software, Information Technology Services",Generative AI is still in its infant stage and not many companies have managed to harness its power in production. This session will cover most things to consider to ship such apps and evaluate their success.,MLOps & GenAI World 2024,A3KschpEU_g
Alison Cossette,Neo4j,Developer Advocate,"Driving GenAI Success in Production: Proven Approaches for Data Quality, Context, and Logging"," Generative AI is a part of our every day work now, but folks are still struggling to realize business value in production.

Key Themes:

Methodical Precision in Data Quality and Dataset Construction for RAG Excellence: Uncover an integrated methodology for refining, curating, and constructing datasets that form the bedrock of transformative GenAI applications. Specifically, focus on the six key aspects crucial for Retrieval-Augmented Generation (RAG) excellence.

Navigating Non-Semantic Context with Awareness: Explore the infusion of non-semantic context through graph databases while understanding the nuanced limitations of the Cosine Similarity distance metric. Recognize its constraints in certain contexts and the importance of informed selection in the quest for enhanced data richness.

The Logging Imperative: Recognize the strategic significance of logging in the GenAI landscape. From application health to profound business insights, discover how meticulous logging practices unlock valuable information and contribute to strategic decision-making.

Key Takeaways:

6 Requirements for GenAI Data Quality

Adding non-semantic context, including an awareness of limitations in distance metrics like Cosine Similarity.

The strategic significance of logging for application health and insightful business analytics.

Join us on this methodologically rich exploration, ""Beyond Vectors,"" engineered to take your GenAI practices beyond the current Vector Database norms, unlocking a new frontier in GenAI evolution with transformative tools and methods!",,,Advanced Technical/Research,2.0,Deployment and integration,"GenAI data quality, Non-semantic context, Logging for business insights





","Alison Cossette is a dynamic Data Science Strategist, Educator, and Podcast Host. As a Developer Advocate at Neo4j specializing in Graph Data Science, she brings a wealth of expertise to the field. With her strong technical background and exceptional communication skills, Alison bridges the gap between complex data science concepts and practical applications. Alison’s passion for responsible AI shines through in her work. She actively promotes ethical and transparent AI practices and believes in the transformative potential of responsible AI for industries and society. Through her engagements with industry professionals, policymakers, and the public, she advocates for the responsible development and deployment of AI technologies. She is currently a Volunteer Member of the US Department of Commerce - National Institute of Standards and Technology's Generative AI Public Working Group Alison’s academic journey includes Masters of Science in Data Science studies, specializing in Artificial Intelligence, at Northwestern University and research with Stanford University Human-Computer Interaction Crowd Research Collective. Alison combines academic knowledge with real-world experience. She leverages this expertise to educate and empower individuals and organizations in the field of data science. Overall, Alison Cossette’s multifaceted background, commitment to responsible AI, and expertise in data science make her a respected figure in the field. Through her role as a Developer Advocate at Neo4j and her podcast, she continues to drive innovation, education, and responsible practices in the exciting realm of data science and AI.",https://www.youtube.com/watch?v=FrpAgeJNgbM,Information Technology Services,,MLOps & GenAI World 2024,FrpAgeJNgbM
"Yizhi Yin, PhD",Neo4j,Sr. Solutions Engineer,Enabling GenAI Breakthroughs with Knowledge Graphs,"Join us for an immersive workshop to explore integrating Large Language Models (LLMs) with knowledge graphs using Neo4j, a leader in graph database and graph analytics, with a Retrieval Augmented Generation (RAG) approach. 

RAG has become the industry standard to overcome LLM limitations like reliance on generic data and lack of enterprise specific information. 

Neo4j's dynamic and interconnected data structure makes it ideal for this, enabling accurate and contextually rich responses. Neo4j’s graph data science algorithms provide additional insights into the knowledge graph, enhancing its capacity to derive new relationships and uncover hidden patterns.

This hands-on session will guide you through building a personal messenger application for personalized product recommendations using RAG patterns. 

By combining the capabilities of LLMs with Neo4j knowledge graph and graph data science, you will gain practical insights into creating sophisticated, intelligent GenAI applications tailored to your enterprise use case.","High level introduction of Knowledge Graph 

Connecting and exploring knowledge graph with Cypher

Implementing vector search with Neo4j

Analyzing vector search results with graph patterns

Enriching search results using graph data science methods

Next steps in GenAI application development and participate in an interactive Q&A session",Basic Python programming skills,Workshop,5.0,"Model dev, training, arch.","Knowledge Graphs, Retrieval Augmented Generation (RAG), Neo4j






",,https://www.youtube.com/watch?v=4EW4QbMgYkI,,,TMLS 2024,4EW4QbMgYkI
"Sasha Lucconi, Monish Gandhi, Deval Pandya","Hugging Face, Gradient Ascent Inc, Vector Institute","AI and Climate Leader, Founder, Vice President of AI Engineering",Connecting the Dots Between AI Ethics and Sustainability,"AI ethics and sustainability considerations have typically been considered separately : work that aims to estimate the carbon footprint of AI models does not typically address their contribution towards shifting the balance of power and amplifying inequalities, and that which aims to evaluate the societal impacts of AI models focuses on aspects such as bias and fairness consistently overlooks their water and energy use. In this panel, we will discuss how the two subjects are related and intertwined, especially in the context of generative AI technologies, which come with many challenges in terms of ethics and the environment.","- Key ethical challenges in AI (bias, fairness, representativity, copyright)
- Environmental impacts of AI (energy, water, natural resources)
- Current state of the art in research on both 
- How to make informed trade-offs between potential benefits of (generative) AI technologies while remaining cognizant of their ethical and environmental impacts",,Business Strategy or Ethics,2.0,"Ethics, governance compliance ","AI Ethics, Sustainability, Generative AI
","Dr. Sasha Luccioni is the AI & Climate Lead at Hugging Face, a global startup in responsible open-source AI, where she works on creating tools and techniques for quantifying AI’s societal and environmental costs. She is also a founding member of Climate Change AI (CCAI) and a board member of Women in Machine Learning (WiML).

Monish: Monish has been passionate about machine learning, AI, and model development for a long time: over ten years ago he built a computer vision based system that could help players win at pool billiards. He has also built models for everything from airplane landing gear systems to crowd behaviour. Over the past few years, he's worked on almost 60 ML/AI projects and brought this passion and experience to help businesses thrive in the coming AI-powered world.

Monish founded Gradient Ascent (GA) - a trusted provider of AI products, services, and solutions for non-AI companies within financial services, technology, industrial, and other sectors. GA has also made investments in AI businesses and is a member of AngelOne. Previously, he held product management, professional services, technical management, and sales roles at a number of fast growing technology companies. Monish often speaks at events and writes about the role of AI in business.

He has a masters degree in Finance and Financial Law (University of London) and an undergraduate degree in Systems Design Engineering (with Dean’s Honours) from University of Waterloo. In his free time, he loves to read, cook, and play tennis. He is a Board Member at CycleTO.

Deval: Deval is the Vice President of AI Engineering at Vector Institute and is passionate about the role of digital technologies in accelerating energy transition and Energy equity, as well as about building machine learning teams and products for societal good.

He holds a Doctorate in Mechanical Engineering and a Masters in Aerospace engineering. Before joining Vector Institute, Deval was leading Data Science and Machine learning teams at Shell. While in that role, his work spanned across various domains, including predictive maintenance, GHG accounting, power value chain, nature-based solutions, biofuels, and hydrogen.

He is passionate about the role of digitalization in energy transition and was the co-founder of the Future Energy Lions network at Shell. Deval also serves as a Director on the technical steering committee of Moja Global, a not-for-profit, collaborative project that brings together a community of experts to develop open-source software under Linux Foundation used for country-level greenhouse gas accounting from the AFOLU sector.

Deval is on the task force for Digitalization in Energy at the United Nations Economic Commission of Europe (UNECE). He enjoys traveling and cooking in his free time.",https://www.youtube.com/watch?v=HGMsahbuXm8,,,TMLS 2024,HGMsahbuXm8
"Ed Shee, Ashley Scillitoe",Seldon,"Head of Developer Relations, Data Science Research Engineer",An Introduction to Drift Detection," Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. In this workshop, we will give a hands-on overview to drift detection, the discipline focused on detecting such changes. We will start by building an understanding of the ways in which drift can occur, and why it pays to detect it. We’ll then explore the anatomy of a drift detector, and learn how they can be used to detect drift in a principled manner.

You will work through a real-world example using Alibi Detect, an open-source Python library offering powerful algorithms for adversarial, outlier and drift detection.You’ll learn how to set-up drift detectors, and deduce what type of drift is occurring. Since data can take many forms, such as image, text or tabular data, you’ll explore how to use existing ML models to pre-process your data into a form suitable for drift detectors. Then, to gain further insights into the causes of drift, you’ll employ state-of-the art detectors which are able to perform fine-grained attribution to instances and features. To assess whether model performance has been affected by drift, you’ll experiment with using model uncertainty based detectors. Finally, you’ll use a novel context-aware drift detector. This takes in context (or conditioning) variables, allowing you to test for drift conditional on context that is permitted to change. We’ll discuss how this functionality can be crucial in many real-world drift detection scenarios.  

This hands-on workshop is targeted at a beginner-intermediate level. No prior knowledge or understanding of drift detection is required (we’ll be covering that) but a basic knowledge of machine learning and some experience with Python will be helpful.","What drift detection is, why it's important and how to get started.",No prior knowledge or understanding of drift detection is required (we’ll be covering that) but a basic knowledge of machine learning and some experience with Python will be helpful.,Workshop,5.0,Performance optimization and efficiency,,"With a background in cloud computing and a passion for machine learning, Ed has combined those skills and now works in the MLOps field where he heads up Developer Relations at Seldon. Organizer of Tech Ethics London and MLOps London, Ed is heavily involved in lots of developer communities and, thankfully, loves both beer and pizza.

Ashley is a data science research engineer at Seldon, where he works on developing production-ready tools for drift, adversarial and outlier detection. Prior to joining Seldon, he spent a number of years as a Research Fellow at The Alan Turing Institute. Here, he explored the use of machine learning for tackling aerospace engineering problems, with a focus on explainability and uncertainty quantification. Ashley also completed a PhD at the University of Cambridge, and is a keen proponent of open-source software.
",https://www.youtube.com/watch?v=qeHjXyN_HV4,,"Our researches have developed novel drift detection algorithms that have not previously been seen elsewhere
",TMLS 2022,qeHjXyN_HV4
Jakob Frick,RadiantAI,Co-founder & CTO,Finding the hidden drivers of AI business value,How do you know how well your AI products are actually working? In this talk we will explore how companies are looking beyond evaluations to tie LLM activity to their business outcomes. We'll look at case studies and examples of the field as well as a framework for identifying the metrics that really move the needle in creating value with Generative AI.,,,Furture of AI,,Business and stakeholder alignment,"AI business value, LLM metrics, Generative AI outcomes","Jakob Frick is the CTO and Co-founder of Radiant AI. Before that he worked at Palantir Technologies across a range of areas from Covid Vaccine distribution work with the NHS, to National-scale Cyber defense to Model integration across platforms. Before that he worked on Open Source software with JP Morgan Chase.",https://www.youtube.com/watch?v=JlyYJnFWR7k,,,MLOps & GenAI World 2024,JlyYJnFWR7k
Ian Yu,Groupby Inc,Machine Learning Engineer,The Gap From Prototype to Production: Lessons Learned from Implementing Applications with LLMs," 2023 was a good year for prototyping LLM-based applications, but 2024 is a great year for productionizing them. However, going into production, there are many unforeseen questions and challenges. These include decisions between managed solutions and custom implementations, balancing the rigour of experimentation with the speed of application development, ensuring maintainability post-deployment, and evaluating and optimizing systems. Common issues also include LLM output inconsistencies at scale that were not apparent during prototyping, tightly coupled systems that are hard to pivot or modify, unclear evaluation objectives, and misaligned product-user fit. 

In this workshop, we will address these questions and challenges at the implementation level, including:
- Strategically align LLMs with product design and application logic
- Empirical tips on designing LLM chaining
- What incorporating LLMs with non-LLMs in a system looks like
- Application maintainability post-deployment
- Discuss various touch points for build vs. buy, such as prompt versioning, orchestration, vector databases
- Trends and predictions spurred by increased focus on production work

This workshop also includes a little hands-on work to demonstrate bite-sized system",Necessary knowledge and considerations when going from LLM application prototype to production," Entry-level understanding of prompting, vector databases, and system design",Workshop,5.0,Deployment and integration,"LLM Production, System Design, Application Maintainability






",,https://www.youtube.com/watch?v=oAzY-Fyyg-4,"Information Technology & Service, Other, Computer Software
",,TMLS 2024,oAzY-Fyyg-4
"Greg Loughnane, Chris Alexiuk",AI Makerspace,"Co-Founder, Co-Founder & CTO",Building an Open-Source Agentic RAG Application with Llama,"This year, people and companies aim to build more complex LLM applications; namely, ones that leverage context and reasoning.  For applications to leverage context well, they must provide useful input to the context window, through direct prompting or search and retrieval.  To leverage reasoning is to leverage the Reasoning-Action pattern, and to be “agentic” or “agent-like.”

The tool with the largest community-building LLM applications is LangChain. LangChain v0.2, the latest version of the leading, incorporates LangGraph directly, the engine that powers stateful (and even fully autonomous) agent cycles.

In this session, we'll break down the concepts and code you need to understand and build the industry-standard agentic RAG application, from soup to nuts.","- A review of the basic prototyping patterns of GenAI, including Prompt Engineering, RAG, Fine-Tuning, and Agents

- Understand agents and agentic behavior as a pattern of reasoning and action

- The big ideas behind giving agents access to tools through [Function Calling](https://openai.com/index/function-calling-and-other-api-updates/)

- Why giving agents access to tools enables search and retrieval (e.g., RAG)

- Why you should choose specific open-source LLMs and embedding models over others

- The core ideas and constructs you’ll need to build RAG applications with LangChain

- How synthetic data can be created and evolved using the [Evol-Instruct](https://arxiv.org/abs/2304.12244) method

- How you should think about evaluating the output of RAG and Agentic systems","- Working knowledge of how to run Machine Learning Python code in Jupyter Notebooks
- Practical knowledge of how to use an interactive development environment with version control so that you can engage with our public GitHub repo.  To test yourself, complete [The AI Engineering Bootcamp Challenge](https://aimakerspace.journey.io/p/aie-challenge)",Workshop,4.0,"Model dev, training, arch.","LangChain, agentic behavior, RAG applications.






",,https://www.youtube.com/watch?v=GUSoSwrfHaM,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Marketing & Advertising, Telecommunications
",,TMLS 2024,GUSoSwrfHaM
"Kyryl Truskovskyi, Rohit Saha",Georgian,"ML Engineer, Applied Research Scientist",Transforming The Retail Industry with Transformers,"In recent years, we have seen amazing results in artificial intelligence and machine learning owing to the emergence of models such as transformers and pretrained language models. Despite the astounding results published in academic papers, there remains a lot of ambiguity and challenges when it comes to deploying these models in the industry because 1) troubleshooting, training, and maintaining these models is very time and cost consuming due to their inherent large sizes and complexities 2) there is not yet enough clarity about when the advantages and challenges of these models outweigh classical ML models. These challenges are even more severe for small and mid-sized companies that do not have access to huge compute resources and infrastructure. In this talk, we discuss these challenges and share our findings and recommendations from working on real-world examples at SPINS, a data/tech company focused on the natural grocery industry. More specifically, we describe how we leverage state-of-the-art language models to seamlessly automate parts of SPINS’ data ingestion workflow and drive substantial business outcomes. We provide a walk-through of our end-to-end MLOps system and discuss how using the right tools and methods has helped to mitigate some of these challenges. We also share our findings from our experimentation and provide insights on when one should use these massive transformer models instead of classical ML models. Considering that we have a variety of challenges in our use cases from an ill-defined label space to a huge number of classes (~86,000) and massive data imbalance, we believe our findings and recommendations can be applied to most real-world settings. We hope that the learnings from this talk can help you to solve your own problems more effectively and efficiently! ",The insights and findings we share in this talk are derived from using the latest ML techniques and tools for solving real-world use cases at SPINS and are not readily available on the internet. We also open the stage for Q&A at the end of this talk to address the questions from the audience.,,Case Study,6.0,Introduction to MLOps and GenAI,,"Kyryl has over eight years of experience in the field of Machine Learning. For the bulk of this career, he has helped build machine learning startups, from inception to a product. He has also developed expertise in choosing and implementing state-of-the-art deep learning architectures and large-scale solutions based on them. 

Rohit Saha is currently an applied research scientist at Georgian’s R&D team and is assisting portfolio companies with their research endeavours. Owing to previous roles, he has experience building end-to-end machine learning pipelines. He holds a master’s degree from the University of Toronto, and his research interests include generative modelling and transfer learning for Computer Vision tasks",https://www.youtube.com/watch?v=0lHEsN8qqkw,,"In this talk, we demonstrate how latest ML techniques and MLOps tools can be used to overcome the challenges of working with massive transformer models. Moreover, we share our findings and recommendations from working on real-world examples at SPINS. Considering that we have a variety of challenges in our use cases from an ill-defined label space to a huge number of classes (~86,000) and massive data imbalance, we believe our findings and recommendations can be applied to most real-world settings.
",TMLS 2022,0lHEsN8qqkw
"Catalina Herrera, Chris Helmus",Dataiku,"Principle Sales Engineer, Sr. Sales Engineer ",De-Risk Your AI Efforts by Removing Friction From Your MLOps Processes,"According to McKinsey, building ML into processes enables leading organizations to increase their process efficiency by 30% or more while also increasing revenues by up to 10%. However, it’s not that simple. Several blockers prevent organizations from overcoming the difficulties encountered when industrializing AI. As a result, it can take up to nine months for teams to go from the proof of concept stage to production. In this context, how do you remove friction from your MLOps process and make your model processes trusted, agile, and controlled, so that you can finally deliver more value from your analytics and model faster?"," In this session, you’ll learn how Dataiku’s MLOps framework can help you to:
-Increase agility and solve difficulties in handoffs between business, data scientists, and IT
-Make your models trusted from the get go (and, therefore, reduce risk)
-Apply model control and approvals to enable, not disable, your AI projects",Understanding of MlOps,Workshop,6.0,Deployment and integration,,"With a passion for data and analytics, Catalina Herrera has spent her entire career helping the industry push beyond digitalization to business transformation. She’s held both educational and technical positions, worked with state-of-the-art technology solutions across multiple industry verticals, and served as a data scientist and advanced analytics consultant. Today she works with Fortune 100 companies and global technology leaders on digital transformation initiatives. 

Chris Helmus has spent his career helping people and organizations embrace self-service analytics and machine learning. His expertise spans from enabling business users to become data experts to MOps at scale with a focus on enabling collaboration.  When he's not working with data you can find Chris at music events in the Denver area.
",https://www.youtube.com/watch?v=DLamcMhpej8,,,TMLS 2022,DLamcMhpej8
"Vincent David, Michael Meredith",Capital One,Senior Director - Machine Learning,Low-latency Model Inference in Finance,"""Model Inference at Capital One as across the Fintech sector is a key aspect of the Model Development Lifecycle. In order to reap the benefits of machine learning models trained by Data Scientists, we are required to deploy said models in a production environment. This enables critical business applications, such as credit decisions or fraud detection. Increasingly, we are faced with demanding non-functional requirements for high resilience and low latency service response leading us to invest into service oriented architectures for model inference. Seldon has emerged over the last years a key solution to address these challenges.

In this presentation we will take a close look at the recently released Seldon V2 and our findings for its application for financial services at enterprise scale, comparing it to its V1 specification. We will summarize our findings as they relate to the benefits of novel improvements as well as challenges we see in establishing much needed controls to establish this new release in a highly regulated environment."""," Practical learnings, insights, and considerations for more effectively deploying models in a production environment - especially complex production environments.",,Case Study,5.0,Deployment and integration,"Low-latency Model Inference, Seldon V2, Financial services

","Experienced Machine Learning & Engineering leader with a history of working in Fintech and Entertainment. Strong data and quantitative background, with deep knowledge of complex systems and experiences working in multiple industries. Skilled in Machine Learning, Cloud Engineering. Passionate about using technology to solve high-value business problems.",https://www.youtube.com/watch?v=2SPvBcryw7w,Banking & Financial Services,,,2SPvBcryw7w
Alon Gubkin,Aporia,CTO & Co-Founder,Unleashing the Full Potential of Your Recommender System in Production,"In this workshop, we will explore how to improve your recommender system in production by monitoring your model and generating insights from production data. We will discuss how to track the behavior of different versions of your model, understand the performance of your model in different data slices, and detect data drift. By leveraging monitoring and insights from production data, you will be able to improve the performance and accuracy of your recommender system and drive better business outcomes. Join us to learn how to take your recommender system to the next level with real-time monitoring and actionable insights."," Everything you need to know to get the most out of your recommender system in production and turn them into a revenue machine. 
1. How to track the behavior of different versions of your model.
2. Understand the performance of your model in different data slices.
3. How to detect data drift.",Basic understanding of ML use cases and ML models in production.,Workshop,4.0,Performance optimization and efficiency,"Recommender Systems, Model Monitoring, Data Drift Detection","In 2019, Alon Gubkin cofounded Aporia, the ML observability platform. Aporia is trusted by Fortune 500 companies and data science teams in every industry to ensure responsible AI and monitor, improve, and scale ML models in production. Alon, an ex-R&D team lead in the elite Unit 81 intelligence unit of the Israel Defense Forces, has led Aporia in raising $30 million from investors like Tiger Global Management and Samsung Next. For two years in a row, 2022 and 2023, Alon was named to Forbes 30 Under 30.",https://www.youtube.com/watch?v=tOzq4ywjJAo,,,TMLS 2023,tOzq4ywjJAo
"Hanieh Arjmand, Spark Tseung",Lydia.ai,"ML Researcher, Applied Data Scientist",Sensitivity and interpretability of AI-models,"Model interpretability is important especially in regulated industries where risk-sensitive decisions typically require transparency and reliability of the underlying model. While often model interpretability gets sacrificed in other fields in order to achieve superior predictive performance, this is not the case in the regulated industries such as healthcare where model fairness plays an important role. In this talk, we will present case studies to illustrate the importance of sensitivity analysis for model interpretability and to showcase our design and implementations. Depending on the use cases of machine learning models, sensitivity tests have to be specifically and carefully designed and implemented. Using our machine learning models on electronic health record (EHR) and human activity, we will discuss potential approaches for designing some of the sensitivity tests, which have helped us understand different aspects of model behaviour and even uncover the unwanted biases and behaviours that had to be eliminated. ","Using case studies from our work, we will discuss potential approaches for designing some of the sensitivity tests, which have helped us understand different aspects of model behaviours and data biases.",,Case Study,4.0,"Ethics, governance compliance ",,"Hanieh Arjmand is a Machine Learning Researcher at Lydia.ai where she focuses on discovering and applying the best machine learning techniques to healthcare and insurance problems to help insurers use machine learning to protect more people. 

Spark Tseung is an Applied Data Scientist at Knowtions Research where he focuses on building frameworks for actuarial and underwriting validation to help insurers use machine learning to protect more people. Spark is working towards his Ph.D. in Statistics and specializes in the application of machine learning methods in Property & Casualty loss modelling and risk selection. ",https://www.youtube.com/watch?v=tkdil4J03NY,,This is a case study that explains how we designed the novel scores used in our mobile app (Well-aged) to represent the health status of the customers and has not presented or shared anywhere before.,,tkdil4J03NY
Nestor Maslej,"AI Index, Stanford Institute for Human-Centered AI",Research Manager,2022 AI Index Report Briefing,"Learn about some of the main trends in AI, as told to you by the 2022 AI Index Report. The AI Index is one of the most widely read annual reports on trends in AI and has informed AI policymakers and industry leaders across the globe. This presentation covers some of the main trends explored in the report, namely trends in areas such as research and development, technical advancement, ethics, economics, policy and education.","That AI is here in a way that it was not before, and that as a society, we need to think critically about the role AI should play in our lives. ",,Technical,3.0,Future trends,,"Nestor Maslej is a Research Manager at Stanford’s Institute for Human-Centered Artificial Intelligence (HAI). In this position, he manages the AI Index and Global AI Vibrancy Tool. Nestor also leads research projects that study AI in the context of technical advancement, ethical concerns and policymaking. In developing tools that track the advancement of AI, Nestor hopes to make the AI space more accessible to policymakers.

Nestor also speaks frequently about trends in AI. He has delivered presentations about the AI Index to teams at the World Economic Forum, Centre for Data Ethics and Innovation and Global Arena Research Institute. Nestor has also testified to the Canadian Parliament’s House of Commons Standing Committee on Access to Information, Privacy and Ethics on the use and impact of facial recognition technology in Canada.

Prior to joining HAI, Nestor worked in Toronto as an analyst in several startups. He graduated from the University of Oxford in 2021 with an MPhil in Comparative Government, where he used machine learning methodologies to study the Canadian Indian Residential schooling system and Harvard College in 2017 with an A.B. in Social Studies.",https://www.youtube.com/watch?v=8vpAJ0K1fOo,,,TMLS 2022,8vpAJ0K1fOo
Reem Al-Halimi,"Navblue, An Airbus Company",AI Enterprise Architec,Unlocking the Potential of Data in the Aviation Industry,"The aviation industry generates enormous amounts of data. Yet, much of that data is underutilized, translating into value add opportunities in multiple areas, including machine learning and sustainability. In this talk, I will take the audience on a tour to visualize the data flow within the aviation ecosystem to appreciate the amounts of data produced and the vast potential that data holds for a more sustainable, less disrupted experience for passengers, airlines, and airports alike. ","1. Understand the data that flows in the aviation ecosystem
2. Understand the value add this data can bring and the challenges faced by the industry to utilize it
3. Learn what to expect when working with data in a mature industry that is just starting to pivot towards more modern data infrastructures.",,Business Strategy or Ethics,2.0,Future trends,"Aviation Data, Data Utilization, Industry Challenges
","Dr. Reem Al-Halimi is the AI Enterprise Architect at NAVBLUE, An Airbus Company. Through her role, she is responsible for re-envisioning flight operations products into smart products, making users’ workflow more efficient and their decisions more effective. Dr. Al-Halimi received her Ph.D. in Computer Science from the University of Waterloo in 2002. Since then, she has worked on a variety of machine learning projects that span many ML areas including generative AI, computer vision, Natural Language Processing, anomaly detection, and predictive models.",https://www.youtube.com/watch?v=mnEWuVA5lxE,"Information Technology & Service, Computer Software
",,TMLS 2024,mnEWuVA5lxE
David Talby,John Snow Labs,CTO,Applying Responsible AI with the Open-Source LangTest Library,"While there's a lot of work done on defining the risks, goals, and policies for Responsible AI, less is known about what you can apply today to build safe, fair, and reliable models. This session introduces open-source tools and examples of using them in real-world projects - to address three common challenges.

The first is robustness - testing and improving a model's ability to handle accidental or intentional minor changes in input that can uncover model fragility and failure points. The third is bias - testing that a model performs equally across gender, age, race, ethnicity, or other population groups. The third is data leakage, in combination with leakage caused by using personally identifiable information in training data. The open-source LangTest library is used to demonstrate how to generate tests, run tests, augment data, and integrate these evaluations into MLOps workflows.

This session is intended for data science practitioners and leaders who need to know what they can do today to build AI & LLM applications that work safely and reliably in the real world.","
This session is intended for data science practitioners and leaders who need to know what they can & should do today to build AI systems that work safety & correctly in the real world.

Background Knowledge:

Basic familiarity with machine learning is assumed.",-,,5.0,"Ethics, governance compliance ","Responsible AI, LangTest library, Model robustness
","David Talby is the Chief Technology Officer at John Snow Labs, helping companies apply artificial intelligence to solve real-world problems in healthcare and life science. David is the creator of Spark NLP – the world’s most widely used natural language processing library in the enterprise. He has extensive experience building and running web-scale software platforms and teams – in startups, for Microsoft’s Bing in the US and Europe, and to scale Amazon’s financial systems in Seattle and the UK. David holds a Ph.D. in Computer Science and Master’s degrees in both Computer Science and Business Administration. He was named USA CTO of the Year by the Global 100 Awards and GameChangers Awards in 2022.",https://www.youtube.com/watch?v=RDPt9dcl6Sc,Computer Software,,,RDPt9dcl6Sc
Arthur Vitui,Red Hat Canada,Senior Data Scientist Specialist Solutions Architect,Deploying LLMs on Kubernetes environments,Learn how to deploy LLMs on Kubernetes environments and use them to enhance your intelligent applications ecosystem with chatbots to talk to your documentation or help you in operations management tasks such as anomaly detection,"-Learn how to configure a Kubernetes environment, such as Red Hat OpenShift, to support the deployment of a Large Language -Model (applied case for hybrid environments).
-Use the deployed LLM to build a RAG based system to ""talk"" to your documentation (operations applied use case)
-Use the deployed LLM to spot and predict traffic anomalies for deployed and monitored applications (operations applied use case)",,Business Strategy or Ethics,4.0,Deployment and integration,"Kubernetes, LLM Deployment, Anomaly Detection
","Arthur is a senior data scientist specialist solution architect at Red Hat Canada. With the help of open source software, he is helping organizations develop intelligent application ecosystems and bring them into production using MLOps best practices.

He has over 15 years of experience in the design, development, integration, and testing of large-scale service enablement applications.

Arthur is pursuing his PhD in computer science at Concordia University, and he is a research assistant in the Software Performance Analysis and Reliability (SPEAR) Lab. His research interests are related to AIOps, with a focus on performance and scalability optimization.",https://www.youtube.com/watch?v=FoEWvU5UbsA,"Computer Software, Information Technology & Service
",,TMLS 2024,FoEWvU5UbsA
"Rohit Saha, Kyryl Truskovskyi, Angeline Yasodhara, Benjamin Ye","Georgian, Opens ML","Machine Learning Scientist, Founder, ML Engineer, ML Scientist, ML Engineer",Leveraging Large Language Models to build Enterprise AI," Generative AI is poised to disrupt multiple industries as enterprises rush to incorporate AI in their product offerings. The primary driver of this technology has been the ever-increasing sophistication of Large Language Models (LLMs) and their capabilities. In the first innings of Generative AI, a handful of third-party vendors have led the development of foundational LLMs and their adoption by enterprises. However, development of open-source LLMs have made massive strides lately, to the point where they compete or even outperform their closed-source counterparts. This competition presents an unique opportunity to enterprises who are still navigating the trenches of Generative AI and how best to utilize LLMs to build enduring products. This workshop (i) showcases how open-source LLMs fare when compared to closed-source LLMs, (ii) provides an evaluation framework that enterprises can leverage to compare and contrast different LLMs, and (iii) introduces a toolkit to enable easy fine-tuning of LLMs followed by unit-testing (https://github.com/georgian-io/LLM-Finetuning-Toolkit)","By the end of this workshop, learn how to create instruction-based datasets, fine-tune open-source LLMs via ablation studies and hyperparameter optimization, and unit-test fine-tuned LLMs.",Python + Familiarity with concepts such as prompt designing and LLMs,Workshop,3.0,"Model dev, training, arch.","Large Language Models (LLMs), open-source LLMs, enterprise AI.
","Rohit: Rohit is a Machine Learning Scientist on Georgian's R&D team, where he works with portfolio companies to accelerate their AI roadmap. This includes scoping research problems to building ML models to moving them into production. He has over 5 years of experience developing ML models across Vision, Language and Speech modalities. His latest project entails figuring out how businesses can leverage Large Language Models (LLMs) to address their needs. He holds a Master's degree in Applied Computing from the University of Toronto, and has spent 2 years at MIT and Brown where he worked at the intersection of Computer Vision and domain adaptation.

Kyryl:  Kyryl is a seasoned ML professional, currently based in Canada. With a rich 9-year background in ML, he has evolved from hands-on coding to architecturing key ML business solutions.

Angeline: Angeline is a Machine Learning Scientist at Georgian, collaborating with companies to accelerate their AI product development. Before joining Georgian, she was a research assistant at the Vector Institute, working at the intersection of machine learning and healthcare, focusing on explainability and causality. From explainability, time series, outlier detection to LLMs, she applies the latest techniques to enhance product differentiation.

Benjamin: Ben is a Machine Learning Engineer at Georgian, where he helps companies to implement the latest techniques from ML literature. He obtained his Bachelor's from Ivey and Master's from Penn. Prior to Georgian, he worked in quantitative investment research.",https://www.youtube.com/watch?v=BTYL-kLmhFE,,,TMLS 2024,BTYL-kLmhFE
Jepson Taylor,VEOX Inc,Former Chief AI Strategist DataRobot & Dataiku,Unleashing the Algorithm Genie: AI as the Ultimate Inventor,"Prepare to have your understanding of AI capabilities turned upside down. Jepson Taylor presents groundbreaking advancements in the field of generative algorithms, where AI systems now possess the ability to invent and optimize their own algorithms. This talk explores how adaptive workflows can produce thousands of novel solutions daily, effectively automating the role of the AI researcher. Through engaging demonstrations, attendees will explore the vast potential of this technology to accelerate innovation across all sectors. Discover how these self-evolving systems are set to redefine the boundaries of what's possible in technology and learn how you can start incorporating these concepts into your own work.",Cutting-edge advancements in multi-agent systems and their role in driving AI innovation. The paradigm shift from prompt engineering to goal engineering in AI development. The power and potential of bespoke algorithms versus general-purpose solutions. How generative algorithms are revolutionizing the field of AI research and development. Practical insights into implementing automated innovation systems for rapid solution generation. Strategies for integrating self-evolving AI systems into various industry applications. Real-world examples and case studies of generative algorithms in action.,,Advanced Technical/Research,3.0,Future trends,"Generative algorithms, Self-evolving systems, AI innovation","Jepson is a popular speaker in the AI space having been invited to give AI talks to companies like Space X, Red Bull, Goldman Sachs, Amazon, and various branches of the US government. Jepson's applied career has covered semiconductor, quant finance, HR analytics, deep-learning startup, and AI platform companies. Jepson co-founded and sold his deep-learning company Zeff.ai to DataRobot in 2020 and later joined Dataiku as their Chief AI Strategist. Jepson is currently launching a new AI company focused on the next generation of AI called VEOX Inc.",https://www.youtube.com/watch?v=8pZa7Wq53OY,Computer Software,"This session is unique because it demonstrates automating the AI researcher and developing code beyond human potential. This is a significant milestone in the AI race, reinventing AI itself and getting the humans out of the way.  ",MLOps & GenAI World 2024,8pZa7Wq53OY
"Kyle Gallatin, Amber Roberts","Etsy, Arize AI","Senior Software Engineer I, Machine Learning, Machine Learning Engineer ",Troubleshooting Unstructured Data with Embeddings,"While 80% of data generated is unstructured images, text, or audio, ML teams working with this unstructured data often ship models blind. This lack of visibility can create costly and time-intensive problems — and some heartache, too. 

Internal embedding representations can be extracted from almost all types of deep learning models, giving an internal glimpse at what the model is “seeing.”

Embeddings are key to workflows that allow teams to identify issues, resolve them, and continually improve models and data. Join us as Amber Roberts, ML Engineer at Arize AI, and Kyle Gallatin, Senior Software Engineer I, Machine Learning at Etsy, as they discuss Etsy’s journey with embeddings, the challenges they’ve encountered, and best practices when troubleshooting unstructured data models. 

",,,Business Strategy,4.0,Performance optimization and efficiency,"Embeddings, Unstructured Data, Troubleshooting","Kyle Gallatin is Senior Software Engineer I, Machine Learning, at Etsy; formerly, he was a data scientist and MLE at Pfizer; he has an MA in Molecular and Cell Biology from Quinnipiac University || Amber Roberts is Machine Learning Engineer @ Arize AI. Previously: PM at Splunk, Head of AI at Insight Data Science. Carnegie Fellow (2016–2017); MS Astrophysics.",https://www.youtube.com/watch?v=ilXJLt9DckA,"Marketing & Advertising
","In a space that is changing daily, and as the use of deep learning models are proliferating, so are the challenges associated with them. Learning directly from one of the most innovative companies leveraging deep learning models today, ML teams can begin implementing best practices and avoid pitfalls, saving their time and energy for where it's needed most.  
",TMLS 2023,ilXJLt9DckA
"Rajiv Shah, Andrew Jardine",Hugging Face,"Machine Learning Engineer, Enterprise Account Executive",Building AI Applications with Transformers,"Transformers have ushered in some of the most innovative and exciting AI technologies, like Dalle and Github's Copilot. Rajiv shows you how to use open-source tools and models to solve use cases like auto-completion, semantic search, and document AI. He covers the power of embeddings, the emergence of Generative AI, and using transfer learning. He will end by touching on emerging trends around multimodal, multi-task, and large language models. The talk will also incorporate a notebook, code snippets, and paper references.",It's easy to get start building advanced AI applications.,,Workshop,5.0,Introduction to MLOps and GenAI,,"Rajiv:
Rajiv Shah is a leading expert on practical AI. At Hugging Face, his primary focus is on enabling enterprises to succeed with AI. He previously led data science enablement efforts across hundreds of data scientists at DataRobot and has been part of data science teams at Snorkel AI, Caterpillar, and State Farm. 

He is a widely recognized speaker on AI, has received many patents, and published research papers in several domains, including sports analytics, deep learning, and interpretability. He received a Ph.D. and a J.D. from the University of Illinois at Urbana Champaign.

Andrew:
Andrew is an Account Executive at Hugging Face where he helps enterprise customers understand how to leverage the 🤗 open-source resources to build state of the art ML. Outside of Hugging Face Andrew is the Toronto chapter lead for MLOps.Community and has a background in NLP, MLOps and engineering.",https://www.youtube.com/watch?v=CGo1Dt7Pxho,"Banking & Financial Services, Insurance
",This workshop is 100% practical and real life based. It is given by someone who has extensive practical experience (and also academic) in the field. This presents a unique opportunity for those who want to learn not only the theoretical foundations about the topic but also the practical challenges and solutions for running production grade Machine Learning NLP systems,,CGo1Dt7Pxho
Connor Joyce,"Microsoft and Author of ""Bridging Intentions to Impact""",Senior User Researcher,AI Features Demand Evidence-Based Decisions," We are in the midst of a technology paradigm shift, and there is significant pressure on product teams to build Generative AI (GenAI) into their products. Navigating these uncharted waters requires decisions based on a deep understanding of user needs to ensure that this new technology is leveraged in the most beneficial way for both users and the business. This presentation emphasizes the necessity of creating a demand for insights by product teams and the democratization of evidence creation. Doing both can be achieved by defining features in a way that highlights the evidence supporting why they should work. By using the novel User Outcome Connection, teams can naturally identify what data is known and unknown about a feature. This framework makes the pursuit of new research to fill the gaps more straightforward, ensuring a solid foundation for decision-making. 

By developing User Outcome Connection frameworks for key features, teams can design solutions that appropriately and effectively incorporate GenAI. This will be showcased through B2B and B2C examples illustrating the practical application and transformative potential of this approach."," Attendees will learn how using the User Outcome Connection framework for key features, enables the strategic use of GenAI where it truly adds value. By the end of this session, participants will be equipped with actionable steps to adopt evidence-based frameworks, ensuring their products meet the evolving demands of technology and user expectations. Join this session to learn how to navigate the AI paradigm shift with evidence-based decisions and design truly impactful AI-enhanced features.",,Business Strategy,5.0,Business and stakeholder alignment,"Evidence-based decisions, User Outcome Connection, GenAI integration","Connor Joyce is the author of ""Bridging Intentions to Impact"" and a Senior User Researcher on the Microsoft Copilot Team, where he is advancing the design of AI-enhanced features. Passionate about driving meaningful change, Connor advocates that companies adopt an Impact Mindset, ensuring that products not only change behavior to satisfy user needs but also drive positive business outcomes. He is a contributor to numerous publications, advises emerging startups, and lectures at the University of Pennsylvania. Based in Seattle, Connor enjoys exploring the outdoors with his dog, Chai, and a local event organizer.",https://www.youtube.com/watch?v=pkkVTqlMQcE,"Computer Software, Banking & Financial Services, Hospital & Health Care, Information Technology Services, Marketing & Advertising, Other",It is a highly practical approach for the creation of Generative AI features. Both recognizing what has changed but also that it still relies on good product development practices.,MLOps & GenAI World 2024,pkkVTqlMQcE
Shreya Rajpal,Guardrails AI,CEO & Cofounder,Building AI Infrastructure for the GenAI Wave,"As Generative AI (GenAI) continues to revolutionize industries, it brings a new set of risks and challenges. This talk focuses on building robust AI infrastructure to manage and mitigate these risks. We will explore the multifaceted nature of GenAI risks and the essential infrastructure components to address them effectively. Key topics include implementing real-time monitoring systems to identify anomalies and biases, designing audit trails for enhanced transparency and developing adaptive security measures to combat emerging threats.

The presentation will also cover governance strategies for GenAI, and the integration of ethical AI frameworks to support responsible development and deployment. This talk is tailored for CISOs, AI ethics officers, ML engineers, and IT architects aiming to build secure and responsible GenAI systems.",,,Future of AI,,"Ethics, governance compliance ","AI infrastructure, Real-time monitoring, Ethical AI frameworks","Shreya Rajpal is the CEO of Guardrails AI, an open source platform developed to ensure increased safety, reliability and robustness of large language models in real-world applications. Her expertise spans a decade in the field of machine learning and AI. Most recently, she was the founding engineer at Predibase, where she led the ML infrastructure team. In earlier roles, she was part of the cross-functional ML team within Apple's Special Projects Group and developed computer vision models for autonomous driving perception systems at Drive.ai.",https://www.youtube.com/watch?v=Se9_38V2TPA,,,MLOps & GenAI World 2024,Se9_38V2TPA
Stefan  Krawczyk,DAGWorks Inc.,CEO & Co-founder,Getting higher ROI on MLOps initiatives: five lessons learned while building out the MLOps Platform for 100+ Data Scientists,"MLOps is hard, because there's so many ""things"" that you might want to integrate and connect with: A/B testing, feature stores, model registries, data catalogs, lineage systems, python dependencies, machine learning libraries, LLM APIs, orchestration systems, online vs offline systems, speculative business ideas, etc. In this talk I’ll cover five lessons that I learned while building out the self-service MLOps platform for over 100 data scientists at Stitch Fix. This talk is for anyone building their own, or buying it all off the shelf. Either way you’re still going to want everything to fit cohesively together, i.e. as a platform, and learning what to avoid/focus on will increase your ROI on MLOps initiatives. ","Five lessons that will help them with MLOps initiatives:
1. Build for immediate adoption.
2. Don’t build for every user equally. Let those with stronger SWE skills do more themselves.
3. Don’t give users direct access to vendor/cloud APIs.
4. Take time to ensure you can live in the shoes of your users.
5. Provide two layers of APIs to keep your development nimble: a foundational layer, and then an opinionated higher level layer.
",,Business Strategy,4.0,Performance optimization and efficiency,"MLOps platform, ROI on MLOps, integration challenges
","A hands-on leader and Silicon Valley veteran, Stefan has spent over 15 years thinking about data and machine learning systems, building product applications and infrastructure at places like Stanford, Honda Research, LinkedIn, Nextdoor, Idibon, and Stitch Fix. A regular conference speaker, Stefan has guest lectured at Stanford’s Machine Learning Systems Design course and is an author of a popular open source framework called Hamilton. Stefan is currently CEO of DAGWorks, an open source startup that is enabling teams a standardized way to build and maintain data, ML and LLM pipelines without the coding nightmares.",https://www.youtube.com/watch?v=GH51aI3krW4,Applicable to anyone building out an internal platform,,,GH51aI3krW4
Danielle Goldfarb,RIWI (Real-Time Interactive Worldwide Intelligence),"Vice President and General Manager of Global Affairs, Economics and Public Policy",The risks of excluding the disengaged from your dataset,"Analysts use machine learning tools to analyze the vast amounts of data now available. Big data is appealing, but even the best tools and techniques will give you noise if the underlying data aren’t robust and inclusive. To reliably predict elections, correctly anticipate consumer demand or to accurately predict the trajectory of a pandemic, leaders need to ask themselves who is (and isn’t) included in their dataset. ","There is a huge emphasis on tools to analyze big data but we need to spend at least as much time on the quality of the underlying dataset. If, for example, we exclude disengaged populations, as some common methods do, we risk getting things wrong, whether understanding a market's potential, predicting an election result, or anticipating the trajectory of the pandemic, the war, or the economy. ",,Business,1.0,"Ethics, governance compliance ",,"Opening speaker, Tedx Toronto: https://www.ted.com/talks/danielle_goldfarb_the_smartest_way_to_predict_the_future; Developing a U of Toronto course on new data tools for global affairs / public policy; see also https://www.linkedin.com/in/goldfarbdanielle/ ",https://www.youtube.com/watch?v=OuDwFdwbZPU,,"With the vast amounts of data out there, and the opportunity that presents for data scientists, we need to stop and think not only about ethical AI (of which much is already written or on the internet) but about data quality and about who is included in datasets. Social media analysis is appealing for ML because it provides a large, continuing dataset. But big data can heighten the risk of drawing a wrong conclusion when applied to a narrow group of voices. In fact, most people are not active on social media. Traditional tools of business intelligence such as focus groups and panel surveys may also amplify our biases when they exclude the diverse set of voices that we need for reliable business intelligence. 
",TMLS 2022,OuDwFdwbZPU
Geoffrey LaPorte,Appen,Applied AI Solutions Architect,Striking the Balance: Leveraging Human Intelligence with LLMs for Cost-Effective Annotations,"Data annotation involves assigning relevant information to raw data to enhance machine learning (ML) model performance. While this process is crucial, it can be time-consuming and expensive. The emergence of Large Language Models (LLMs) offers a unique opportunity to automate data annotation. However, the complexity of data annotation, stemming from unclear task instructions and subjective human judgment on equivocal data points, presents challenges that are not immediately apparent. 

In this session, Chris Stephens, Field CTO and Head of AI Solutions at Appen will provide a overview of an experiment that the company recently conducted to test the tradeoff between quality and cost of training ML models via LLMs vs human input. Their goal was to differentiate between utterances that could be confidently annotated by LLMs, and those that required human intervention. This differentiation was crucial to ensure a diverse range of opinions or to prevent incorrect responses from overly general models. Chris will walk audience members through the dataset used as well as methodology for the experiment, as well as the company’s research findings. ","Geoff will walk audience members through an experiment that highlights a key issue with using a vanilla LLM—it might struggle with complex real-world tasks. Researchers recommend exercising caution when relying solely on LLMs for annotation. Instead, a balanced approach combining human input with LLM capabilities is recommended, considering their complementary strengths in terms of annotation quality and cost-efficiency.",,Applied Case Studies,7.0,"Ethics, governance compliance ","Data annotation, Human-LLM collaboration, Cost-efficiency","Geoff is a seasoned tech innovator with over 13 years of experience, transitioning from management consulting to software development. He specializes in bridging the gap between technology and business strategy, consistently delivering user-focused, high-impact solutions. Geoff is known for pushing boundaries and tackling complex technology challenges with a passion.",https://www.youtube.com/watch?v=ZLOHQLyJs58,"Computer Software, Information Technology Services",This session provides an overview of an exclusive experiment that the Appen team conducted to test when it is appropriate to leverage LLMs to automate data annotation.,MLOps & GenAI World 2024,ZLOHQLyJs58
Bhaskarjit Sarmah,BlackRock,Senior Data Scientist,Learning Embedded Representation of the Stock Correlation Matrix using Graph Machine Learning,"Understanding non-linear relationships among financial instruments has various applications in investment processes ranging from risk management, portfolio construction and trading strategies. Here, we focus on interconnectedness among stocks based on their correlation matrix which we represent as a network with the nodes representing individual stocks and the weighted links between pairs of nodes representing the corresponding pair-wise correlation coefficients. The traditional network science techniques, which are extensively utilized in financial literature, require handcrafted features such as centrality measures to understand such correlation networks. However, manually enlisting all such handcrafted features may quickly turn out to be a daunting task. Instead, we propose a new approach for studying nuances and relationships within the correlation network in an algorithmic way using a graph machine learning algorithm called Node2Vec. In particular, the algorithm compresses the network into a lower dimensional continuous space, called an embedding, where pairs of nodes that are identified as similar by the algorithm are placed closer to each other. By using log returns of S&P 500 stock data, we show that our proposed algorithm can learn such an embedding from its correlation network. We define various domain specific quantitative (and objective) and qualitative metrics that are inspired by metrics used in the field of Natural Language Processing (NLP) to evaluate the embeddings
in order to identify the optimal one. Further, we discuss various applications of the embeddings in investment management.",In this paper we have shown how to create stock embedding representation from stock correlation matrix. And evaluated the learnt embeddings using a quantitative way,"Network Science, Machine Learning, Word Embeddings",,5.0,"Model dev, training, arch.",,"Bhaskarjit is a data scientist and has solved business problems in many domains including Retail, FMCG, Banking, Media & Entertainment etc. using machine learning. Currently he is working as a data scientist BlackRock where he builds predictive models for financial markets. His research interests are Network Science, AI Interpretability, Uncertainty, NLP etc.",https://www.youtube.com/watch?v=TxEb9cRL5Bs,,"This speech is centered around feature extraction from networks. In this speech, will first introduce the traditional hand crafted feature extraction technique from networks. And then will explain how we can use graph machine learning for automatic feature extraction in the form embeddings. And how to evaluate those embeddings in quantitative way
",TMLS 2022,TxEb9cRL5Bs
"Valerii Podymov, Roshan Issac, Vlad Ryzhkov, Joey Zhou",FreshBooks,"Lead Data Scientist, Machine Learning Engineer, Senior Data Engineer, Senior Data Engineer",Builidng a fully automated ML Platform using Kubeflow and declarative approach to development of end-to-end ML pipelines.,"Recent innovations in the ML ecosystem have seen the emergence of operationally-focused technology like declarative systems and data-centric AI. These techniques appear to be a radical change for AI practitioners, who can now more simply frame use cases and manage workflows. In this talk, we’ll take a look at the history of AI to see the progress that has been made and how we’ve arrived at where we are now. How are high-tech companies handling AI initiatives internally, and why aren’t we all copying them? Has MLOps been the promised solution to simplifying deployment and monitoring of production AI? How do we create a simpler paradigm for operationalizing AI? All these questions and more will be addressed.","A journey to higher levels of MLOps maturity is unique for any company and has no recipes due to experimental nature of MLOps. Many insights and ideas in this area are the results of investments by big names (Google, Microsoft, Amazon) and knowledge sharing between smaller companies like us working on similar problems. We are grateful for this opportunity to contribute to the ecosystem so that others can learn from us. ",,Case Study,6.0,Deployment and integration,,"Valerii joined FreshBooks a year ago to lead and grow a team of Data Scientists and Machine Learning Engineers. He has an experience in multiple industries ranging from Electronics to Clean Tech and has contributed to the development of innovative solutions for a variety of brands such as LG Electronics, Panasonic, Samsung, Toyota, Scotiabank, Cineplex. He has a University Degree in Telecom Engineering and PhD in Automated Control Systems. Author of 20 patented inventions in Signal Processing, Electronics and Computing.

Roshan works as a Machine Learning Engineer at FreshBooks where he is building ML Platform on Vertex AI and bringing MLOps best practices to the organization. He was previously at the same role with Cineplex. He has a Bachelor Degree in Computer Science and Engineering and hold graduate certificates in AI & Project Management. Overall he has 8+ years of experience in Machine Learning, Data Analytics and CRM software working in different startups and companies in Canada and India. He published papers in IEEE conferences and was a speaker at Libre Software Meeting (LSM), France.

Vlad joined FreshBooks a year ago with extensive Data Engineering background and he works on building ML Platform bringing best practices in large-scale data processing to the company. He has a PhD in System Analysis, Management and Information Processing. Overall, his 15+ years of software development experience comprises such areas as financial systems, e-commerce, e-sport and airlines in Canada and overseas.

Joey joined FreshBooks three months ago and works on the continuous monitoring framework for the ML team. Before, he had an experience in the tech industry, ranging from social-dating to e-commerce, in multiple roles such as Data Scientist and Machine Learning Engineer. He built a recommender systems for one of the largest e-commerce platforms in China. With hands-on experience in building and productionizing ML models, he is ready to pursue his passion for MLOps at FreshBooks.",https://www.youtube.com/watch?v=T1XJr6BQ0nw,,"Managing MLOps is highly immature topic with lack or absence of commonly accepted best practice, so the experience of any company in growing over MLOps maturity levels is always unique.  
",TMLS 2022,T1XJr6BQ0nw
Andreea Munteanu,Canonical,AI/ML Product Manager,Generative AI: the open source way,"Generative AI is probably the topic of the year. Leaders across the world feel the pressure of missed opportunities related to the latest technology. Professionals are also left worried about the impact that latest technology will have on their roles. Data scientists and machine learning engineers are challenge and need to quickly upskill. With such a stretched picture in mind, will generative AI deliver up to the great promises that it has right now? 

This lighting talk will depict the opportunities that open source gives to generative AI to accelerate innovation.  Between anxiety and enthusiasms, the latest technologies bring a new angle to the market. Let’s learn together about genAI with open source: from models to tooling to applications
",,,Ignite Lightning Talk,,Future trends,"Generative AI, Open Source, Innovation","Andreea is a Product Manager at Canonical, leading the MLOps area. With a background in Data Science in various industries, such as retail or telecommunications, Andreea used AI techniques to enable enterprises to benefit from their initiatives and make data-driven decisions. Andreea is looking to help enterprises get started with their AI projects and then deploy them to production, using open-source, secure, stable solutions. 
",https://www.youtube.com/watch?v=81AxU8s3ocQ,"Hospital & Health Care, Information Technology & Service
","Secure MLOps is still a hot topic - it is being approached, but not often for highly sensitive data, even if industries have this clear need.
",MLOps & GenAI World 2023,81AxU8s3ocQ
"Greg Loughnane, Chris Alexiuk",AI Makerspace,"Co-Founder, Co-Founder & CTO",Building Agentic and Multi-Agent Systems with LangGraph (Pt. 2),"2024 is the year of agents, agentic RAG, and multi-agent systems! 

This year, people and companies aim to build more complex LLM applications and models; namely, ones that are ever-more capable of leveraging context and reasoning.  For applications to leverage context well, they must provide useful input to the context window (e.g., [in-context learning](https://openai.com/index/language-models-are-few-shot-learners/)), through direct prompting or search and retrieval (e.g., [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401), or RAG).  To leverage reasoning is to leverage the Reasoning-Action ([ReAct](https://arxiv.org/abs/2210.03629)) pattern, and to be “agentic” or “agent-like.”  Another way to think about agents is that they enhance search and retrieval through the intelligent use of tools or services. 

The best practice tool in the industry for building complex LLM applications is LangChain.  To build agents as part of the LangChain framework, we leverage LangGraph, which allows us to bake in cyclical reasoning loops to our application logic.  LangChain v0.2, the latest version of the leading [infrastructure orchestration tooling](https://github.com/a16z-infra/llm-app-stack?tab=readme-ov-file#orchestrators), incorporates LangGraph directly, the engine that powers stateful (and even fully autonomous) agent cycles.

In this session, we'll break down all the concepts and code you need to understand and build the industry-standard agentic and multi-agent systems, from soup to nuts."," - A review of the basic prototyping patterns of GenAI, including Prompt Engineering, RAG, Fine-Tuning, and Agents
- The core ideas and constructs to build agentic and multi-agent applications with LangGraph
- ⛓️ Build custom agent applications with LangGraph
- 🤖 Develop multi-agent workflows with LangGraph",,Workshop,4.0,"Model dev, training, arch.","LangGraph, Agentic systems, Multi-agent workflows




","[Dr. Greg Loughnane](https://www.linkedin.com/in/gregloughnane/) is the Co-Founder & CEO of AI Makerspace, where he is an instructor for their [AI Engineering Bootcamp](https://maven.com/aimakerspace/ai-eng-bootcamp). Since 2021 he has built and led industry-leading Machine Learning education programs.  Previously, he worked as an AI product manager, a university professor teaching AI, an AI consultant and startup advisor, and an ML researcher.  He loves trail running and is based in Dayton, Ohio.

[Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/) is the Co-Founder & CTO at AI Makerspace, where he is an instructor for their [AI Engineering Bootcamp](https://maven.com/aimakerspace/ai-eng-bootcamp). Previously, he was a Founding Machine Learning Engineer, Data Scientist, and ML curriculum developer and instructor. He’s a YouTube content creator YouTube who’s motto is “Build, build, build!” He loves Dungeons & Dragons and is based in Toronto, Canada.",https://www.youtube.com/watch?v=uPuoysjaCbw,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Insurance, Information Technology Services, Marketing & Advertising, Telecommunications, Other","In just three hours, you can go from not knowing agents, LangChain, or LangGraph, to building robust, production-grade multi-agent applications with industry-leading open-source tooling.  The only thing you'll have to do is figure out what to build, and why... ",MLOps & GenAI World 2024,uPuoysjaCbw
Anne Martel,Sunnybrook Research Institute / University of Toronto,"Professor in Medical Biophysics at the University of Toronto, the Tory Family Chair in Oncology at Sunnybrook Research Institute, and a Faculty Affiliate at the Vector Institute, Toronto.",Artificial Intelligence And Digital Pathology: Making The Most of Limited Annotated Data,"
Obtaining large datasets with detailed annotations for medical imaging AI projects is a time consuming and expensive process as it usually requires the input of expert radiologists and pathologists. Collecting data to train outcome prediction models is even more challenging as the number of patients with both imaging and follow up data may be small, and only weak labels are available. This talk will describe several semi-supervised and self-supervised approaches which can make more efficient use of small and/or weakly labelled datasets. The focus will be on digital pathology but the methods described are applicable any medical imaging modality.
",Self-supervision and smart sampling strategies are essential in digital pathology,,Advanced Technical/ Research,6.0,Introduction to MLOps and GenAI,,"Anne Martel is a Professor in Medical Biophysics at the University of Toronto, the Tory Family Chair in Oncology at Sunnybrook Research Institute, and a Faculty Affiliate at the Vector Institute, Toronto. Her research program is focused on medical image and digital pathology analysis, particularly on the development of self-supervised and weakly supervised methods for segmentation, diagnosis, and prediction/prognosis. In 2006 she co-founded Pathcore, a software company developing complete workflow solutions for digital pathology. 
Dr Martel is an active member of the medical image analysis community and is a fellow of the MICCAI Society which represents engineers and computer scientists working in this field.  She has served as board member of MICCAI and is currently on the editorial board of Medical Image Analysis, on of the leading journals in the field. ",https://www.youtube.com/watch?v=blPsL1sGFPc,,,TMLS 2022,blPsL1sGFPc
Adam Probst,ZenML,CEO & Co-Founder,Supercharge ML Teams: ZenML's Real World Impact in the MLOps Jungle,"Supercharge ML Teams: ZenML's Real World Impact in the MLOps Jungle
In the complex ecosystem of machine learning operations, teams often find themselves entangled in a dense jungle of tools, workflows, and infrastructure challenges. This talk explores how ZenML, an open-source MLOps framework, is cutting through the underbrush to create clear paths for ML teams to thrive.
We'll dive into real-world case studies demonstrating how ZenML has empowered organizations to streamline their ML pipelines, from experimentation to production. Attendees will learn how ZenML addresses common pain points such as reproducibility, scalability, and collaboration, enabling teams to focus on innovation rather than operational overhead.
Key topics include:

Navigating the MLOps tooling landscape with ZenML as your compass
Achieving seamless transitions from laptop to cloud deployments
Enhancing team productivity through standardized, yet flexible, ML workflows
Lessons learned from implementing ZenML in diverse industry settings

Whether you're a data scientist, ML engineer, or team lead, you'll gain practical insights on how to leverage ZenML to supercharge your ML initiatives and conquer the MLOps jungle.",,,Furture of AI,,Deployment and integration,"ZenML, MLOps framework, ML pipelines","Adam Probst is the Co-founder and CEO of ZenML, an open-source MLOps framework simplifying machine learning pipelines. He holds a degree in Mechanical Engineering and studied at both Stanford University and the Technical University of Munich. Before co-founding ZenML, Adam gained valuable experience in the ML startup world within the commercial vehicle industry. Driven by a passion for customer-centric solutions, Adam is obsessed with unlocking the tangible benefits of MLOps for businesses.",https://www.youtube.com/watch?v=J_ItDeB12gY,,,MLOps & GenAI World 2024,J_ItDeB12gY
Vanessa Pizante,Sobeys,Data Scientista,Building a Measurement System for Personalization: A Bayesian Approach,"Personalization programs can have a significant impact on a business, but measuring their effectiveness can be challenging. Our team at Sobeys faced the task of building a system that could communicate the value of our personalized offers to external stakeholders and inform our internal team about what works and what does not. This required us to develop an experimentation and measurement platform that could measure the effects of a hierarchy of treatment variants, compute these effects across different customer segments, and aggregate the results consistently. In this talk, we will share our experience in creating a configurable, automated process that can handle repeated experimentation and measurement. 

Simple frequentist A/B measurement approaches proved to be insufficient when measuring the effectiveness of our complex hierarchy of treatment variants, across an independent hierarchy of customer segments. The issue is that contradictory results can regularly arise when slices of hierarchical data are tested independently. For instance, two segments can show a positive lift individually, but the overall lift across both segments could be negative (this phenomenon is known as Simpson’s Paradox). Multilevel models can be leveraged in both the frequentist and Bayesian paradigm to help address this. However, in our use case the Bayesian paradigm offered several useful advantages including the capability to explicitly define the structure of the hierarchy via prior specification and better handling of smaller customer segments. 

In general, attendees of this talk can expect to learn: 

- How to effectively assign customers to treatment variants to optimize measurement - regardless of the statistical paradigm in use. 

- The basics of Bayesian statistics and the advantages it can offer in measurement of hierarchical experiments 

- How we used this Bayesian model to attribute lift to different components of the program, compared model variants, and measured results across various customer segments 

- How we adopted an MLOps-like approach in designing our measurement platform in order to support this Bayesian model.","Bayesian statistics at scale and taking an automated approach to complex Bayesian modelling is a topic I struggled to find much about. Also dealing with nested experiments in a robust manner, at scale, within this automated framework. ",,Case study,3.0,"Model dev, training, arch.","Bayesian Statistics, Hierarchical Experimentation, Measurement Systems





","I am a data scientist at Sobeys working on the personalization team to help deliver personalized offers to customers through our Scene+ rewards program. My academic background is more mathematics focused, as I have an undergraduate degree in Applied Mathematics from the University of Calgary and a Masters in Statistics from the University of Toronto. During my career, I've also learned a lot about software development, design and MLOps; and enjoy combining these newer skills with my math-focused education to build effective and long-lasting machine learning and measurement frameworks.",https://www.youtube.com/watch?v=2DZogx96aR4,"Food & Beverages
",Large amount of Data and Scale (hundreds of thousands of items across different region and banners) and its direct impact on the business and customers.,,2DZogx96aR4
Niels  Bantilan,Union.ai,Chief Machine Learning Engineer,Fine-tuning Language Models with Declarative ML Orchestration,"The use of Language Models (LMs) has become more widespread in recent years, thanks in part to the broader accessibility of datasets and the ML frameworks needed to facilitate the training of these models. Many of these models are large – hence the terminology of Large Language Models (LLMs) – and serve as so-called foundation models, which are trained by organizations with the compute resources to train them. These foundation models, in turn, can be fine-tuned by the broader machine learning community for specific use cases, perhaps on proprietary data. One of the barriers that make fine-tuning these models is infrastructure: even with cloud tools like Google Colab and the wider availability of consumer-grade GPUs, putting together a runtime environment to fine-tune these models is still a major challenge. This workshop will give attendees hands-on experience on how to use Flyte to declaratively specify infrastructure so that they can configure training jobs to run on the required compute resources to fine-tune LMs on their own data.","This workshop has two main learning goals. First, attendees will learn the main concepts behind Flyte, a workflow orchestrator for data and machine learning. Many of these concepts are orchestrator-agnostic, such as containerization for reproducibility, declarative infrastructure, and type-safety. Secondly, they will also learn how to leverage the latest deep learning frameworks that optimize memory and compute resources required to fine-tune language models in the most economical way.","Intermediate Python, working knowledge of Docker, and intermediate knowledge of machine learning.",Workshop,4.0,"Model dev, training, arch.","Language Models, Fine-Tuning Techniques, Declarative ML Orchestration","Niels is the Chief Machine Learning Engineer at Union.ai, and core maintainer of Flyte, an open source workflow orchestration tool, author of UnionML, an MLOps framework for machine learning microservices, and creator of Pandera, a statistical typing and data testing tool for scientific data containers. His mission is to help data science and machine learning practitioners be more productive.

He has a Masters in Public Health with a specialization in sociomedical science and public health informatics, and prior to that a background in developmental biology and immunology. His research interests include reinforcement learning, AutoML, creative machine learning, and fairness, accountability, and transparency in automated systems.",https://www.youtube.com/watch?v=lUH95zahGcM,,,TMLS 2023,lUH95zahGcM
Liran Hason,Aporia,CEO,The framework for great ML products,"F1 score, OKRs, Precision, KPIs - are more related than you’d think.
As an ML Engineer and business leader Liran will talk about the frustration that both Data Scientists and business stakeholders experience with ML projects. ","We’ll get down to the core reasons, and discuss a framework for building successful ML products, achieving data science-business alignment, and accomplishing trust and model value recognition.",,Business,4.0,Business and stakeholder alignment,,"Liran Hason is the Co-Founder and CEO of Aporia, a full-stack ML observability platform used by Fortune 500 companies and data science teams across the world to ensure responsible AI. Prior to founding Aporia, Liran was an ML Architect at Adallom (acquired by Microsoft), and later an investor at Vertex Ventures. Liran created Aporia after seeing first-hand the effects of AI without guardrails. In 2022, Forbes named Aporia as the “Next Billion-Dollar Companies”. ",https://www.youtube.com/watch?v=ZhblmLqUc6U,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Marketing & Advertising, Telecommunications, Any industry releasing AI/GenAI products
",,TMLS 2022,ZhblmLqUc6U
Rex Lam,Autodesk,"Director, Machine Learning Platform ",The transformative role of AI/ML in heavy industries,"AI/ML represents opportunities for Autodesk to drive insights & innovative solutions in architecture, design, and manufacturing tools.",,,Ignite Lighting Talk,,Introduction to MLOps and GenAI,"AI/ML, Heavy Industries, Architecture and Design","Rex Lam leads the Machine Learning Platform team to build platform capabilities that enable full ML cycle development and operational tools at Autodesk that aim to enable ML solutions faster, trusted and scalable.",https://www.youtube.com/watch?v=Wk64V7G6sHA,,,TMLS 2022,Wk64V7G6sHA
"Surbhi Rathore, Kamelia Aryafar, Shingai Manjengwa, Manas Bhuyan, Laila Paszti","Symbl.ai, Google Cloud AI, ChainML, Deloitte Consulting LLP, Kirkland & Ellis LLP
","CEO & Co-founder, Senior Engineering Director, Head of AI Education, AI & Data Leader, Retail & Consumer Products Practice Data Leader",Business Panel: GenAI Use-cases Across Industry Verticals. Early Trends and ROI,,,,Business Strategy,,Business and stakeholder alignment,"GenAI use-cases, Industry verticals, ROI
","Surbhi is the CEO and Co-founder of Symbl.ai, building state of the art understanding and generative AI for communication data. As a software developer turned CEO, she is on a mission to deliver exceptional and easy builder-first experience to create distinct product experiences that redefines how machines can elevate human interactions at scale. She's a strong advocate for immigrant founders and firmly believes in creating value through collaboration without borders.

-
Kamelia Aryafar is a Senior Engineering Director in Google Cloud AI overseeing teams building Generative Search and  Recommendations AI Solutions. Prior to Google, Kamelia was the Executive Vice President, Chief Algorithms and Analytics Officer, and board member for Overstock.com, where she led the company’s. Engineering, analytics, data platform, machine learning, data science and algorithmic products.  She also worked on product teams at Etsy. Kamelia holds a Ph.D. and M.Sc. in computer science and ML from Drexel University.-Shingai is the Head of AI Education at ChainML where she works with clients on AI education, adoption, and implementation.
-
Shingai takes clients from an AI product idea to an affordable and scalable deployment. A seasoned data scientist, Shingai has a background in retail analytics, generating data insights to top tier manufacturers and retailers, and she has led technical education at the Vector Institute for AI. Shingai is also the founder of Fireside Analytics Inc., an online data science education company. Over half a million learners have taken her courses on platforms such as IBM and Coursera. Shingai sits on a number of boards and advisory committees, and she is an award-winning speaker and children's book author.
-
""Manas is a distinguished AI & Data leader in Deloitte's Strategy & Analytics practice, specialized in accelerating digital transformation journey for global organizations in Retail and Consumer Products sector. With over 18 years of diverse experience in technology, product management, and consulting, he is a recognized thought leader and trusted advisor to executive teams. Manas aids his business partners in Data & Platform modernization, and unlock business value leveraging Data, Cloud, IoT, AI/ML, and GenAI technologies. 

An MBA from Cranfield School of Management, he has authored articles and frequently speaks at industry events on Data, Cloud, AI/ML, & GenAI.""

--
A machine learning engineer turned lawyer, Laila Paszti is a technology & IP transactions partner in the Bay Area office of Kirkland & Ellis LLP. Laila leverages her significant experience in technology, intellectual property and privacy to help clients navigate risk and protect their commercial interests, including when using and developing AI systems.

Laila’s practice focuses on technology and related aspects of mergers and acquisitions, corporate finance, licensing and other commercial transactions. She has acted on both sides of multinational mergers and acquisitions, patent purchases and sales, and licensing and IP transactions. Laila routinely advises clients on evolving multijurisdictional privacy regimes, cybersecurity, big data platform diligence and IP matters. She is sought out for her guidance on artificial intelligence and commercialization, including on patent strategy.
Before law school, she held positions as a software applications engineer at ExxonMobil and Capstone Technology where she designed and implemented neural-network based control systems to optimize industrial processes. Laila holds a B.A.Sc. in Chemical Engineering from the University of Toronto, a M.A.Sc. in Chemical Engineering from the University of Waterloo and a J.D. from the University of Toronto, where she was a law review editor. She is admitted to practice law in California, New York, and Ontario (Canada).  Laila is a Certified Information Privacy Professional (Europe and Canada) (CIPP/E and CIPP/C).


--
Ron Bodkin is the co-founder and CEO of ChainML, which provides an open-source platform for rapidly developing customized generative AI applications using collaborating ‘agents’. The platform enables the robust deployment and monitoring of generative AI models ensuring they can be operated with confidence and accuracy. He has over 15 years of AI experience, specializing in data science, analytics, machine learning, and large scale data processing. Formerly, he worked in Google’s Cloud CTO office, with a focus on Applied Artificial Intelligence leading efforts in industry applications and responsible AI. He also was the VP and GM of AI at Teradata, following Teradata’s acquisition of his company, Think Big Analytics, and led AI Engineering & CIO at Vector Institute. ",https://www.youtube.com/watch?v=r1FVentlxw8,Information Technology & Service,,,r1FVentlxw8
Zachary Carrico,Apella,Senior Machine Learning Engineer,From Black Box to Glass Box: Interpreting your Model,"Interpretability is crucial for improving model performance, reducing biases, and ensuring compliance with AI safety and fairness regulations. In this session, complex neural networks will be transformed from opaque ""black boxes"" into interpretable ""glass boxes"" by exploring a wide range of neural network-specific interpretability techniques. Attendees will learn about methods such as saliency maps, integrated gradients, Grad-CAM, SHAP, and activation maximization. The session will combine theoretical explanations with practical demonstrations, helping attendees effectively improve transparency and trust in neural network predictions.","Attendees will learn what to consider when choosing an interpretability technique. They will gain insights into methods such as saliency maps, Grad-CAM for visualizing important regions in images, and integrated gradients for attributing feature importance. By the end of the session, participants will know how to use these tools to make neural networks more understandable and how to communicate their insights to diverse stakeholders.",,Research or Advanced Technical,5.0,"Ethics, governance compliance ","Interpretability, Biases, Saliency maps"," Zac is a Senior Machine Learning Engineer at Apella, specializing in machine learning products for improving surgical operations. He has a deep interest in healthcare applications of machine learning, and has worked on cancer and Alzheimer's disease diagnostics. He has end-to-end experience developing ML systems: from early research to serving thousands of daily customers. Zac is an active member of the ML community, having presented at conferences such as Ray Summit, TWIMLCon, and Data Day. He has also published eight journal articles. He is passionate about advancing model interpretability and reducing model bias. In addition, he has extensive experience in improving MLOps to streamline the deployment and monitoring of models, reducing complexity and time. Outside of work, Zac enjoys spending time with his family in Austin and traveling the world in search of the best surfing spots.",https://www.youtube.com/watch?v=OFxjwW45Zmk,,"This session offers a unique focus on interpretability techniques tailored specifically to neural networks. We dive into the nuances of deep learning interpretability and its importance, comparing methods designed to make sense of complex architectures. This session combines theoretical depth with practical, real-world use cases to ensure that attendees not only understand how these techniques work, but also how to effectively apply them. This session also emphasizes making neural network predictions accessible to non-technical audiences, providing strategies to translate complex model behavior into actionable insights for stakeholders.",MLOps & GenAI World 2024,OFxjwW45Zmk
Sandeep  Singh,Beans.AI,Head of Applied AI,Stable Diffusion for Your Images: Custom Dream," ""Welcome to the """"Stable Diffusion for Your Images: Custom Dream"""" workshop! In this one-hour session, we will dive into the fascinating world of stable diffusion techniques for creating custom images.

Dreambooth is an innovative platform that allows users to unleash their creativity and transform ordinary images into extraordinary works of art. Through stable diffusion, we will explore how to manipulate and enhance images in a visually stunning and captivating way.

During the workshop, participants will learn the fundamentals of stable diffusion and its applications in image editing. We will cover various techniques, including color manipulation, texture enhancement, and image blending, to create visually striking and unique compositions.

Additionally, we will delve into the intricacies of Dreambooth's user-friendly interface, providing hands-on demonstrations and step-by-step guidance. Participants will have the opportunity to experiment with different filters, effects, and settings, unleashing their artistic potential and transforming their photos into mesmerizing masterpieces.

Whether you are a professional photographer looking to add an extra flair to your work or an amateur enthusiast eager to explore new creative avenues, this workshop is designed to inspire and empower you. Join us for an hour of exploration, experimentation, and artistic expression as we unlock the potential of stable diffusion with Dreambooth.""",Stable Diffusion is very easy for anybody to use and customize for the purpose at hand.,,Advanced Technical/Research,6.0,Performance optimization and efficiency,"Stable Diffusion, Custom Images, Dreambooth
","""Sandeep Singh is a leader in applied AI and computer vision in Silicon Valley's mapping industry, and he is at the forefront of developing cutting-edge technology to capture, analyze and understand satellite imagery, visual and location data.

With a deep expertise in computer vision algorithms, machine learning and image processing and applied ethics, Sandeep is responsible for creating innovative solutions that enable mapping and navigation software to accurately and efficiently identify and interpret features to remove inefficiencies of logistics and mapping solutions.

His work includes developing sophisticated image recognition systems, building 3D mapping models, and optimizing visual data processing pipelines for use in logistics, telecommunications and autonomous vehicles and other mapping applications.

With a keen eye for detail and a passion for pushing the boundaries of what's possible with AI and computer vision, Sandeep's leadership is driving the future of applied AI forward.""",https://www.youtube.com/watch?v=SS13OVZktQQ,"Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Insurance
",,MLOps & GenAI World 2023,SS13OVZktQQ
"Rahm Hafiz, Dan Adamson",Armilla AI,"CTO and Co-Founder, CEO and Co-founder","Making GenAI Safe, Trustworthy and Fit-for-purpose with Auto Alignment","We discuss a new alignment templating  technology that can be used to enhance the fairness, robustness and safety of the generative AI by understanding the expected behaviour of the model, measuring where the model is underperforming with synthetic test data, and iteratively improving the model with minimal humans in the loop fine-tuning approach. This alignment platform can be used for use case specific tasks including coercing a generative model to be more fair towards under represented groups, less toxic and less misogynistic, more leaning towards a desired political viewpoints, to use specific tone while generating answers for customer service applications, to preserve PII, to guard models from generating potentially harmful responses etc. Our alignment technology can interact with users as needed to adjudicate critical decision points to guide its intention-understanding, data generation, testing and tuning capabilities to be more contextual. ","by the end of the workshop, participants will understand common shortcomings of Gen AI, how to use a self-governing alignment technology to overcome those shortcomings and to make GenAI useful for user specific tasks. ","concepts of large language models, text 2 image models.",Workshop,4.0,"Ethics, governance compliance ","Generative AI Alignment, Fairness and Safety, Auto Alignment Technology","Rahm Hafiz is the co-founder and CTO of Armilla AI, a company helping institutions testing and building more fair, safe, trustworthy, and useful AI. In the past, Rahm headed AI initiatives at Outside IQ and Exiger where he worked with global financial institutions, government and regulatory bodies to bring innovative AI into reality. Rahm’s PhD work offers an efficient and modular framework for syntactic and semantic analyses and understanding of natural language by addressing some long standing NLP problems including correct processing of ambiguity. Rahm’s research on NLP has been published in over 10 reputable journals and conferences. 

Dan Adamson is the Co-Founder and CEO of Armilla.AI, a company helping institutions testing and building more fair, safe, trustworthy, and useful AI. Previously, he founded and served as OutsideIQ’s CEO, since its inception in 2010 until its acquisition in 2017 by Exiger, where he remained as their President overseeing product and cognitive computing research. Dan also previously served as Chief Architect at Medstory, a vertical search start-up acquired by Microsoft.  He is an expert on vertical search, investigative use of big data and cognitive computing with more than 15 years in the industry. He holds several search algorithm and cognitive computing patents, has been named among the most influential “must-see” thought leaders in AI and FinTech, in addition to being a recipient of numerous academic awards and holding a Master of Science degree from U.C. Berkeley.  ",https://www.youtube.com/watch?v=btHYUxBFlu8,"Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology & Service, Marketing & Advertising
",,TMLS 2023,btHYUxBFlu8
Amir Hossein Karimi,University of Waterloo,Assistant Professor,"Advances in Algorithmic Recourse: Ensuring Causal Consistency, Fairness, & Robustness","Explore the intersection of causal inference and explainable AI applied for fair and robust algorithmic recourse in AI applications across healthcare, insurance, and banking. This session highlights the role of causal consistency in correcting biases and ensuring transparent model decisions."," * Foundations of Causal Inference: Understand the basics and importance of causal reasoning in AI.

* Integrating Causality in AI Systems: Practical approaches for embedding causal methods to improve fairness and accountability.

* Case Studies: Insights from healthcare, insurance, and banking on implementing causal tools for better decision-making.

* Future Trends: Emerging technologies and methodologies in algorithmic recourse that are setting the stage for more reliable AI systems.",,Research or Advanced Technical,5.0,"Ethics, governance compliance ","Amir	Hossein Karimi	519-242-9170	amirhkarimi@gmail.com	University of Waterloo	Assistant Professor	Advances in Algorithmic Recourse: Ensuring Causal Consistency, Fairness, & Robustness	 	Explore the intersection of causal inference and explainable AI applied for fair and robust algorithmic recourse in AI applications across healthcare, insurance, and banking. This session highlights the role of causal consistency in correcting biases and ensuring transparent model decisions.	"" * Foundations of Causal Inference: Understand the basics and importance of causal reasoning in AI.

* Integrating Causality in AI Systems: Practical approaches for embedding causal methods to improve fairness and accountability.

* Case Studies: Insights from healthcare, insurance, and banking on implementing causal tools for better decision-making.

* Future Trends: Emerging technologies and methodologies in algorithmic recourse that are setting the stage for more reliable AI systems.""	Research or Advanced Technical","Dr. Amir-Hossein Karimi is an Assistant Professor in the Electrical & Computer Engineering department at the University of Waterloo where he leads the Collaborative Human-AI Reasoning Machines (CHARM) Lab. The lab’s mission is to advance the state of the art in artificial intelligence and chart the path for trustworthy human-AI symbiosis. In particular, the group is interested in the development of systems that can recover from or amend poor experiences caused by AI decisions, assay the safety, factuality, and ethics of AI systems to foster trust in AI, and effectively combine human and machine abilities in various domains such as healthcare and education. As such, the lab’s research explores the intriguing intersection of causal inference, explainable AI, and program synthesis, among others.

Amir-Hossein’s research contributions have been showcased at esteemed AI and ML-related platforms like NeurIPS, ICML, AAAI, AISTATS, ACM-FAccT, and ACM-AIES, via spotlight and oral presentations, as well as through a book chapter and a highly regarded survey paper in the ACM Computing Surveys. Before joining the University of Waterloo, Amir-Hossein gained extensive industry experience at Meta, Google Brain, and DeepMind and offered AI consulting services worth over $250,000 to numerous startups and incubators. His academic and non-academic endeavours have been honoured with awards like the Spirit of Engineering Science Award (UofToronto, 2015), the Alumni Gold Medal Award (UWaterloo, 2018), the NSERC Canada Graduate Scholarship (2018), the Google PhD Fellowship (2021), and the ETH Zurich Medal (2024).",https://www.youtube.com/watch?v=amgBv-E3-sQ,"Banking & Financial Services, Information Technology & Service, Insurance
",,TMLS 2024,amgBv-E3-sQ
Rob Levy,Lightning AI,Staff Engineer,Deploying and Evaluating RAG pipelines with Lightning Studios,Learn how to use Lightning Studios to quickly deploy AI agents and accelerate your evaluation of RAG pipelines.,Learn how to use Lightning Studios to quickly deploy AI agents and accelerate your evaluation of RAG pipelines.,,Applied Case Studies,5.0,Deployment and integration,"Lightning Studios, RAG Pipelines, AI Agents
","Robert Levy is a Staff Engineer at Lightning AI, a NYC-based startup. At Lightning AI, the team is focused on defining the next-generation development and deployment pipeline for other AI and ML operators. Specifically, Rob's work focuses on the data ingress and egress needs of applied models and the optimization of model deployments.",https://www.youtube.com/watch?v=JO2mJ00rCkU,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance
",,TMLS 2024,JO2mJ00rCkU
"Danny Chiao, Eddie Esquivel, Abhin Chhabra","Tecton, Shopify","Tech Lead, Feast, Sr. Solutions Architect, ML Platform Tech Lead",Building a Fraud Detection Model with Feature Stores (Includes Bonus Case Study: How Shopify uses Feast to Manage its ML Features),"In this workshop, we'll show how to build a real-time fraud detection system using some of the latest tooling for managing ML data pipelines. We'll walk through the process of building, deploying, and serving real-time data pipelines, highlighting the differences between a traditional feature store (using Feast, the open source feature store) and a feature platform (using Tecton). 

We'll present common architectural patterns and walk you through building a model in three stages:

- Batch, daily computed predictions

- Online predictions using batch features

- Online predictions using real-time features



","You will learn how to:

- Build new features

- Automate the transformation of batch data

- Automate the transformation of streaming and real-time data

- Create training datasets

- Serve data online using DynamoDB or Redis

- Build fraud detection system using Tecton and Feast 
","Attendees should have functional knowledge of Python, SQL and Spark, as well as familiarity with the challenges of data engineering for ML.",Workshop,4.0,Deployment and integration,,"Danny Chiao is an engineering lead at Tecton/Feast Inc working on building a next-generation feature store. Previously, Danny was a technical lead at Google working on end to end machine learning problems within Google Workspace, helping build privacy-aware ML platforms / data pipelines and working with research and product teams to deliver large-scale ML powered enterprise functionality. Danny holds a Bachelor’s degree in Computer Science from MIT. | 

Eddie Esquivel is a Solutions Architect at Tecton, where he helps customers implement feature stores as part of their stack for Operational ML. Prior to Tecton, Eddie was a Solutions Architect at AWS.

Abhin leads the feature store team for Shopify's ML Platform. ",https://www.youtube.com/watch?v=2Ztf0gYzubs,,"Danny and Eddie are core members of the Feast and Tecton Engineering and Solutions Architect teams. They have deep expertise in working with dozens of end-users to build real-time recommendation systems using feature stores. They also have a lot of experience working on ML infrastructure at Google, AWS, and Tecton.
",TMLS 2022,2Ztf0gYzubs
Michael Thriffiley,Reka AI,Head of Sales,Multimodal Agents You Can Deploy Anywhere,"Reka develops multimodal AI that can be deployed in the cloud, on premises, or on devices. Our frontier models are trained from scratch in an end-to-end fashion to understand text, images, video, and audio. They address the needs of both enterprises and consumers for building powerful applications such as video analysis, speech-to-speech translation, and multimodal document understanding. Join us as we share about how you can use Reka models and our agentic framework to build agents that can see, hear, and speak.",,,Furture of AI,,Introduction to MLOps and GenAI,"Multimodal AI, Agentic framework, Model deployment","David Cheng is an Engineering Lead at Reka, an AI Research and Product company building multimodal artificial intelligence to empower organisations and businesses. He holds a degree in Computer Science from Caltech. Before Reka, David worked at Google Cloud, leading teams around distributed databases and applied ML.",https://www.youtube.com/watch?v=hU2mQTKYA80,,,MLOps & GenAI World 2024,hU2mQTKYA80
Niels Bantilan,Union.ai,Chief ML Engineer,RAG Hyperparameter Optimization: Translating a Traditional ML Design Pattern to RAG Applications," In the era of Foundation LLMs, a lot of energy has moved from the model training stage to the inference stage of the ML lifecycle, as we can see in the explosion of different RAG architectures. But has a lot changed in terms of the techniques to systematically improve performance of models at inference time? In this talk, we’ll recast hyperparameter optimization in terms of improving RAG pipelines against a “golden evaluation dataset” and see that not much has changed at a fundamental level: gridsearch, random search, and bayesian optimization still apply, and we can use these tried and true techniques for any type of inference architecture. All you need is a high quality dataset."," You’ll learn about hyperparameter optimization (HPO) techniques that are typically used in model training and apply them to the context of RAG applications. This session will highlight the conceptual and practical differences when implementing HPO in the AI inference setting and see how some of the traditional concepts in ML still apply, such as the bias-variance tradeoff.",,Research or Advanced Technical,4.0,Performance optimization and efficiency,"Hyperparameter optimization (HPO), RAG pipelines, Inference optimization","Niels is the Chief Machine Learning Engineer at Union.ai, and core maintainer of Flyte, an open source workflow orchestration tool, author of UnionML, an MLOps framework for machine learning microservices, and creator of Pandera, a statistical typing and data testing tool for scientific data containers. His mission is to help data science and machine learning practitioners be more productive.",https://www.youtube.com/watch?v=Z586T79U2e8,"Computer Software, Information Technology Services","This session revisits traditional concepts in ML in the context of modern AI inference architectures to explore what assumptions still hold, and which ones we can break.",MLOps & GenAI World 2024,Z586T79U2e8
Zain Hassan,Weaviate,Senior ML Developer Advocate,Scaling Vector Database Usage Without Breaking the Bank: Quantization and Adaptive Retrieval,"Everybody loves vector search and enterprises now see its value thanks to the popularity of LLMs and RAG. The problem is that prod-level deployment of vector search requires boatloads of both CPU, for search, and GPU, for inference, compute. The bottom line is that if deployed incorrectly vector search can be prohibitively expensive compared to classical alternatives.

The solution: quantizing vectors and performing adaptive retrieval. These techniques allow you to scale applications into production by allowing you to balance and tune memory costs, latency performance, and retrieval accuracy very reliably.

I’ll talk about how you can perform realtime billion-scale vector search on your laptop! This includes covering different quantization techniques, including product, binary, scalar and matryoshka quantization that can be used to compress vectors trading off memory requirements for accuracy. I’ll also introduce the concept of adaptive retrieval where you first perform cheap hardware-optimized low-accuracy search to identify retrieval candidates using compressed vectors followed by a slower, higher-accuracy search to rescore and correct.

These quantization techniques when used with well-thought-out adaptive retrieval can lead to a 32x reduction in memory cost requirements at the cost of ~ 5% loss in retrieval recall in your RAG stack.",How to scale the usage of vector databases using vector quantization techniques to reduce memory footprint and adaptive retrieval to minimize loss of accuracy that usually occurs when using compressed vectors.,,Workshop,3.0,Performance optimization and efficiency,"Vector search, quantization techniques, adaptive retrieval.






","Zain Hasan is a Senior Developer Advocate at Weaviate an open-source vector database. He is an engineer and data scientist by training, who pursued his undergraduate and graduate work at the University of Toronto building artificially intelligent assistive technologies. He then founded his company developing a digital health platform that leveraged machine learning to remotely monitor chronically ill patients. More recently he practiced as a consultant senior data scientist in Toronto. He is passionate about open-source software, education, community, and machine learning and has delivered workshops and talks at multiple events and conferences.",https://www.youtube.com/watch?v=BtMXkUr5tj4,,,TMLS 2024,BtMXkUr5tj4
Eric Duffy,Tentorrent,Sr Director Business Development,RISC-y Business: How open chip architectures are changing the AI ecosystem for the better,"The RISC-V ISA has been described as the 'Linux of the chip world'.  It has been used in many small scale applications, but in recent times it has seen a rise in popularity for high performance system designs for AI and HPC applications... all of this, in spite of a relatively nascent software ecosystem.  Learn why companies like Tenstorrent are turning to RISC-V over traditional x86 and ARM architectures.",,,Ignite Lighting Talk,,Future trends,"RISC-V, AI Ecosystem, Chip Architectures","Eric Duffy, from Tenstorrent, is a Computer Scientist by training and a Business Development professional by trade.  Eric has previously worked in consulting for Data Management, FinTech and Cloud systems, and has worked with companies across public and private sectors to deliver high-performance analytical and Machine Learning solutions.  

Today, Eric is focused on the intersection between AI/ML algorithms and next-generation computers.",https://www.youtube.com/watch?v=Ekz6vd8ex4Q,,,TMLS 2023,Ekz6vd8ex4Q
Alex Kim,Iterative.ai,Solutions Engineer,ML Experimentation with DVC and VS Code,"Learn how to manage and make your machine learning projects reproducible with an open-source tool DVC and its extension for VS Code. 
We will see how to track datasets and models, run, compare, visualize, and track machine learning experiments right in VS Code IDE.",Learn how to generate many reproducible ML experiments without leaving the context of their IDE,"Python, ML basis, VS Code",Workshop,4.0,"Model dev, training, arch.",,"Alex Kim is a Solutions Engineer at Iterative. His background is in physics, software engineering, and machine learning. In the last couple of years, he became increasingly interested in the engineering side of ML projects: processes and tools needed to go from an idea to a production solution.",https://www.youtube.com/watch?v=sRpKkbN6U-M,,They'll be able to see one way to solve the frustration of tuning model parameters before you can really start training the model.,,sRpKkbN6U-M
Jullian Yapeter,Signal 1,Machine Learning Scientist,AI for Hospitals at Scale,"An exploration into the technical processes employed at Signal 1 that enables the training and deployment of machine learning models across various hospital settings, including zero-shot learning applications in patient deterioration prediction that generalizes even to unseen hospitals.

This talk will also cover the specifics of our microservice architecture which underpins our system's capability to consistently deliver timely and effective inference results, enabling scalable, data-driven decisions in patient care.

Attendees will gain insights into the practical challenges and solutions encountered in developing AI applications that can seamlessly integrate into and impact real-world clinical settings.

Whether you're interested in the nuances of model development, deployment, or the practical implications of AI in healthcare, this session will offer valuable technical knowledge and perspectives.

We invite you to join this technical discourse at the intersection of AI and healthcare, contributing to a dialogue that's shaping the future of AI applications in medical settings."," - An overview of the practical challenges in deploying ML in hospitals, such as generalization and scalability
- How we at Signal 1 tackle some of these challenges
- Discussions about some of the problems we're still working on",,Research or Advanced Technical,5.0,Deployment and integration,"AI in Healthcare, Machine Learning Models, Scalability and Deployment


","Jullian is a Machine Learning Scientist at Signal 1. His focus is at the intersection of model development and ML infrastructure / MLOps. He is an engineer with a BASc. in Mechatronics Engineering from the University of Waterloo, and a M.S. in Computer Science from the University of Southern California. He was a research assistant at the CLVR Lab at USC, working on large-scale Offline RL under Prof. Joseph Lim. Jullian has industry experience working on AI / Computer Vision systems at Disney Imagineering, IBM, and a few different start-ups. Overall, he’s passionate about improving people’s lives through technology.",https://www.youtube.com/watch?v=9qij_-VoRQ0,"Computer Software, Hospital & Health Care, Information Technology & Service
",,TMLS 2024,9qij_-VoRQ0
Susrutha Gongalla,Stitch Fix,Principal Machine Learning Engineer,Supercharging Recommender Systems: Unleashing the Power of Distributed Model Training,"Stitch Fix utilizes a sophisticated multi-tiered recommender system stack, encompassing feature generation, scoring, ranking, and business policy decision-making. This presentation delves into the training architecture of the scoring model, a deep learning model that predicts the likelihood of a user purchasing an item. I will walk through our journey, detailing the transition from training on a single GPU to leveraging multiple GPUs through pytorch's Distributed Data Parallel (DDP) strategy. Additionally, I will share empirical results highlighting the efficiency of GPU utilization as we scale up with DDP across multiple GPUs.
"," I would like to show the implementation details of moving from using one GPU to multiple GPUs for model training. I hope this will give the attendees enough knowledge to implement it themselves, allowing them to train bigger (and better) machine learning models.",,Case Study,6.0,"Model dev, training, arch.","Recommender systems, Distributed model training, PyTorch Distributed Data Parallel (DDP)
","Susrutha Gongalla is an experienced Machine Learning Engineer with over 8 years of experience in developing end-to-end machine learning models. She is currently a Principal Machine Learning Engineer in the recommender systems team at Stitch Fix, where she leads the optimization of the end-to-end model lifecycle stack. Her primary focus is on developing and improving the scoring model that generates purchase probabilities for clothing items. Prior to joining Stitch Fix, Susrutha worked as a tech lead at Intuit, where she led the development of recommender systems to improve customer engagement and reduce churn. Besides recommender systems, she also worked on projects using Natural Language Processing techniques to derive insights from unstructured data. Her passion lies in using machine learning to drive business impact and data-driven decision making.

Susrutha holds a Master's degree from Carnegie Mellon University and a Bachelor's degree from Indian Institute of Technology Indore. She has several patents in applications of machine learning and is a recognized leader in the field. ",https://www.youtube.com/watch?v=8OBLdOKJffo,"Computer Software, Information Technology & Service, Recommender Systems
",,MLOps & GenAI World 2023,8OBLdOKJffo
Ezequiel Lanza,Intel,AI Open Source Evangelist,OpenFL: A Federated Learning Project to Power (and secure) Your Projects,"OpenFL is a Python 3 framework for Federated Learning. Designed to be flexible, extensible and easily learnable tool for data scientists is a community supported project that enables organizations to collaboratively train a model without sharing sensitive information, originally developed by Intel Labs and the Intel Internet of Things Group. The team would like to encourage any contributions, and aims to be community-driven. It employs narrow interfaces and allows running all the processes within Trusted Execution Environments (TEE) to provide confidentiality of data and models, integrity of computation, and enable attestation of compute resources. 
To protect information while still leveraging ML models to automate scan analysis, Intel Labs and UPenn used data from over 71 medical institutions to apply and test the efficacy of federated learning for brain tumor edge detection. https://d1io3yog0oux5.cloudfront.net/_49fa703c82f3582cf6fc2f8361247b8e/intel/news/2022-12-05_Intel_and_Penn_Medicine_Announce_Results_of_1593.pdf

With FL hardware and software, sensitive data can be secured at the source, while the AI model still benefits from a larger data set.

Learn how you can adopt, contribute and secure federated learning.",How Federated Learning can be implemented in a secure way.,,Technical,3.0,Security and Privacy,"Federated Learning, Data Privacy, Secure AI Models





","Passionate about helping people discover the exciting world of artificial intelligence, Ezequiel is a frequent AI conference presenter and the creator of use cases, tutorials, and guides that help developers adopt open source AI open source tools.",https://www.youtube.com/watch?v=IpOIQ994vko,"Banking & Financial Services, Environmental Services, Hospital & Health Care
","I've been working for the last year on this project
",TMLS 2023,IpOIQ994vko
Jordan Shaw,Half Helix,Creative Technology Lead,Tips & Tricks for Intentional Text-to-Image Generation,A creative evolution of ML in the arts. The speaker started producing artwork and exhibiting ML pieces in 2015. The reflective perspective of progression in ML and AI while observing creative model biases provides a unique perspective for the future of ML in creative fields and may influence popular culture down the road.,"The exploration, evolution and progression of ML throughout the last couple of years through a creative lens. ",,Technical,4.0,Future trends,,"Jordan Shaw is an artist, and creative technologist raised and is currently based in Toronto, Canada. He grew up in Scarborough and received his MFA from OCAD University's Digital Futures program leading to his thesis being exhibited during Vector Festival at InterAccess. Before that, he completed his undergraduate degrees at Carleton University and Algonquin College, where his final installation was exhibited and recognized during ACM SIGGRAPH.

His work is related to exposing the hidden and unseen aspects of technology and the digital environment around us. The manifestation of this work tries to visualize the hidden interactions between people and technology, data collection and these digital systems trying to understand the physical world.

Jordan has exhibited internationally in Australia, Canada, Germany, Spain and the United States of America. ",https://www.youtube.com/watch?v=keOq5U74H9Q,,,TMLS 2022,keOq5U74H9Q
Milan Kordic,Tenstorrent,Senior Machine Learning Engineer,Introducing the Tenstorrent Model Zoo,"Tenstorrent AI accelerator hardware is specially designed to accelerate artificial intelligence and machine learning applications, competing on a performance-per-dollar basis. For developers and engineers, having access to efficient AI computing power and an easy-to-use software API is critical for running large-scale and compute-intensive models such as BERT, GPT3, BART, ResNet50, and T5. In this workshop, we will introduce the Tenstorrent developer ecosystem including an overview of the hardware product line, the end-to-end software stack including BUDA, PyBUDA and Model Zoo, the stages of an AI model from pre-training, fine-tuning, evaluation and inference, and a hands-on demo of the Tenstorrent Model Zoo highlighting the key steps developers need to take to get their model running on Tenstorrent AI hardware.","- Overview of Tenstorrent's hardware product line
- End-to-end overview of Tenstorrent's software stack
- Stages of an AI model from pre-training, fine-tuning, evaluation and inference
- Hands-on demo of the Tenstorrent Model Zoo","- Knowledge of deep neural network models used for applications such as NLP and computer vision
- Knowledge of machine learning model training and inference
- Knowledge of computer hardware such as CPU, GPU, AI accelerators, etc. is helpful, but not required",Workshop,4.0,Deployment and integration,,"Milan is a Senior Machine Learning Engineer at Tenstorrent and a member of the Customer Success team. His role is to support Tenstorrent customers and the community of ML developers using Tenstorrent hardware to successfully build and deploy their AI solutions. With an educational background in Electrical and Computer Engineering and past work experiences as a Machine Learning Engineer, Data Scientist, and Analytics Engineer, Milan has strong knowledge of AI / ML systems and computer hardware architecture.",https://www.youtube.com/watch?v=NOjyDYDEUC4,,,TMLS 2022,NOjyDYDEUC4
Serg Masis,Syngenta,Lead Data Scientist & Bestselling Author,QA in ML,"Trust is mission-critical for any technology, so if AI/ML solutions are to supplant and complement software, AI must reach the reliability standards currently expected from software. The difference is Quality Assurance (QA) has existed in software for three decades, and the burgeoning field of ML has barely begun to perform quality controls:
1) We will take a journey through the history of QA, discuss why it is crucial, and what lessons from other disciplines and industries we can apply to machine learning.
2) Then, will discuss what important role Explainable AI methods, not to mention best practices in MLOps, data engineering, and data science, can play.
3) Lastly, we will discuss the challenge ahead. Given the many steps in an Machine learning (ML) and the many qualities to assess in an ML model, choreographing and standardizing tasks in a QA effort is a challenging undertaking. New roles for ML QA will likely appear within DevOps, SecOps, and MLOps teams to ensure increased reliability and robustness. Still, also, the roles of data scientist and Machine Learning engineer will evolve to enhance quality.
Thus session is ultimately about what business stakeholders and practitioners can do to make AI/ML more trustworthy to the end-users of this technology. ","- What the history of quality assurance (QA) teaches us about how QA can be implemented in ML.
- What tools and roles already exist in ML that can enforce QA.
- What's missing to make QA work much better.",-,,2.0,"Ethics, governance compliance ","QA in ML, Explainable AI, MLOps
",Bestselling author of ML/AI books. Lead Data Scientist at multinational agribusiness company creating models for sustainable practices in agriculture.,https://www.youtube.com/watch?v=OOXA-DEBVS0,"ANY
",The connection between quality assurance and AI/ML which isn't online,,OOXA-DEBVS0
Jaeman An,VESSL AI,Co-founder & CEO,Mastering Enterprise-Grade LLM Deployment: Overcoming Production Challenges,"This session delves into the practical challenges of deploying Large Language Models (LLMs) in production, particularly for enterprise applications. We’ll cover topics such as managing computational resources, optimizing model performance, ensuring data security, and adhering to compliance standards. The talk will also showcase strategies to mitigate these challenges, focusing on infrastructure management, latency reduction, and model reliability. Case studies from industries such as healthcare, finance, and e-commerce will illustrate how enterprises can safely and efficiently integrate LLMs into their existing systems.",,,Ignite Lighting Talk,,Deployment and integration,"LLM deployment, Enterprise applications, Model reliability","Jaeman An graduated from KAIST with a degree in Electrical and Electronic Engineering. He has extensive expertise in DevOps and machine learning, having played a pivotal role in managing the DevOps infrastructure for the mobile game 'Cookie Run,' which achieved 10 million daily active users. He later joined a medical AI startup as VP of Engineering, where he successfully launched and operated various AI services. Through this experience, he identified inefficiencies in the machine learning development and operations processes and founded VESSL AI to address these challenges.",https://www.youtube.com/watch?v=dRgwzUk1s-g,"Computer Software, Information Technology Services, Hospital & Health Care, Banking & Financial Services, Insurance","This session stands out by offering practical, real-world strategies for overcoming the unique challenges of deploying LLMs in production, with specific case studies from industries that require stringent security and performance standards.",MLOps & GenAI World 2024,dRgwzUk1s-g
Amber Roberts,Arize AI,MLE,Troubleshooting your ML Models in Production,"Taking a model from research to production is hard — and keeping it there is even harder! As more machine learning models are deployed into production, it is imperative to have tools to monitor, troubleshoot, and explain model decisions. In this workshop attendees will implement ML observability firsthand in the Arize platform to see if their fraud model is drifting, underperforming, and/or exhibiting bias. Participants will monitor, surface, resolve, and improve performance on ML models in production.","In this workshop, you’ll learn best practices for how to:

Account for model, feature and actuals drift to ensure your models stay relevant
Troubleshoot performance degradations across various cohorts
Avoid common pitfalls from misleading evaluation metrics to imbalanced datasets
",,Workshop,3.0,Performance optimization and efficiency,,"Amber Roberts is a community-oriented Machine Learning Engineer at Arize AI, an ML observability company. Amber’s role at Arize looks to help teams across all industries build ML Observability into their productionalized AI environments. Previously, Amber was a product manager of AI at Splunk and the Head of Artificial Intelligence at Insight Data Science. A Carnegie Fellow, Amber has an MS in Astrophysics from the Universidad de Chile.",https://www.youtube.com/watch?v=tqJeglryVOk,,"A unique perspective on how to approach responsible AI from an operational perspective, beyond value propositions and ideals. Panel members will be major thought leaders championing responsible, fair, and ethical AI, examples found here - who all have unique and diverse educational and professional backgrounds. In addition, Aparna will cover novel explainability approaches to help manage the AI transparency problem. 
",TMLS 2022,tqJeglryVOk
Matthew Honnibal,Explosion AI,Founder and CTO,How many Labelled Examples do you need for a BERT-sized Model to Beat GPT4 on Predictive Tasks?,"Large Language Models (LLMs) offer a new machine learning interaction paradigm: in-context learning. This approach is clearly much better than approaches that rely on explicit labelled data for a wide variety of generative tasks (e.g. summarisation, question answering, paraphrasing). In-context learning can also be applied to predictive tasks such as text categorization and entity recognition, with few or no labelled exemplars.

But how does in-context learning actually compare to supervised approaches on those tasks? The key advantage is you need less data, but how many labelled examples do you need on different problems before a BERT-sized model can beat GPT4 in accuracy?

The answer might surprise you: models with fewer than 1b parameters are actually very good at classic predictive NLP, while in-context learning struggles on many problem shapes --- especially tasks with many labels or that require structured prediction. Methods of improving in-context learning accuracy involve increasing trade-offs of speed for accuracy, suggesting that distillation and LLM-guided annotation will be the most practical approaches.

Implementation of this approach is discussed with reference to the spaCy open-source library and the Prodigy annotation tool.","On predictive tasks, LLMs currently perform much worse than you'd expect if you benchmark them directly against other approaches. If you're working on a task where it's not worth putting 20-40 hours of effort into the model, just use an LLM. If it's a problem worth doing well, prototype with the LLM, but also create training and evaluation data, and compare approaches.",,Advanced Technical/Research,5.0,"Model dev, training, arch.","In-context learning, Predictive tasks, Labeled data requirements
","Major work includes:

* ExplosionAI GmbH: Founder and CTO
* spaCy: Open-source NLP library in use by thousands of companies, with over 100m downloads. Particularly known for efficiency and API design.
* Prodigy: Developer-focussed annotation tool, with active learning and scriptability features. Licenses purchases by almost 1000 companies.
* Thinc: Open-source ML library built for spaCy, designed around function composition.

Entered the field in 2005 as a linguist, transitioning towards computer science over PhD and post-doctoral research. Left academia in 2014. Originally from Sydney, now in Berlin.",https://www.youtube.com/watch?v=3iaxLTKJROc,"Banking & Financial Services, Computer Software, Information Technology & Service, Insurance
",,MLOps & GenAI World 2023,3iaxLTKJROc
Myles Harrison,NLP from Scratch,Consultant and Trainer,Getting started with Generative Text and Fine-tuning LLMs in Hugging Face,"If you're new to working with LLMs hands-on in code, this is the session for you! In this introductory workshop, you'll get working with Hugging Face and the transformers library for generating text from LLMs and applying performance efficient fine-tuning methods to a generative text model.

Whether you are starting from near zero or have some prior knowledge of large language models, this workshop is your jumping off point to get you started on working with LLMs."," •​​ Define large language models (LLMs) and the transformer architecture; understand the history of their development, key concepts, and high-level details of their structure and function
•​​ Be introduced to Hugging Face and the transformers library and see applications thereof in code, using LLMs for generative text
•​​ Define fine-tuning and understand the motivation for applying it to existing large language models for generative text 
• Applying fine-tuning to a generative text model using the Hugging Face transformers library and a text dataset
• Be introduced to approaches for working with large language models efficiently on consumer hardware: performance efficient fine-tuning (PEFT) and model quantization",Basic python & knowledge of LLMs,Workshop,3.0,"Model dev, training, arch.","Hugging Face, Fine-Tuning, Generative Text






",,https://www.youtube.com/watch?v=i51OAQ0h-gQ,Other,,TMLS 2024,i51OAQ0h-gQ
Meryem Arik,TitanML,CEO,"Navigating LLM Deployment: Tips, Tricks and Techniques","Unlock the power of self-hosted language models to drive innovation in financial services, healthcare, defense, and beyond. Join our expert session to learn industry best practices for optimizing, deploying, and monitoring these cutting-edge AI solutions in-house. Through real-world case studies, Meryem Arik, CEO of TitanML, will share practical tips to help you navigate the challenges and maximize the value of bringing large language models into your organization's AI workflow. Walk away with the knowledge and confidence to leverage self-hosted LLMs to power your next-generation applications and maintain your competitive edge.

"," 1. Best practices for optimizing, deploying, and monitoring self-hosted language models. The talk will provide practical tips and real-world case studies to guide attendees on effectively implementing these powerful AI solutions in-house.

2. Understanding the challenges and opportunities of self-hosted LLMs. Attendees will learn how to navigate the potential hurdles and maximize the value of integrating these cutting-edge language models into their organization's AI workflow.

3. Confidence and knowledge to leverage self-hosted LLMs for building next-gen applications. The session aims to empower attendees with the insights and expertise needed to harness the power of self-hosted language models to drive innovation, maintain a competitive edge, and create applications in critical industries like finance, healthcare, and defense.

In essence, the talk focuses on equipping attendees with the practical know-how, strategic understanding, and inspiration to successfully adopt and utilize self-hosted LLMs within their enterprises to power transformative AI solutions.",,Workshop,3.0,Deployment and integration,"Self-hosted LLMs, optimizing AI solutions, deployment best practices.






","Meryem Arik is the Co-founder and CEO of TitanML, a pioneering company empowering enterprises to harness the full potential of Generative AI without compromising on data security.
A Forbes 30 Under 30 honoree, Meryem spent several years as a rates derivatives structurer at Barclays, covering major corporate, sovereign and supranational clients across EMEA. She holds a Master's degree in Physics and Philosophy from the University of Oxford.
At TitanML, Meryem is on a mission to accelerate enterprise adoption of cutting-edge AI technologies by providing a secure and scalable foundation for building mission-critical applications. Under her leadership, TitanML has become the platform of choice for organizations seeking to leverage Generative AI while maintaining complete control over their sensitive data.",https://www.youtube.com/watch?v=12lS3NZmFJ0,"Banking & Financial Services, Insurance, Hospital & Health Care
",,TMLS 2024,12lS3NZmFJ0
"Fion Lee-Madan, David Van Bruwaene, Susie Lindsay","FAIRLY AI, Law Commission of Ontario","Co-Founder and COO, Founder and CEO, Counsel",Managing AI and the Associated Risks in a Regulatory Environment,"Artificial Intelligence (AI) is a tool that can be used both to manage risks in high-stake industries such as financial services as well as poses some risks by itself. This expert panel will discuss AI yesterday versus AI today, focusing on how AI has evolved and developed from simple automation to complex decisioning. Our experts will cover AI regulatory trends globally and in the US, best practices in risk and compliance management, and complementing technologies that can be utilized to counter new and emerging AI risks.",Latest expert views on managing AI risks in the fast changing regulatory environment,,Business,2.0,"Ethics, governance compliance ",,"Fion has over 20+ years of experience in enterprise software as a Solutions Architect (ex-Sapient, ex-Intuit, ex-ATG - acquired by Oracle.) She double majored in Computer Science and Human Biology at the University of Toronto and has an MBA from Boston University. She is a technical committee member of the CIO Strategy Council of Canada. She is a champion of DE&I, and a major supporter for women in tech as both a mentor and coach. She guest lectures for AI Ethics at Lighthouse Labs, an education company with a goal to bring more diversity into the Data Science field.

David has developed a deep understanding of ethics and formal logic, model theory, and Natural Language Processing (NLP) throughout his career in business and academia. He taught Business Ethics at the University of Waterloo and graduate level Ethics at Cornell and is a sought-after speaker for AI Ethics, Compliance and Risk Management at conferences around the world. On an exchange scholarship from Cornell to Berkeley, he became fascinated with powerful Natural Language Processing. David applied this background to cyberbullying and related NLP in his first AI startup ViSR (acquired by SafetoNet) where he was the Head of Data Science, then became the CEO and Board Member. His unique perspective as a practicing Data Scientist to being the CEO and Board Member at the top made David conscious of tensions between technical vs business decisions and the impact on people’s lives in the resulting automation of decision-making. 

Susie is Counsel at the Law Commission of Ontario where she leads numerous LCO projects including: AI and the Civil Justice System, Protection Orders, and the LCO’s joint initiative on AI and Human Rights with the Ontario Human Rights Commission and Canadian Human Rights Commission.  Before joining the Commission Susie practised regulatory law at a large communications company, and civil defence litigation at a boutique litigation firm. Susie is a graduate of Queen’s Law School, a Fulbright Scholar, has a Master of Laws from Harvard Law School and was a fellow at the Berkman Klein Centre for Internet & Society at Harvard University.",https://www.youtube.com/watch?v=9GmebUl5S48,,We always get repeat invite for following up presentations as this is a trending topic.,,9GmebUl5S48
Kartik Talamadupula,Symbl.ai,Director of AI Research,Building a Conversation-focused LLM on Communication Data,"Communication data is a powerful resource for building large language models (LLMs). In this talk, we will discuss the challenges and opportunities of using communication data to build a conversation-first LLM.

We will introduce Nebula, a conversation-focused LLM from Symbl.ai that is trained and fine-tuned on a large dataset of communication data. Nebula is able to generate and analyze text, translate languages, write different kinds of creative content, and answer questions in an informative way, all with a focus on conversation.

We will discuss the challenges of using communication data to train LLMs, including the need for high-quality data, the need to address data privacy and security concerns, and the need to develop ML Ops pipelines that can handle the scale and complexity of communication data.

We will also discuss the opportunities that communication data presents for building LLMs. For example, communication data can be used to train LLMs that are more aware of the context of a conversation, and that are better able to understand and respond to human emotions.","Specializing generative models for conversations, and the challenges and opportunities there-in",Basic knowledge of AI and ML terminologies,Workshop,3.0,"Model dev, training, arch.","Conversation-focused LLM, Communication data, Nebula
","Kartik Talamadupula is a research scientist and Director of AI Research at Symbl.ai, where he works on Symbl's latest AI product, the Nebula large language model (LLM). Prior to this, he was a Senior Researcher and Research Manager at IBM Research in Yorktown Heights, NY and Cambridge, MA. He obtained his Ph.D. in Artificial Intelligence from Arizona State University in 2014, and has 15+ years' experience in the field of AI.",https://www.youtube.com/watch?v=SIuTl5i-WEg,,,MLOps & GenAI World 2023,SIuTl5i-WEg
"Tina Shen, Charles Zhu",Loblaw Digital,"Machine Learning Engineer, Machine Learning Engineer ",Optimizing Personalized User Experience: In-session Recommendations Across E-commerce Verticals,"This talk presented by Loblaw Digital, delves into the nuanced domain of personalized recommendation systems within e-commerce. The presentation will initiate with an examination of user behaviours on different platforms, as well as existing solutions, identifying their limitations and illuminating the pathway towards innovative solutions that significantly enhance user engagement and experience.

Reflecting on the increasing demand for personalization in today's competitive e-commerce landscape, various use cases of in-session recommendations will be discussed. These are effectively executed across our well-known platforms such as Loblaws (Grocery), Real Canadian Superstore (Grocery), Shoppers Drug Mart (Beauty, Personal Care & Health Products), and Joe Fresh (Fashion Industry).

In highlighting our in-house in-session recommendation model detailed at https://arxiv.org/abs/2401.16433), we show how it takes account multiple item/user data to offer effective personalized suggestions across different shopping situations. The model's flexibility allows it to optimize customer experiences by addressing the complexity of user behaviour in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behaviour (switching intentions) in a shopping session.

An overview of the system design behind this model is presented alongside its impressive performance results - providing you a clear picture of practically applicable solutions enhancing shopper experiences on digital platforms. Additionally, we will briefly share the ongoing online evaluations and discuss anticipated improvements moving forward. By attending this session by Loblaw Digital, attendees can expect to gain comprehensive insights into tailoring recommendations that accurately capture customer needs in real-time across various verticals within e-commerce.","1. Comprehensive understanding of the current landscape and limitations of personalized recommendation systems in e-commerce.

2. Insight into the practical application and benefits of in-session recommendations across various platforms in fashion, grocery, beauty, personal care and health products.

3. Detailed understanding of Loblaw Digital's innovative in-house adaptable recommendation model that leverages multiple item/user data to generate effective personalized, real-time recommendations.

4. Technical insights into our Neural Pattern Associator (NPA), a pioneering item-association-mining model that employs vector quantization to encode common user intentions as quantized representations.

5. An overview on how NPA permits of users' shopping intentions through an attention-driven lookup during the reasoning phase, resulting in coherent and self-interpretable recommendations.

6. A sneak peek into ongoing online evaluations and potential improvements for enhancing e-commerce experiences via tailored real-time product suggestions.",,Applied Case Studies,4.0,Deployment and integration,"Personalized Recommendation Systems, In-session Recommendations, User Behaviour.
","Tianshu (Tina) Shen is a dedicated Machine Learning Engineer at Loblaw Digital, with over 2 years of experience in the e-commerce industry and over 4 years specializing in recommender systems. Tina holds a Master's in Applied Science from the University of Toronto. Her research primarily focused on conversational recommender systems and she has published more than five conference papers during her master’s studies.

At Loblaw Digital, Tina has been instrumental in designing and building diverse machine learning solutions that deliver personalized recommendations across several e-commerce platforms such as Joe Fresh and Real Canadian Superstore. Her work significantly enhances user experiences through personalized and real-time product recommendations across these various verticals.

Through her presentation ""Optimizing Personalized User Experience: In-session Recommendations Across E-commerce Verticals"", Tina aims to share valuable insights acquired from hands-on application of advanced methodologies at Loblaw Digital - inspiring peers and attendees towards innovative strides within today's dynamic e-commerce landscape.

Charles Zhu is a Machine Learning Engineer on the P13n Recommendations Team at Loblaw Digital. He primarily works in machine learning productionization and ML pipeline governance for the company’s Helios Recommender Engine. Prior to working at Loblaw, Charles worked with the City of Toronto analyzing transportation safety data, and in astrophysics as a software engineer.",https://www.youtube.com/watch?v=dN8DeDrYe2E,,,TMLS 2024,dN8DeDrYe2E
Harshit Agarwal,Faire Wholesale Inc,Senior Machine Learning Engineer,"ML Deployment at Faire: Predicting the Future, Serving the Present","How Faire transitioned a traditional infrastructure into a modern, flexible model deployment and serving stack that supports a range of model types, while ensuring operational excellence and scalability in a dynamic e-commerce environment.

Over the past few years at Faire, we have overhauled our ML serving infrastructure, moving from hosting XGBoost models in a monolithic service to a flexible and powerful ML deployment and serving stack that powers all types of models, small and big. 

In this talk, we'll cover how we set up a system that makes it easy to migrate, deploy, scale, and manage different types of models. Key points will include how we set up infrastructure as code and CI/CD pipelines for smooth deployment, automated testing, and created user-friendly tools for managing model releases. We'll also touch on how we built in observability and monitoring to keep an eye on model performance and reliability.

Come and learn how Faire's ML serving stack helps our team quickly bring new ideas to life, while also maintaining the operational stability needed for a growing marketplace."," 1. How to best structure an ML serving and deployment infrastruture
2. How to build testing and observability into your deployment and serving infra
3. How to build production grade tools that your data scientists and MLEs will love
4. See how we are serving users at scale and the design choices that we made",,Research or Advanced Technical,5.0,Deployment and integration,"ML serving infrastructure, Scalability, CI/CD pipelines"," Harshit is a Senior MLE at Faire, currently building out Faire's ML Platform. He has been working on ML infrastructure problems for over 6 years, from feature stores to ML training and serving.",https://www.youtube.com/watch?v=CVJhosjEvYE,Computer Software,"Building to accommodate and migrate a wide set of existing users while supporting new users as well is generally tougher than just building for a new use case. This talk will cover how we did that, what additional requirements it presented, and how doing so is a critical part of building a new production grade system in a large organization.",MLOps & GenAI World 2024,CVJhosjEvYE
Eddie Mattia,Outerbounds,Data Scientist,LLMs from hallucinations to relevant responses,Present a taxonomy of methods for controlling LLMs. Listeners will learn the broad strokes of how apps based on retrieval-augmented generation (RAG) and instruction tuning work and where they fit into the big picture of generative AI. We focus on how these techniques can be used to make generated responses more relevant.,,,Ignite Lightning Talk,,Introduction to MLOps and GenAI,"Controlling LLMs, RAG Methods, Instruction Tuning","Eddie Mattia is a data scientist working on Metaflow and foundation models at Outerbounds. He began using Python to teach applied math in grad school. Since then, Eddie has worked at startups and at Intel building machine learning software.",https://www.youtube.com/watch?v=sm4QGPcXCuA,,,MLOps & GenAI World 2023,sm4QGPcXCuA
Anish Shah,Weights & Biases,ML Engineer,Hemm: Holistic Evaluation of Multi-modal Generative Models," Join Anish Shah for an in-depth session on fine-tuning and evaluating multimodal generative models. This talk will delve into advanced methodologies for optimizing text-to-image diffusion models, with a focus on enhancing image quality and improving prompt comprehension.
Learn how to leverage Weights & Biases for efficient experiment tracking, enabling seamless monitoring and analysis of your model's performance. Additionally, discover how to utilize Weave, a lightweight toolkit for tracking and evaluating LLM applications, to conduct practical and holistic evaluations of multimodal models.
The session will also introduce Hemm, a comprehensive library for benchmarking text-to-image diffusion models on image quality and prompt comprehension, integrated with Weights & Biases and Weave. By the end of this talk, you'll be equipped with cutting-edge tools and techniques to elevate your multimodal generative models to the next level."," Advanced Fine-Tuning Techniques: Explore methods for fine-tuning text-to-image diffusion models to enhance image quality and prompt comprehension.
Optimizing Image Quality: Understand the metrics and practices for assessing and improving the visual fidelity of generated images.
Enhancing Prompt Comprehension: Learn how to ensure your models accurately interpret and respond to complex textual prompts.
Utilizing Weights & Biases: Gain hands-on experience with Weights & Biases for tracking experiments, visualizing results, and collaborating effectively.
Leveraging Weave: Discover how Weave can be used for lightweight tracking and evaluation of LLM applications, providing practical insights into model performance.
Introduction to Hemm: Get acquainted with Hemm and learn how it facilitates comprehensive benchmarking of text-to-image diffusion models.
Holistic Model Evaluation: Learn best practices for conducting thorough evaluations of multimodal models, ensuring they meet desired performance standards across various metrics.",,Workshop,3.0,"Model dev, training, arch.","Multimodal generative models, Fine-tuning techniques, Benchmarking"," Join Anish Shah for an in-depth session on fine-tuning and evaluating multimodal generative models. This talk will delve into advanced methodologies for optimizing text-to-image diffusion models, with a focus on enhancing image quality and improving prompt comprehension.
Learn how to leverage Weights & Biases for efficient experiment tracking, enabling seamless monitoring and analysis of your model's performance. Additionally, discover how to utilize Weave, a lightweight toolkit for tracking and evaluating LLM applications, to conduct practical and holistic evaluations of multimodal models.
The session will also introduce Hemm, a comprehensive library for benchmarking text-to-image diffusion models on image quality and prompt comprehension, integrated with Weights & Biases and Weave. By the end of this talk, you'll be equipped with cutting-edge tools and techniques to elevate your multimodal generative models to the next level.",https://www.youtube.com/watch?v=wHC6pnfWsWI,"Computer Software, Marketing & Advertising, Telecommunications","Seamless integration of Weights & Biases and Weave for robust experiment tracking
Hands-on experience with Hemm, a cutting-edge library for holistic model evaluation
Practical techniques to optimize and evaluate text-to-image models at scale
Streamlined workflows for continuous improvement of multimodal generative models",MLOps & GenAI World 2024,wHC6pnfWsWI
Serena McDonnell,Delphia,Lead Data Scientist,The Role of Alternative Data in Investing,"Applying alternative data to quantitative equity strategies has high potential and unique challenges. In this talk, we will use Delphia's machine learning driven long-short equity market neutral strategy as context to discuss the following:
- Case studies to highlight the advantages of alternative data in investing in general.
- The promise of alternative data in quantitative equity strategies.
- The challenges in working with alternative data in Delphia's strategy","- Understand the advantages of alternative data in investing in general.
- Understand the promise of alternative data in quantitative equity strategies, and the challenges.
- Develop an opinion on the value of alternative data, when to invest in it, and when to consider sticking to more traditional data sources.",,Case Study,4.0,Business and stakeholder alignment,,"Serena is a Lead Data Scientist and quant researcher at Delphia, where she uses machine learning to power the fund's long-short equity market neutral strategy. Passionate about knowledge sharing and continuous learning, Serena co-hosts Deep Random Talks, a podcast which focusses on machine learning, product development, and knowledge management. She is an organizer of AI Socratic Circles (AISC), a highly technical machine learning reading group for industry professionals. As part of AISC, Serena leads a research group that focusses on applying natural language processing and representation learning to recommender systems. Serena holds an M.Sc. in Mathematics from the Hong Kong University of Science and Technology, and a B.S.C. in Mathematics and Biology from McGill University.",https://www.youtube.com/watch?v=o0Pc-xOu_o8,,,TMLS 2022,o0Pc-xOu_o8
Yuval Fernbach,JFrog,VP & CTO,From Silos to Synergy: MLOps & Developers Unified,"In the evolving landscape of software, machine learning is no longer an isolated
component—it's an integral part of the entire secure software supply chain. For ML engineers,
this shift presents an exciting opportunity to go beyond experimentation and model development
to actively contribute to the secure and scalable delivery of AI solutions. This talk will explore
how unifying MLOps with traditional software development processes enhances security,
streamlines deployments, and enables ML models to be part of the broader company-wide
deployment strategy. Learn how becoming part of the overall software supply chain can
empower you to make a bigger impact, ensuring your models reach production safely and
effectively, while aligning with global deployment standards.",,,case study & business strategy,6.0,Deployment and integration,"MLOps, Software supply chain, Model deployment","Yuval Fernbach is the CTO of MLOps at JFrog, previously co-founder and CTO of Qwak.
With over a decade of experience in data and machine learning, Yuval led the creation of a
user-friendly ML Platform that simplifies building, training, and deploying models. Before Qwak,
he served as an ML Specialist at AWS, helping clients harness machine learning to drive
business transformation. Yuval is passionate about using data and technology to foster
innovation.",https://www.youtube.com/watch?v=JKqVc8xf2vo,,,MLOps & GenAI World 2024,JKqVc8xf2vo
Jess  Leung,Recursion,Staff ML Engineer,Creating the World’s Premier Biological Foundation Model,"Recursion has built the world's most expansive biological foundation model, boasting billions of parameters and trained on hundreds of millions of high-resolution images. Dive into an exploration of Recursion's groundbreaking approach that is reshaping and revolutionizing the drug discovery process. This talk will provide an in-depth look at the infrastructure and tooling necessary for building such models. We'll share insights into the intricate process of efficient data management, large-scale model training, scaling inference, and effective use of our in-house supercomputer and public cloud environments.","What it takes (infrastructure, techniques, talent, culture and practices) that is involved in training large deep learning models",,Case Study,5.0,"Model dev, training, arch.","Biological foundation model, Large-scale model training, Drug discovery","Jess Leung has been shipping machine learning to production throughout their career.  They are currently a Staff Machine Learning Engineer at Recursion, where they lead the ML Platform team.  Prior to Recursion, Jess has held technical leadership where they have shipped products in a wide variety of domains including: internet-scale platforms, e-commerce, life science solutions, public transportation services, and financial systems.  Jess holds a B.Sci in Electrical Engineering from Queen's University.",https://www.youtube.com/watch?v=DEDEKvIL1l8,,"Applying ML to Biology/Chemistry
",MLOps & GenAI World 2023,DEDEKvIL1l8
Vinay Kumar Sankarapu,Arya.ai,Founder and CEO,DL-Backtrace by AryaXAI: A Model Agnostic Explainability for Any Deep Learning Models (LLMs to CV)," In today's rapidly evolving AI landscape, deep learning models have become increasingly complex and opaque. They often function as ""black boxes"" that make decisions without transparent reasoning. This lack of explainability raises concerns in mission-critical applications where understanding the ""why"" behind a model's decision is as important as the decision itself.

In this talk, we will introduce DL-Backtrace, a new technique designed at AryaXAI to explain any deep learning model—an LLM, traditional computer vision model, or beyond. We will discuss the algorithm and benchmarking results of DL-Backtrace against techniques like SHAPE, LIME, GradCAM, etc., for various DL models like LLMs (Llama 3.2), NLP (Bert), CV (ResNet), and tabular data.",The importance of explainability in mission-critical functions and model pruning; The current scope of explainability for deep learning models; complexities in scaling explainability for large models like LLMs; drawbacks with various current techniques; background about DL-Backrace; Results of the method for various DL models and subsequent work.,,Advanced Technical/Research,4.0,"Ethics, governance compliance ","Explainability, DL-Backtrace, Model transparency","Vinay Kumar Sankarapu is the Founder and CEO of Arya.ai. He did his Bachelor's and Master's in Mechanical Engineering at IIT Bombay with research in Deep Learning and published his thesis on CNNs in manufacturing. He started Arya.ai in 2013, one of the first deep learning startups, along with Deekshith, while finishing his Master's at IIT Bombay.

He co-authored a patent for designing a new explainability technique for deep learning and implementing it in underwriting in FSIs. He also authored a paper on AI technical debt in FSIs. He wrote multiple guest articles on ‘Responsible AI’, ‘AI usage risks in FSIs’. He presented multiple technical and industry presentations globally - Nvidia GTC (SF & Mumbai), ReWork (SF & London), Cypher (Bangalore), Nasscom(Bangalore), TEDx (Mumbai) etc. He was the youngest member of ‘AI task force’ set up by the Indian Commerce and Ministry in 2017 to provide inputs on policy and to support AI adoption as part of Industry 4.0. He was listed in Forbes Asia 30-Under-30 under the technology section.",https://www.youtube.com/watch?v=3ayUy9W6KvM,"Banking & Financial Services, Insurance, Information Technology Services, Hospital & Health Care",We will present the research paper we will release in November on DL-Backtrace.,MLOps & GenAI World 2024,3ayUy9W6KvM
Shelby Heinecke,Salesforce,Senior AI Research Manager,A Practical Guide to Efficient AI," In the past two years, we’ve witnessed a whirlwind of AI breakthroughs powered by extremely large and resource-demanding models. And as engineers and practitioners, we are now faced with deploying these AI models at scale in resource constrained environments, from cloud to on-device. In this talk, we will first identify key sources of inefficiency in AI models. Then, we will discuss techniques and practical tools to improve efficiency, from model architecture selection, to quantization, to prompt optimization.",,,case study & advanced technical,3.0,Performance optimization and efficiency,"Efficiency, Model optimization, Quantization"," Dr. Shelby Heinecke leads an AI research team at Salesforce. Shelby’s team develops cutting-edge AI for product and research in emerging directions including autonomous agents, LLMs, and on-device AI. Prior to leading her team, Shelby was a Senior AI Research Scientist focusing on robust recommendation systems and productionizing AI models. Shelby earned her Ph.D. in Mathematics from University of Illinois at Chicago, specializing in machine learning theory. She also holds an M.S. in Mathematics from Northwestern and a B.S. in Mathematics from MIT. Website: www.shelbyh.ai",https://www.youtube.com/watch?v=3HS0Cz_WFHM,"Computer Software, Information Technology Services",,MLOps & GenAI World 2024,3HS0Cz_WFHM
Joseph Tenini,Universal Music Group,Principal Data Scientist,Building Trust in ML Systems,"As builders of ML systems, much time and effort is spent in building our own trust with the technology we are developing. This can take the form of model accuracy metrics, compute efficiency, and core functionality achieved. There is another, often more daunting, step to be considered: building trust in the technology with non-technical users and other stakeholders who will be impacted by its adoption.
In this talk, we explore four pillars of building trust in ML systems with non-technical stakeholders:
Describing performance relative to an interpretable and intuitive baseline.
Quantifying uncertainty as part of the delivery process.
Sharing ""the why"" in non-binary decision processes.
Designing for 2nd order process effects.
After this talk, machine learning practitioners and managers will be equipped to build trust in the products they develop—enabling maximum value and impact from their work.",,,case study & advanced technical,3.0,Business and stakeholder alignment,"Trust in ML systems, Stakeholder alignment, Uncertainty quantification","Joseph Tenini has worked in data science for over a decade in a variety of industries including healthcare, publishing, digital marketing, and entertainment. He has developed, deployed, and managed the lifecycle of a variety of ML-enabled products in many different settings. His specific expertise lies in recommender systems, reinforcement learning, and process improvement. He holds a PhD in Mathematics from the University of Georgia.",https://www.youtube.com/watch?v=hLplCy-ufgw,"Hospital & Health Care, Marketing & Advertising",,MLOps & GenAI World 2024,hLplCy-ufgw
Shreya  Rajpal,Guardrails AI,Founder,Your AI applications need Guardrails: Here's how to build them,"Large Language Models (LLMs) such as ChatGPT have revolutionized AI applications, offering unprecedented potential for complex real-world scenarios. However, fully harnessing this potential comes with unique challenges such as model brittleness and the need for consistent, accurate outputs. These hurdles become more pronounced when developing production-grade applications that utilize LLMs as a software abstraction layer.

In this talk, we will tackle these challenges head-on. We introduce Guardrails AI, an open-source platform designed to mitigate risks and enhance the safety and efficiency of LLMs. We will delve into specific techniques and advanced control mechanisms that enable developers to optimize model performance effectively. Furthermore, we will explore how implementing these safeguards can significantly improve the development process of LLMs, ultimately leading to safer, more reliable, and robust real-world AI applications.","The audience will learn about the challenges and risks associated with Large Language Models (LLMs) and how the open-source platform, Guardrails AI, addresses these issues by providing specific techniques and advanced control mechanisms to optimize model performance, leading to safer, more reliable, and robust real-world AI applications.",,Advanced Technical/Research,4.0,Security and Privacy,"LLM challenges, Guardrails AI, Model safety and optimization
","Shreya Rajpal is the creator and maintainer of Guardrails AI, an open source platform developed to ensure increased safety, reliability, and robustness of large language models in real-world applications. Her expertise spans a decade in the field of machine learning and AI. Most recently, she was the founding engineer at Predibase, where she led the ML infrastructure team. In earlier roles, she was part of the cross-functional ML team within Apple's Special Projects Group and developed computer vision models for autonomous driving perception systems at Drive.ai.",https://www.youtube.com/watch?v=fdUY6_7ZX8M,"Banking & Financial Services, Hospital & Health Care, Information Technology & Service, Insurance, Telecommunications
",This talk will cover implementing end-to-end guardrails and safety interventions for Large Language Models (LLMs). These measures will promote a comprehensive approach to evaluating safety-first LLM development.,,fdUY6_7ZX8M
Zunair Waseem,Nebuly AI,Founding GTM,How to Build your Own LLM User Feedback Loop with Nebuly.,"User feedback is key to turning any good product into a great one, and LLM-powered products are no exception. However, less than 1% of LLM users provide explicit feedback (thumbs up/down), making it difficult to improve LLM responses and enhance the user experience. Learn how Nebuly helps companies with LLMs in production to build their own LLM User Feedback Loop.",,,Furture of AI,,Introduction to MLOps and GenAI,"LLM user feedback, Nebuly, Product improvement","Zunair Waseem is a graduate of the University of North Texas with a background in information systems. He originally began his career in the non-profit sector, doing humanitarian work overseas. After being in the space for nearly half a decade, Zunair became part of the founding team at Nebuly. He enjoys spending time outdoors, reading, being with family, and is committed to continuous learning. He also speaks three languages.",https://www.youtube.com/watch?v=OHJtZg9Owns,,,MLOps & GenAI World 2024,OHJtZg9Owns
Mahdi Torabi Rad,MLBoost,President,Uncertainty Quantification with Conformal Prediction: A Path to Reliable ML Models,"In today's high-stakes applications ranging from medical diagnostics to industrial AI, understanding and quantifying uncertainty in machine learning models is paramount to prevent critical failures. Conformal prediction, also known as conformal inference, offers a practical and robust approach to create statistically sound uncertainty intervals for model predictions. What sets conformal prediction apart is its distribution-free validity, providing explicit guarantees without relying on specific data distributions or model assumptions.

This hands-on workshop reviews the core concepts of conformal prediction, demonstrating its applicability across diverse domains such as computer vision, natural language processing, and deep reinforcement learning. Participants will gain a deep understanding of how to leverage conformal prediction with pre-trained models like neural networks to generate reliable uncertainty sets with customizable confidence levels.

Throughout the workshop, we'll explore practical theories, real-world examples, and Python code samples, including Jupyter notebooks for easy implementation on real data. From handling structured outputs and distribution shifts to addressing outliers and models that abstain, this workshop equips attendees with the tools to navigate complex machine learning challenges while ensuring model reliability and trustworthiness."," - What sets conformal prediction apart from other methods of uncertainty quantification?
- The principles and theory behind conformal prediction for uncertainty quantification in machine learning
- Techniques for creating statistically rigorous uncertainty sets/intervals using conformal prediction
- How to apply conformal prediction to pre-trained machine learning models, such as neural networks, for reliable uncertainty quantification
- Hands-on experience with implementing conformal prediction in Python using libraries like scikit-learn and NumPy
- Examples showcasing the application of conformal prediction in diverse domains such as financial forecasting, natural language processing (NLP), and computer vision","Basic understanding of machine learning concepts, including model training and evaluation. 
Familiarity with Python programming and libraries such as NumPy, Pandas, and scikit-learn",Workshop,4.0,Performance optimization and efficiency,"Conformal prediction, uncertainty quantification, machine learning models.
","Mahdi Torabi Rad, Ph.D. is a computational scientist, engineer, self-trained software developer, mentor, and YouTube content creator with over 10 years of experience in developing mathematical, statistical, and machine-learning models, as well as computer codes to predict complex phenomena. He has published in top-tier journals of Physics, Engineering, and ML and has extensive experience as an ML Lead in various DeepTech startups. Mahdi is also the YouTuber behind the channel MLBoost, known for its popular videos on ML topics, including Conformal Prediction, which have garnered tens of thousands of views in less than a year.",https://www.youtube.com/watch?v=7aXiQjBE_ak,"Banking & Financial Services, Information Technology & Service, Insurance, Computer Software
",,TMLS 2024,7aXiQjBE_ak
Mark Gibbas,Weather Source,CEO,Expert Insight - The Value of Solving Business Weather Challenges,"Weather impacts a wide range of business functions, this talk explores examples of these impacts and how to solve them.",,,Ignite Lighting Talk,,Business and stakeholder alignment,"Weather, Business, Impacts","
Mark has grown Weather Source from a small start-up to a leading provider of weather and climate technology. In its early years, Weather Source primarily provided weather information and consulting. During this time, Mark developed a vision for a scalable weather operating system that could serve clients across any industry and provide each with information relevant to their specific needs and locations of interest. Today, Weather Source’s OnPoint Platform and related products provide actionable weather insights to a growing list of top-ranked companies.
Prior to founding Weather Source, Mark applied his expertise in meteorology and computer science at Applied Insurance Research (AIR) and TASC. Among his notable accomplishments is the development of long-range forecasting systems that laid the foundation for the use of such technology at WSI and The Weather Company. Mark has also enjoyed working on projects for the United Nations’ World Meteorological Organization, where he advanced the meteorological capabilities of several Latin American countries. Mark holds a bachelor’s degree in meteorology with minors in computer science and mathematics.
",https://www.youtube.com/watch?v=q0x-n9FbZzQ,,,TMLS 2023,q0x-n9FbZzQ
Genesis Stefan,Genesis Cloud,Director of Product Management at Genesis Cloud,MemeGPT: Creating a Large Language Model to Generate Memes,"We built a Large Language Model to generate memes. We show you how, by using an open-source model as the foundation, you can construct your own Large Language Model and maintain complete control over your data and AI application",,,Ignite Lightning Talk,,"Model dev, training, arch.","Large Language Model, Meme Generation, Open-Source Model","Stefan Ojanen is Director of Product Management at Genesis Cloud, an accelerator cloud start-up focused on providing infrastructure for the AI space. Prior to joining Genesis Cloud, Stefan managed two data science teams at Scorable, Berlin based AI start-up that developed a holistic XAI-solution for asset managers. Stefan is a founding member of the MLOps community in Munich and also works as a freelance consultant on various AI projects.",https://www.youtube.com/watch?v=GWcjyf1asD0,,,MLOps & GenAI World 2023,GWcjyf1asD0
Jamie Dborin,TitanML,Co-Founder,How do you scale to billions of fine-tuned LLMs?,"Batched LORA Inference, a method gaining popularity that could be used to scale to billions of personalized, finetuned llms without paying the compute cost that this would naively imply. ",,,Ignite Lightning Talk,,Performance optimization and efficiency,"Batched LORA Inference, Scaling LLMs, Compute Efficiency","Jamie Dborin is a co-founder of TitanML, and works at TitanML building inference infrastructure for LLMs. ",https://www.youtube.com/watch?v=JGzas8cfrtw,,,MLOps & GenAI World 2023,JGzas8cfrtw
Vatsal Patel,MongoDB,Senior Data Scientist,A Data Scientist Guide to Unit & End to End Testing,"A comprehensive guide designed to equip data scientists with essential knowledge and practical skills for testing their developed and deployed models.

Key Topics Covered:
- Why Testing is Crucial in ML: Understand the importance of testing in the machine learning lifecycle and how it ensures model reliability and performance.

- Test-Driven Development: Learn about the TDD methodology, its benefits, and how it encourages writing clean, maintainable code by defining tests before implementing the functionality.

- Tools for Testing ML Models: Explore the tools available for unit testing, end-to-end testing, and CI/CD integration, such as `unittest`, `pytest`, `drone`, `GitHub Actions` etc.

- Unit Testing:
- Basic understanding of what unit testing is and its importance in verifying individual components of the ML pipeline.
- Best Practices: Identify best practices on how to approach testing, write test cases, and implement tests using popular frameworks.
- Tutorial: Practical examples illustrating how to write and run unit tests for data preprocessing functions and modelling.
- Running Tests: Instructions on running unit tests locally using `pytest`.

- End-to-End (E2E) Testing:
- Basic understanding of end-to-end testing, which validates the entire ML workflow from data ingestion to model serving.
Dependency Injection: Understand dependency injection and how it helps isolate components and create flexible test configurations.
- Best Practices: Best practices on defining workflows, writing test cases for critical paths, and implementing tests using E2E frameworks.
- Tutorial: Example of writing end-to-end tests for a scoring/training pipeline.
- Running Tests: Guidance on executing E2E tests locally using `pytest`.

Integration to CI/CD Pipelines: Learn how to automate model testing and deployment by integrating unit and end-to-end tests into CI/CD pipelines, ensuring continuous validation of code changes. This part will leverage Makefile & go over code coverage.

This presentation is ideal for data scientists, machine learning engineers, and anyone developing and deploying ML models who want to enhance their testing practices. By the end of the session, attendees will have a solid understanding of unit and end-to-end testing principles, practical examples to follow, and the confidence to implement these testing strategies in their projects.",,,Workshop,4.0,Performance optimization and efficiency,"Unit testing, End-to-end testing, CI/CD pipelines","Vatsal Patel, Senior Data Scientist, MongoDB",https://www.youtube.com/watch?v=6iKu2_suYrE,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Hospital & Health Care, Food & Beverage, Information Technology Services, Insurance, Marketing & Advertising, Telecommunications, Other",,MLOps & GenAI World 2024,6iKu2_suYrE
Nima Safaei,Scotiabank,Sr. Data Scientist,Explainable Machine Learning in Finance – What If Machines Beat the Human Intelligence?,"Explainability is a necessity when applying AI/ML models for high-risk applications in the presence of many exogenous factors, latent variables, and disturbance processes. The good examples are financial, and econometric applications where, regardless of the model performance, the explainability of the outcomes is crucial for storytelling – extracting value from the AI/ML model in terms of some financial metrics and communicating them effectively across the company and with stakeholders. 
In complex environments with many exogenous factors, multiple causes and effects are interacting in complex ways through mechanisms that are often only partially understood. When some variables are latent (unobserved), the causal structure imply a complicated set of constraints on the distribution of observed variables.
Explainability in ML is two folded, Casual Explainability (also known as interpretability) and Counterfactual Explainability. While the former addresses ‘why’, the latter addresses ‘how’ small and plausible perturbations of the input variables modify the output?  For financial storytelling, the former is much more important than the latter because the reasoning through a causal chain brings more confidence on outcomes. A higher confidence can be translated as a lower (market/capital/operational) risk of the false alarms. 
One major challenge in high-risk applications with complex environments is that the explanations provided by machine should be verified by a subject matter expert (SME), otherwise, the outcomes cannot be trusted for further decision making. The question is when we realize that the machine may beat the human by leveraging the information beyond the SME’s knowledge? Should we expect the machine to leverage a level of intelligence which we cannot handle it? In this talk, I try to discuss and answer the above questions through a causal inference lens.",Explainability in Financial Applications thru. Causal Inference,,Advanced Technical,5.0,"Ethics, governance compliance ","Explainability, Financial Applications, Causal Inference","Oral talk/presentation in various word-calss AI/ML conferences such as TMLS 2021/2022, CORS/Informs, ICML 2021, NVIDIA GTC 2020/2021",https://www.youtube.com/watch?v=K66__Z3LZCw,"Banking & Financial Services, Hospital & Health Care, Information Technology & Service, High-risk areas such as Aerospace & Aviation 
","One major challenge in high-risk applications with complex environments is that the explanations provided by the machine should be verified by a subject matter expert (SME), otherwise, the outcomes cannot be trusted for further decision making. The question is when we realize that the machine may beat the human by leveraging the information beyond the SME’s knowledge? Should we expect the machine to leverage a level of intelligence which we cannot handle it?
",TMLS 2023,K66__Z3LZCw
Jesse Cresswell,Layer 6 AI,Sr. Machine Learning Scientist,Quantifying the uncertainty in model predictions,"Neural networks are infamous for making wrong predictions with high confidence. Ideally, when a model encounters difficult inputs, or inputs unlike data it saw during training, it should signal to the user that it is unconfident about the prediction. Better yet, the model could offer alternative predictions when it is unsure about its best guess. Conformal prediction is a general purpose method for quantifying the uncertainty in a model's predictions, and generating alternative outputs. It is versatile, not requiring assumptions on the model and being applicable to classification and regression alike. It is statistically rigorous, providing a mathematical guarantee on model confidence. And, it is simple, involving an easy three-step procedure that can be implemented in 3-5 lines of code. In this talk I will introduce conformal prediction and the intuition behind it, along with examples of how it can be applied in real-world usecases.",Conformal prediction is a mathematical/statistical procedure which may be difficult to understand for less technical audience members. In my talk I will cut through the mathematics to provide the intuition of conformal prediction using visual aids and real-world examples. The audience will take away several settings where they ca immediately apply the technique.,,Advanced Technical / Research,6.0,Performance optimization and efficiency,"Uncertainty in Model Predictions, Conformal Prediction, Neural Networks





",Spoke at TMLS 2022 about Privacy and Fairness. I lead a team of ML scientists at TD's research lab. My team focuses on Trustworthy AI.,https://www.youtube.com/watch?v=-K8vDIyT3xY,"Banking & Financial Services, Computer Software, Hospital & Health Care, Insurance, Marketing & Advertising
","This topic has not been seen before at TMLS. I will not shy away from the mathematical understanding, since that is a main benefit of the approach, but my focus will be on providing the intuition first, visually. As a lead scientist at Layer 6 AI @ TD, I have personally used conformal prediction in real-world financial modelling applications and can provide the audience guidance on the best practices.
",TMLS 2023,-K8vDIyT3xY
"Raheleh Givehchi, Hicham Benzamane",Pelmorex Corp. (The Weather Network),"Lead Data Scientist, Team Lead, Data Engineering Insights",Challenges and Learnings of Machine Learning at scale,How to train millions of models while keeping cost very low and predict on daily basis with the highest performance.,"- Real world Data optimization techniques for training and prediction
- Scale techniques using Cloud infrastructure","- basics on Machine Learning
- basics on Cloud Computing ",Workshop,5.0,"Model dev, training, arch.","Machine Learning at Scale, Data Optimization Techniques, Cloud Infrastructure","Raheleh Givehchi - Lead Data Scientist
I am a lead data scientist with several  years of experience in data science and machine learning. I specialize in delivering valuable insights through data analytics and advanced data-driven methods. Currently, I am working on the Weather Insight Platform (WIP) to provide data-driven solutions to weather-related challenges.

Hicham Benzamane - Team Lead, Data Engineering Insights
- Multiple years working experience in software development and Data engineering.
- Now leading a team of software and data engineers to leverage data and ML to empower Pelmorex customers with cutting edge insights.",https://www.youtube.com/watch?v=QDoH3HLcPhY,,,TMLS 2023,QDoH3HLcPhY
Ryan Turner,DVC,ML Solutions Engineer,How NOT to get ML Models into production,"Ryan Turner will be presenting on the various pitfalls of productionizing ML models. Common issues include dependency management, correct testing processes, and over reliance on Python notebooks. There will be no shortage of satire and metaphor. The talk will draw upon the collective experience of several ML engineers at DVC.",,,Ignite Lightning Talk,,Deployment and integration,"ML Production Pitfalls, Dependency Management, Testing Processes","Ryan has worked as an ML engineer at companies like Uber and Twitter. He is now developing the platform at Iterative.AI. He grew up in Santa Cruz, CA. After spending a few years in the UK and then Canada, he now lives in Reno, NV.",https://www.youtube.com/watch?v=F6RxTgntCLs,,,MLOps & GenAI World 2023,F6RxTgntCLs
"Roxana Sultan, Dr. Benjamin Haibe-Kains, Jun Ma, Ronald Xie, Rex Ma","Vector Institute, Team Fight Tumour
","Chief Data Officer and VP, Health, Senior Scientist, Princess Margaret Cancer Centre; Associate Professor, Medical Biophysics at the University of Toronto, Postdoctoral Fellow at the University of Toronto, PhD candidate Faculty of Medicine at University of Toronto, PhD Student University of Toronto",Cancer Image Segmentation,"Radiation therapy planning for head and neck cancer is a time-consuming and complex task for radiologists. AI-based tools have tremendous potential for segmenting regions of interest and optimizing therapy planning. The Vector Institute and the Cancer Digital Intelligence Program (CDI) from the Princess Margaret Cancer Centre launched a Machine Learning Challenge in June 2022 focused on cancer image segmentation.
Building on foundational work from the lab of scientist, Dr. Benjamin Haibe-Kains, ten teams from Vector and UHN participated in the Challenge leveraging RADCURE, the largest head-and-neck cancer treatment dataset of its kind, containing the imaging, treatment, demographic and clinical data of 2745 head and neck cancer patients.
In this presentation, moderated by Vector’s Roxana Sultan, Dr Haibe Kains will provide an overview of his preliminary work and the winning Challenge team, Fight Tumour, will describe their winning submission.
Presenters:
•        Roxana Sultan (Chief Data Officer and VP, Health, Vector) - Moderator
•        Dr. Benjamin Haibe-Kains (Senior Scientist, Princess Margaret Cancer Centre; Associate Professor, Medical Biophysics at the University of Toronto; Faculty Affiliate, Vector Institute
•        Team Fight Tumour (comprised of Jun Ma, Ronald Xie, and Rex Ma)",,Basic ML,Workshop,5.0,Introduction to MLOps and GenAI,,"Roxana Sultan: Roxana Sultan is the Chief Data Officer and Vice President, Health at the Vector Institute. She leads Vector’s data strategy and its contributions to Ontario’s and Canada’s health sector. Along with our health team and partners, Roxana drives applications of AI to life sciences, fostering research, health sector and industrial sponsor projects, and initiatives to advance the health space, contributing to short-, medium-, and long-term impact achievements within the Ontario health ecosystem.
Roxana is the former Executive Director of the Provincial Council for Maternal and Child Health, where she led the implementation of evidence-based clinical quality improvement and access initiatives in obstetric, neonatal, and pediatric health services across Ontario.  Her career includes leadership roles with The Hincks-Dellcrest Centre (now “SickKids Centre for Community Mental Health”), the Princess Margaret Cancer Centre in the University Health Network, the Canadian Institutes of Health Research (CIHR), Cancer Care Ontario, and the Hospital for Sick Children.
 As an Adjunct Lecturer with the Institute of Health Policy, Management, and Evaluation (IHPME) at the University of Toronto (U of T), Roxana teaches a graduate-level course on intelligent medicine, machine learning, and knowledge representation.  She also serves as the Vice Chair of the Board of the Canadian Cancer Society – Ontario Division. 
Roxana completed her graduate education with the Department of Molecular and Medical Genetics at U of T, and holds a Masters of Health Science from IHPME.
 
Dr. Benjamin Haibe-Kains: Dr. Benjamin Haibe-Kains is a Senior Scientist at the Princess Margaret Cancer Centre: University Health Network, Associate Professor in the Medical Biophysics department of the University of Toronto, and Faculty Affiliate at the Vector Institute. Dr. Haibe-Kains earned his PhD in Bioinformatics at the Université Libre de Bruxelles (Belgium). Supported by a Fulbright Award, he did his postdoctoral fellowship at the Dana-Farber Cancer Institute and Harvard School of Public Health (USA). Dr. Haibe-Kains’ research focuses on the integration of high-throughput data from various sources to simultaneously analyze multiple facets of carcinogenesis. Dr. Haibe-Kains’ team is analyzing large-scale radiological and (pharmaco)genomic datasets to develop new prognostic and predictive models to improve cancer care.


Jun Ma is a Postdoctoral Fellow in the Department of Laboratory Medicine & Pathobiology at the University of Toronto. His research interests focus on the interdisciplinary areas of deep learning and medical image analysis, aiming to develop accurate, fast, and generalizable algorithms to improve healthcare. He has published seven first-author papers on top journals, such as TPAMI, TMI, and MedIA. He is the lead organizer of the MICCAI 2021-2022 FLARE Challenge.
 
Ronald Xie received his BSc in Microbiology and Immunology at the University of British Columbia in 2018. He then received his MPhil in Computational Biology at the Department of Applied Mathematics and Theoretical Physics at the University of Cambridge in 2019. Ronald is currently a PhD candidate in Computational Biology and Molecular Genetics (CBMG) at the Faculty of Medicine at University of Toronto. His research interests lie in deep learning applications in electron microscopy and single cell omics.
 
Rex Ma is currently a Computer Science Ph.D. student at the University of Toronto. He is interested in AI in healthcare and computation biology in general, with research focusing on multi-omics integration using machine learning.",https://www.youtube.com/watch?v=FSB2gRE7rHs,,"New research
",TMLS 2022,FSB2gRE7rHs
"Miguel Mendez, Anand Nimkar, Preeti Shivpuri",Deloitte,"Generative AI and CCAI Technical Lead, Generative AI Capability Leader, Executive Director","Understanding  Where Generative AI fits into Business

",Additional Questions: How dangerous is Generative AI?,Understand where Generative AI fits into business,,Business Strategy PANEL,5.0,Business and stakeholder alignment,"Generative AI, Business Integration, Panel Discussion





","Miguel Mendez is a Generative AI and CCAI technical leader in Deloitte’s Omnia AI practice where he leads large deliveries across multiple industries
",https://www.youtube.com/watch?v=1l2W8YlvVM4,"Computer Software
",,TMLS 2023,1l2W8YlvVM4
"Michel Dubois, Cameron Schuler, Alex LaPlante, Sara Hooker","Mila, Vector Institute, Amii, Borealis AI @ RBC, Cohere for AI","Principal Director, Experimental Development, Chief Commercialization Officer & VP, Industry Innovation, Chief Operating Officer, Interim Head, Sara - Head of Cohere for AI",Panel: How companies can navigate and approach the new advancements in generative AI,,,,Buisness Strategy - Panel,,Business and stakeholder alignment,"Generative AI, Business Strategy, Panel Discussion





","Michel - As Director, AI Activation at Mila, the Quebec Artificial Intelligence Institute, Michel Dubois actively participates in the development of AI for the benefit of all. He holds a master's degree in mathematics and is currently a PhD candidate in engineering (machine learning). He is also the author of a patent on the mathematical optimization of high-bandwidth signal switching. Over the past 28 years, Michel Dubois has consolidated his experience in several aspects of machine learning and artificial intelligence. Before joining Mila, Michel Dubois held the role of Associate Partner at IBM, and, in his previous role, he was Vice President of Artificial Intelligence at Newtrax, a Montreal start-up that quickly experienced international success, reaching an average annual revenue growth of 80% and opening offices in Santiago, Perth, Moscow and London.

Cameron - Cameron Schuler is the Chief Commercialization Officer & Vice President, Industry Innovation at the Vector Institute. He is the former Executive Director of Amii, where, for 8 years, he led one of the top-ranked Machine Learning and AI groups in the world. Cameron’s multifaceted career has covered finance, business & product development, consumer products, IT and general management from start-ups to mature companies. His industry experience includes Alternative Energy, Banking, Consumer Products, Information Technology (Consumer and Enterprise), Investment Sales and Trading, Life Sciences, Manufacturing, Medical Devices, Oil & Gas, and Oil & Gas Services. Roles have included COO, CFO, President and CEO and he was COO & CFO of a food manufacturer whose products lead to sales of over 250 million units.

Kirk - Kirk Rockwell is a project and operations manager with more than 20 years of experience managing technology and innovation initiatives, across a broad range of sectors in partnership with academia, all levels of government, and some of Canada’s largest companies.

Within an arm's length government agency, he had responsibility for investment of public funds into research and innovation projects across multiple sectors including energy, clean tech, agriculture and health, utilizing technologies like AI and Machine Learning, Nanotechnology and Genomics. 

He has experience with numerous public/private partnerships (P3s) including membership on advisory and governance boards where the stakeholders include large multinational corporations, the governments of Alberta and Canada, universities, and other small or medium-sized companies.

He has a Diploma in Environmental Technologies, a Bachelor of Science Degree in Environmental Sciences, and a Master’s of Business Administration Degree with a specialization in Innovation and Entrepreneurship.

Kirk is currently the VP, Public Strategy and Gants at the Alberta Machine Intelligence Institute (Amii) in Edmonton, Alberta, Canada. He resides in Edmonton with his wife and two daughters.

Alex - Alex LaPlante is currently the interim Head of Borealis AI, RBC's R&D lab for artificial intelligence, where she and her team build and deploy leading-edge AI solutions to complex business problems found across the enterprise. Previously, Alex led Borealis' business development and product management teams, which saw her design the company's long-term product vision and build strategic partnerships with RBC stakeholders. Before joining Borealis, Alex held a number of leadership roles at the intersection of technology and finance. She holds a PhD in operations research from the University of Toronto.

Sara - Sara Hooker leads Cohere For AI, a non-profit research lab that seeks to solve complex machine learning problems. Cohere For AI supports fundamental research that explores the unknown, and is focused on creating more points of entry into machine learning research. With a long track-record of impactful research at Google Brain, Sara brings a wealth of knowledge from across machine learning. Her work has focused on model efficiency training techniques and optimizing for models that fulfill multiple desired criteria -- interpretable, efficient, fair and robust. Before Cohere For AI, she was the founder of Delta Analytics, a non-profit that brings together researchers, data scientists, and software engineers to volunteer their skills for non-profits around the world. Sara Hooker is on the advisory board of Patterns and on Kaggle's ML Advisory Research Board. In addition, Sara currently serves on the World Economic Forum council on the Future of Artificial Intelligence.
",https://www.youtube.com/watch?v=BzS74tByHpw,(Panel discussion),,,BzS74tByHpw
Dr. Ehsan Amjadian,RBC,Head of Data Science,"Privacy & Security of Large Language Models, Risks and Mitigation","Large language models (LLMs) are powerful AI systems that can generate natural and artificial language content such as programming languages for various tasks and applications. However, LLMs also pose significant security and privacy risks, such as leaking sensitive information from their training data, producing unsafe or malicious code, and enabling adversarial attacks by malicious parties. Some examples of adversarial attacks on LLMs are PromptInject, which can hijack the model's goal or leak its prompt, differentiable language model attack, which can fool text classifiers by fine-tuning a pre-trained language model, and gradient-based attack, which can generate perturbations that exploit the model's vulnerability. In this talk, we review some of the main challenges and threats associated with LLMs for code and natural language, and survey some of the existing and proposed solutions to mitigate them. We will discuss some of the ethical and legal implications of using LLMs, and suggest some directions for future research and development.","1. Enumerating Privacy & Security Concerns with Large Language Models (LLMs)
2. Provide some understanding of LLms
3. Talk about Other Risks Associated with the use of LLMs
4. Go through some of the approaches that can mitigate some of these risks",,Advanced technical,5.0,Security and Privacy,"Privacy Risks, Security Concerns, Mitigation Strategies






","Dr. Ehsan Amjadian is the Head of Data Science at the Royal Bank of Canada (RBC), where he has led numerous advanced AI products and initiatives from ideation to production and has filed multiple patents in the areas of Data Protection, Finance & Climate, and Computer Vision & its applications to Satellite Images. He earned his Ph.D. in Deep Learning & Natural Language Processing from Carleton University, Canada and is presently an Adjunct Professor of Computer Science at University of Waterloo. He is published in a variety of additional Artificial Intelligence and Computer Science domains including Recommender Engines, Information Extraction, Computer Vision, and Cybersecurity.",https://www.youtube.com/watch?v=On9bmpkL7EA,"Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology & Service, Insurance, Telecommunications
",,TMLS 2023,On9bmpkL7EA
Jawad Ahmed,Loblaw Digital,Staff Applied Scientist,"Solving Product Substitutions, A Big Problem in Grocery E-Commerce – Through Self-Supervised ML","Background:
Loblaw Companies Ltd is the largest grocery retailer in Canada. It operates multiple popular banners with Real Canadian Superstore, No Frills, and T&T being some of the most popular ones. E-commerce of grocery has become a significant part of the business accounting for more than $2 billion in sales per year.

Problem:
Shopping for groceries online is an inherently different process than shopping in person. We take for granted the in-store shopper’s ability to make quick decisions on the fly when faced with the issue of product availability.

We fulfill from stores to ensure freshness which has a very dynamic inventory. This makes promises of items collected, sometimes a day or two after the order depending on the customer’s delivery date, affected by many factors - some of which we cannot control. Thus, we need a solution to substitute items that are out of stock at the time of picking to make sure the customer experience is minimally impacted. While shopping at a physical store, a customer can make a suitable choice of an alternative. In the e-commerce process of grocery shopping, either the customer has to make a selection of the substitute, or the Loblaw employee picking the order on behalf of the customer needs a relevant suggestion on the best substitute for the given item, personalized for the given customer.

Loblaw has historical data available on what selection was made by customers from the list of various possible substitute options available for a given item. Additionally, there is data available on the choices made by pickers - the employees who shop at the store to fulfill customers’ orders. This provides us an opportunity to tailor product similarities toward product substitutions that are tied to business metrics.

Solution:
We explored multiple solutions to solve this problem. Our most promising solution that we wish to present leverages features extracted from text descriptions and images of products. In this talk, we will discuss how our approach evolved over time and how this cutting-edge self-supervised method is a big improvement over the traditional techniques.","The talk covers the data curation process by which we prepared a benchmarking Products Substitutions dataset using historical human-selected substitutions data at Loblaw.

The audience will learn about self-supervised ML approaches we used to recommend product substitutions benchmarked against the above mentioned products substitutions Testset.",,Case Study,5.0,Performance optimization and efficiency,,"Jawad currently works as a Staff Applied Scientist at Loblaw Digital, supporting ML teams building personalization and recommender systems for different lines of business of Loblaw Companies. 
He has 8 years of industry experience in Applied AI/ML. Previously, he worked at Flipp, Dialpad and McKinsey Solutions.

His areas of interest are using ML research applications to help build products with scalable ML solutions in NLP, Conversational AI, Computer Vision and Recommender Systems. 
LinkedIn (https://www.linkedin.com/in/jawadahmed90)",https://www.youtube.com/watch?v=Y0zAdGyvLL0,,"The speech will give a walkthrough of an extremely common problem faced by customers in the grocery delivery domain. The solutions discussed haven't touch upon the aspect of Machine Learning application in the public domain. Therefore, it would be a relatable topic for most of the audience, while also having an element of novelty.
",TMLS 2022,Y0zAdGyvLL0
Jonas  Mueller,Cleanlab,Chief Scientist,Operationalizing data-centric AI: Practical algorithms + software to quickly improve ML datasets,"In applied ML projects, experienced data scientists know that improving data brings higher ROI than tinkering with models. However the process of finding and fixing problems in a dataset is highly manual (ad hoc ideas explored in Jupyter notebooks). Cleanlab develops open-source software to help make this process more: efficient (via novel algorithms that automatically detect certain issues in data) and systematic (with better coverage to detect different types of issues). 

This talk will describe how high-level ideas from data-centric AI can be operationalized across a wide variety of datasets (image, text, tabular, etc). I will introduce novel algorithmic strategies to automatically identify various issues in data that we have researched and published papers on with extensive benchmarks. These include detection of label errors, bad data annotators, out-of-distribution examples, and other dataset problems that once identified can be easily addressed to significantly improve trained models.  Thousands of data scientists have started using this sort of data-centric AI software, and results from a few case studies  will be presented.  I will conclude with a discussion of where the data-centric AI movement is headed next, and key obstacles that deserve more attention.","How to best practice data-centric AI in real-world ML projects. This covers automated methods to check the dataset for various issues common in ML data as well as how to efficiently address the issues to improve the dataset and subsequent ML model. I will cover novel algorithms invented by our research team and case studies which showcase the benefits of data-centric AI in real-world ML applications.

The intended audience is folks with experience in supervised learning who want to develop the most effective ML for messy, real-world applications. Some of the content will be technical, but not require a deep understanding of how particular ML algorithms/model work (having completed one previous ML course/project should suffice). The topics should be of interest to anybody working in: computer vision, natural language processing, audio/speech or tabular data, and other standard supervised learning applications, as well as DataOps folks.",,Advanced Technical/Research,4.0,"Model dev, training, arch.","Data-centric AI, Dataset improvement, Automated data issue detection
","Jonas Mueller is Chief Scientist and Co-Founder at Cleanlab, a software company providing data-centric AI tools to efficiently improve ML datasets. Previously, he was a senior scientist at Amazon Web Services developing AutoML and Deep Learning algorithms which now power ML applications at hundreds of the world's largest companies. In 2018, he completed his PhD in Machine Learning at MIT, also doing research in NLP, Statistics, and Computational Biology.  Jonas has published over 30 papers in top ML and Data Science venues (NeurIPS, ICML, ICLR, AAAI, JASA, Annals of Statistics, etc). This research has been featured in Wired, VentureBeat, Technology Review, World Economic Forum, and other media. He loves contributing to open-source, and helped create the fastest-growing open-source software for AutoML (https://github.com/awslabs/autogluon) and Data-Centric AI (https://github.com/cleanlab/cleanlab).  An avid educator, he also taught the first-ever course on data-centric AI at MIT: https://dcai.csail.mit.edu/",https://www.youtube.com/watch?v=Z2tLJqIWJQY,"Banking & Financial Services, Computer Software, Hospital & Health Care, Insurance, Marketing & Advertising, Legal, Manufacturing, Agriculture, E-commerce, Analytics
","cleanlab is the most popular open-source tool for data-centric AI, but a talk like this demonstrating its various functionalities for improving different types of data cannot be found anywhere online. This talk combines how the methods work, why they are useful, and how to apply them together in practice, in a way that has never been presented before. Many of the methods presented are brand new too having only been published months prior.",MLOps & GenAI World 2023,Z2tLJqIWJQY
Ilyas Iyoob,University of Texas,Faculty - University of Texas; Head of Research - Kyndryl; Venture Partner - Clutch VC,GenAI ROI: From Pilot to Profit,"In this session, we will dive deep into the real-world ROI of Generative AI moving beyond pilot projects and into scalable, value-driving solutions. With real world examples of our enterprise implementations, we reveal the hidden costs, unexpected value, and key metrics that truly matter when measuring success. We will also explore practical steps to overcome “pilot paralysis” and strategies for balancing innovation with cost control."," Whether you're a decision-maker or AI leader, this session will provide actionable insights on how to make GenAI work for your business, ensuring it delivers measurable impact and not just hype.",,case study & business strategy,1.0,Business and stakeholder alignment,"ROI, Scalable solutions, Cost control","Dr. Ilyas Iyoob is faculty of Data Science and Artificial Intelligence in the Cockrell School of Engineering at the University of Texas. He pioneered the seamless interaction between machine learning and operations research in the fields of autonomous computing, health-tech, and fin-tech. Previously, Dr. Iyoob helped build a cloud computing AI startup and successfully sold it to IBM. He currently advises over a dozen venture funded companies and serves as the Global Head of Research at Kyndryl (IBM Spinoff). He has earned a number of patents and industry recognition for applied Artificial Intelligence and was awarded the prestigious World Mechanics prize by the University of London.",https://www.youtube.com/watch?v=CqlXBAc4oEM,,"Expect a candid, no-fluff discussion on ROI.",MLOps & GenAI World 2024,CqlXBAc4oEM
"Ville Tuulos, Eddie Mattia",Outerbounds,"CEO, Data Scientist",LLMs in Practice: A Guide to Recent Techniques and Trends,"In this workshop, attendees will learn about methods for working with LLMs. Our stories will be guided by examples you can run on your laptop or in a (free) hosted cloud environment provided to attendees. Developers will expand their awareness of how researchers and product designers are working with LLMs, with emphasis on connecting high-level concepts such as fine-tuning and vector databases to the fundamental math and APIs data scientists should understand. Business-minded executives can either get hands - on or follow the higher-level stories to deepen their sense of what is possible with LLMs, the technicalities behind risks they introduce, and how they fit into the arc of ML. The primary value of this workshop will be as a guide to help teams set reasonable goals in the complex and fast-moving world of LLMs, and understand what you need to successfully support your team’s next LLM projects.","There are cheap (e.g., APIs) and expensive (e.g., fine-tuning, training) ways to build on top of LLMs. The methods you choose have consequences in apps you can build and how your dev team works. We will learn how to think about these choices as we develop basic apps you can use as templates for future genAI projects. Learners have the option to follow along in a provided dev environment where we will unpack these choices and make the tradeoffs and decision space concrete. ","* Basic knowledge of Python
* Ability to use the command line
* Ability to use common ML algorithms in a notebook environment",Workshop,5.0,Introduction to MLOps and GenAI,"LLMs, Fine-tuning, Vector databases
","Ville has been developing infrastructure for machine learning for over two decades. He has worked as an ML researcher in academia and as a leader at a number of companies, including Netflix where he led the ML infrastructure team that created Metaflow, a popular open-source framework for data science infrastructure. He is a co-founder and CEO of Outerbounds, a company developing modern human-centric ML. He is also the author of the book, Effective Data Science Infrastructure, published by Manning.

Eddie Mattia is a data scientist at Outerbounds who began using Python to teach applied math in grad school. Since then, Eddie has worked at startups and at Intel building machine learning software",https://www.youtube.com/watch?v=o_AwhzInKAE,,"We acknowledge that there is a lot of useful content available on LLMs online, but we also believe that most of this content has three major blindspots.   Most of this content concentrates on the exhibition of a novel model or a unique application of a model, typically designed for singular experimental use. In practice, any serious adopter likely needs to build LLM training and inference workflows using easily repeatable experiments that lead to consistently reproducible results.   A substantial amount of this content is the result of tireless efforts from individuals who lack the means to conduct large-scale LLM training, which implies they don't encounter the typical challenges associated with such scale, but serious adopters need to think of said challenges from day 1, since operating at scale does not afford the luxury of being reactive.  Most content on ML does not lay enough emphasis on the collaborative aspect of Machine Learning and Data Science. A good model (LLM or otherwise) is a collective effort of several people, but unlike traditional software we don’t yet have a consistent pattern on enabling collaboration when working on models and data.   As part of our work with Metaflow, we've been assisting numerous organizations on their LLM path, from optimizing an LLM to run on a single GPU to facilitating large-scale, multi-node, multi-GPU LLM training. The LLMOps practices we'll delve into in our presentation are derived directly from our work across this broad spectrum of LLM use-cases. We believe that our hands-on experience combined with the blindspots of online content on LLMs positions our talk uniquely to benefit our audience. 
",MLOps & GenAI World 2023,o_AwhzInKAE
Wendy Foster,Shopify,Director of Engineering and Data,Data Storytelling for Beginners,"Data science is a balancing act—math and science have their role to play, but so do art and communication. Storytelling can be the binding force that unites them all. In this talk, I’ll explore how to tell an effective data story and illustrate with examples from our practice at Shopify.
",,,Business Strategy,2.0,Business and stakeholder alignment,"Data Storytelling, Communication, Shopify","Wendy Foster is a Director of Engineering and Data at Shopify, where she leads the teams building merchant analytics, helping entrepreneurs understand and make best, supported data informed decisions for their businesses.",https://www.youtube.com/watch?v=rC07Yp3SrF0,"Computer Software, Information Technology & Service
",,TMLS 2023,rC07Yp3SrF0
"Kathryn Hume, Nijan Giree, Arup Saha, Alex Lau",RBC,"VP Digital Channels Technology, Director Mobile Development, Digital, Director, Android Development, Senior Director, Android and Mobile Services Development",Upskilling Your Full-Stack Development Team in Machine Learning," As the machine learning landscape evolves, it's becoming easier for traditional software development teams to build and implement models themselves. Generative AI further democratizes ML implementation, with traditional tasks like classification or summarization being possible with well-engineered prompts. 

In this session, we will walk through how an Android native development team built skills to implement various kinds of machine learning models themselves. We'll share lessons learned along the way, and tips for scaling and democratizing machine learning across the enterprise.

"," Practical tips for building machine learning skills in a software development team
Techniques to scale knowledge of a new domain across a large team
Lessons learned in the nuances of applying various neural network techniques and how to overcome obstacles in production at scale for a 10-million client user base",,Applied Case Studies,4.0,Business and stakeholder alignment,"Machine Learning, Generative AI, Full-Stack Development






","Kathryn Hume is the Vice President of Digital Channels Technology at the Royal Bank of Canada. She is responsible for the software engineering and development of the mobile and online banking platforms at RBC. Alongside her primary role at RBC, she is a board member for AI-Redefined and CanadaHelps, and an advisor for Lytical Ventures. She has led multiple technology teams at RBC, including the personal investments engineering team and the Borealis AI machine learning team. Prior to joining RBC, Kathryn held leadership positions at Integrate.ai and Fast Forward Labs, where she helped over 50 Fortune 500 organizations develop and implement AI programs. She is a widely respected author and educator on technology and innovation, with work appearing at TED, HBR, the Globe and Mail. She has given guest lectures on AI at Harvard, MIT, and the University of Toronto, and served as a visiting professor at the University of Calgary Faculty of Law. She holds a PhD in Comparative Literature from Stanford University and speaks seven languages.

Nijan Giree is an experienced developer with a keen interest in artificial intelligence. He is committed to devising practical AI-driven solutions to complex challenges.

Arup: Experienced Director Of Development with a demonstrated history of working in the banking industry. Skilled in Android Development, Machine Learning, AI, Deep Learning, NLP, CI/DevOps, Google Cloud Platform, Database, SOA, Websphere, Enterprise Architecture, and Agile Methodologies.

Alex Lau is the Senior Director of Android and Mobile Services development at RBC.  He leads a passionate team of software engineers that is responsible for developing RBC’s Android Banking and Avion applications.  Alex has been developing mobile solutions for the past 10 years.  Prior to joining RBC, he led development teams at TD, Good Technology/BlackBerry and IBM building a variety of products, from consumer facing applications to enterprise tools like MDM and BYOD containers.  Alex holds a Master and Bachelor of Mathematics degree in Computer Science from the University of Waterloo.

",https://www.youtube.com/watch?v=ee0VjItJ4Lo,"Banking & Financial Services, Information Technology & Service
",,TMLS 2024,ee0VjItJ4Lo
Amrit Krishnan,Vector Institute,Senior Applied Machine Learning Specialist,An open-source approach to build guardrails for deploying ML models in healthcare,"The use of ML in healthcare applications is rising steadily. Deployment of these systems requires a responsible approach, and regulation is lagging behind. 
At the Vector Institute, in strong collaboration with our stakeholders, we are building an open-source software framework to address this gap. Specifically, we are focussing on rigorous
evaluation and monitoring of the ML system across patient sub-populations. We will show how we can generate evaluation and monitoring reports for end-users, using two use cases. 
Additionally, we will also discuss challenges in implementing monitoring sub-systems in healthcare settings. ","You will learn how to use an open-source toolkit to develop ML models on health data, with a focus on deployment and workflow integration.","Clinical data, ML, Software development, Open-source",Workshop,5.0,Deployment and integration,"ML in Healthcare, Open-Source Framework, Evaluation and Monitoring","Amrit is a Senior Applied ML Specialist at the Vector Institute, currently working on building tools for enabling ML deployment for healthcare. Amrit is passionate about Healthcare ML, Robotics, Open-Source and Software Engineering for productionizing ML systems.",https://www.youtube.com/watch?v=5w0kndFWuI8,,,TMLS 2023,5w0kndFWuI8
Suhas Pai,"        
Hudson Labs",CTO,Making RAG (Retrieval Augmented Generation) Work,"The RAG (Retrieval Augmented Generation) paradigm drives a large proportion of LLM-based applications. However, getting RAG to work beyond prototypes is a challenging ordeal. In this talk, we will go through some of the common pitfalls encountered when implementing RAG along with techniques to alleviate them. We will showcase how robustness can be built into the design of the RAG pipeline and how to balance them against factors like latency and cost."," What can go wrong with RAG?

Techniques to alleviate RAG shortcomings - specifically, tightly coupled models, layout and context-aware fine-tuned embeddings, retrieval text refinement, query expansion, and interleaved retrieval.",,Applied Case Studies,6.0,Deployment and integration,"RAG Pitfalls, Alleviation Techniques, RAG Pipeline Robustness
","Suhas Pai is a NLP researcher and co-founder/CTO at Hudson Labs a Toronto based startup. At Hudson Labs, he works on text ranking, representation learning, and productionizing LLMs. He is also currently writing a book on Designing Large Language Model Applications with O'Reilly Media. Suhas has been active in the ML community, being the Chair of the TMLS (Toronto Machine Learning Summit) conference since 2021 and also NLP lead at Aggregate Intellect (AISC). He was also co-lead of the Privacy working group at Big Science, as part of the BLOOM open-source LLM project.",https://www.youtube.com/watch?v=VSxjENFFZLc,"Banking & Financial Services, Computer Software, Other
",,TMLS 2024,VSxjENFFZLc
Robinson Garcia,Petrobras,R&D Project Manager & Technology Specialist,Digital Twin as a tool for Industrial Asset Management,"Valued at 500 million CAD, Asset360 is a solution that improves efficiency and reduces the maintenance backlog of offshore installations at Petrobras. The project started back in 2018 after successful experiments with semantic segmentation, and after the signature of two cooperation terms with a partner university. We have built a Streetview-like platform and an information extraction solution over the past two years (+4000 registered users). Currently, we are experimenting with Human in the loop learning, recommendation system, and multi-objective optimization to increase value creation.  Our moonshot is to create a two-sided platform that reduces the distance between specialized developer partners (research labs and startups) and internal consumers.",,,Ignite Lighting Talk,,Performance optimization and efficiency,"Digital Twin, Industrial Asset Management, Human-in-the-Loop Learning","Robinson Garcia graduated in Mechanical Engineering (2006) and did his MBA at the Rotman School of Management (2018). He currently works at the Petrobras Research Center (Cenpes), leading cooperation agreements with universities and startups to develop solutions for industrial asset management.",https://www.youtube.com/watch?v=45AdHA54lZE,,,TMLS 2022,45AdHA54lZE
Hien Luu,DoorDash,Head of Machine Learning Platform,Scaling & Evolving the Machine Learning Platform at DoorDash,"As DoorDash business grows, the online ML prediction volume grows exponentially to support the various Machine Learning use cases, such as the ETA predictions, the Dasher assignments, the personalized restaurants and menu items recommendations, and the ranking of the large volume of search queries.

In this session, we will share our journey of building and scaling our Machine Learning platform and particularly the prediction service, the various optimizations experimented, lessons learned, technical decisions and tradeoffs made. We will also share how we measure success and how we set goals for the future.",The challenges and learning lessons from building an ML platform to support ML at scale,,Case Study,5.0,Introduction to MLOps and GenAI,,"Hien Luu is a Sr. Engineering Manager at DoorDash, leading the Machine Learning Platform team. He is particularly passionate about the intersection between Big Data and Artificial Intelligence. He is the author of the Beginning Apache Spark 3 book. He has given presentations at various conferences such as GHC 2022, Data+AI Summit, XAI 21 Summit, MLOps World, YOW Data!, appy(), QCon (SF,NY, London).",https://www.youtube.com/watch?v=cFEPajyQq0Q,"Computer Software, Food & Beverages, Information Technology & Service
","This is a case study about our journey of building ML platform at DoorDash
",TMLS 2022,cFEPajyQq0Q
Keri Olson,IBM,VP Product Management," Code Smarter, not harder: Generative AI in the Software Development Lifecycle",In this session we will dive into the quickly evolving landscape of AI coding assistants and how they play a pivotal role in the software development lifecycle (SDLC). Join us to learn how IBM watsonx Code Assistant serves as your AI-powered coding companion through examples and a live demo.," How to use a generative AI code assistant (IBM watsonx Code Assistant) to generate, query and document your code - and much more. Learn how to use the same tools to modernize legacy code (in Java) to create a modern code-base fit for today’s needs.",,Advanced Technical/Research,6.0,Introduction to MLOps and GenAI,"Generative AI, Software development lifecycle (SDLC), AI-powered coding assistant","Keri Olson is the Vice President of Product Management for IBM AI for Code and Head of Product for IBM watsonx Code Assistant. She has over 20 years of experience in enterprise software and has held roles in Product Management, Engineering, Operations, Transformation, and Corporate Consulting. Keri is passionate about software and product development, AI for Code, driving innovation, and building strong technical and business partnerships. She is based in Rochester Minnesota, and she enjoys volunteering in the community as well as mentoring technical and business professionals to help build the next generation of leaders.",https://www.youtube.com/watch?v=MyF2YadiODU,"Computer Software, Information Technology Services","Hands-on discussion of how generative AI is helping developers become much more productive to not only write new code but to also modernize existing, legacy code.",MLOps & GenAI World 2024,MyF2YadiODU
Lee Twito,Lemonade,GenAI Lead," Code Generation Agents: Architecture, Data Modeling Challenges, and Production-Ready Considerations"," This session provides an in-depth look at the architecture of multi-agent systems for code generation, emphasizing practical solutions to common data modeling challenges. We’ll explore how focusing on signature data, employing linters, indexing codebases, and utilizing GraphRAG can address issues often faced in agent-driven coding environments. We’ll examine examples where models fall short, and show new concepts that resolve these challenges. You’ll leave with a practical understanding of how multi-agent systems are structured, as well as insights on minimizing bugs and enhancing reliability."," You’ll leave with a deep understanding of how advanced multi-agent systems for code generation operate, including solutions to common data modeling challenges. You’ll also discover practical techniques—like using linters, codebase indexing, and graph retrieval-augmented generation—that you can apply to make your own agent systems more reliable and efficient.",,case + business,5.0,"Model dev, training, arch.","Multi-agent systems, Code generation, Data modeling challenges",,https://www.youtube.com/watch?v=68v7rzWpKx4,"Computer Software, Information Technology Services, Other","This session stands out due to its foundation in recent academic research combined with real-world testing. You’ll gain insights from a variety of battle-tested agents—some of which are open source—allowing you to explore the code yourself. The session is a culmination of thorough hands-on research of tools like Devin, OpenDevin, AIDER, and OpenInterpreter.
",MLOps & GenAI World 2024,68v7rzWpKx4
Ramon Perez,Seldon,Developer Advocate,Introduction to Building ML Microservices: A hands-on approach with examples from the music industry,"Serving a machine learning model is not particularly easy, especially if we add two or three models in parallel to the mix, in which case, a single model deployment recipe might start to crumble. To tackle the challenges around serving individual or multiple models in production, we have handy tools like MLServer and Seldon Core. The former is a python library that allows us create machine learning microservices with one or multiple models in the same service, and the latter allows us to build simple-to-complex inference graphs that can help us handle A/B testing, shadow and canary deployment, feature transformations, and model monitoring. If you want to learn how to use open-source tools to build microservices based on your different use cases and model recipes, come and join this hands-on workshop and get started with several of the key steps in the machine learning workflow as we walk through fun examples from the broader music industry.","The core of the workshop will teach participants how to create machine learning microservices and inference graphs, and how to monitor the predictions made by these services. The main use case we'll follow throughout the workshop comes from the music industry, so this will be a fun and content-rich 3 hours to go through. 

Throughout the workshop, we will be building a creative ML platform in several incremental steps. In the first 50 minutes of the workshop, we will set up the user interface and the back-end our application, and then we'll spin up the first model we will interact with. In the second 50-minute section, we will start adding different functionalities to our platform by running new machine learning models inside our inference server. Lastly, we'll create different replicas of each model, develop an inference graph to come up with unique tunes, and conduct AB testing on our service to assess and evaluate the output of different models when compared with real songs.

Within our 3 hours together, we'll have two 10- to 15-minute breaks and there will be plenty of exercises for participants to complete.",-,Workshop,3.0,Deployment and integration,"ML microservices, Inference graphs, Model deployment
","Ramon is a data scientist and educator currently working in the Developer Relations team at Seldon in London. Prior to joining Seldon, he worked as a freelance data professional and as a Senior Product Developer at Decoded, where he created custom data science tools, workshops, and training programs for clients in various industries. Before Decoded, Ramon wore different research hats in the areas of entrepreneurship, strategy, consumer behavior, and development economics in industry and academia. Outside of work, he enjoys giving talks and technical workshops and has participated in several conferences and meetup events.",https://www.youtube.com/watch?v=RsAHprbnIvY,"Computer Software, Information Technology & Service, Marketing & Advertising
",It's an interesting personal problem I wanted to tackle and shows how to overcome lots of technical challenges the audience can relate to.,,RsAHprbnIvY
"Nicolás Venegas Oliva, Cristóbal Guzmán Guzman",LATAM Airlines,"Technical Lead of Advanced Analytics, Staff Data Scientist",Scaling Advanced Analytics in the Worst Crisis in the Industry Area,"For data science teams looking to create real business value with AI – MLOps is not something that’s ‘nice to have’ – It’s a MUST HAVE. To make MLOps work for your organization, you need to have the right tools combined with the right skillset across the different roles, and a unified process. For LATAM Airlines Group, being faced with the worst airline industry crisis following the COVID-19 pandemic, MLOps was imperative. We set off to create a cross-company MLOps strategy and implement it across dozens of use cases. In this talk, we will share our MLOps strategy, provide tips for success, pitfalls to avoid based on our own data science journey and dive into two of our use cases.","Scaling of MLOps teams, impact measurement, selection and training of highly technical teams.",,Case Study,5.0,Introduction to MLOps and GenAI,,"2 years of experience in backend development, 2+ years in data processing and the last 3+ years as Advanced Analytics technical leader at LATAM Airlines. During this time the team has grown from 9 to 48 highly trained professionals. It has also become the team with the highest impact generation within the company and a reference in the region in terms of MLOps and measured business impact through data products.",https://www.youtube.com/watch?v=ICX2Bj9xhGI,,,TMLS 2022,ICX2Bj9xhGI
Rajat Arya,XetHub,Co-founder,AI as an Engineering Discipline,"The field of Artificial Intelligence (AI) has transformed over the last few decades, and has evolved from a deeply mathematical and theoretical discipline into a software engineering discipline. For the first time in history, AI is truly accessible. However, an open question is what is the right way to use AI? What are the engineering best practices around AI? In this talk we first briefly discuss how modern AI came about and how it has changed the rules of Machine Learning development. Then we will try to establish some new guidelines and engineering principles. that will allow you to cut through the noise of AI tooling, and assist in determining what is most effective for your tasks. And we will close with some examples you can apply to your next ML or AI project.","In this talk you should learn how to approach AI/ML development with software engineering principles, recognize that AI development is simply ML development, and have a set of tools and processes to adopt in your next ML project.",,Applied Case Studies,4.0,Introduction to MLOps and GenAI,"AI Engineering, Machine Learning Development, Engineering Best Practices






","XetHub Co-founder with over 20 years of industry experience in various roles including support, engineering, product, and sales. Some highlights include co-designing the ML Data Platform at Apple, shipped 1st version of Microsoft OneDrive, being an early engineer in AWS RDS (scaled database instances 5K-100K), and being the 1st employee of the ML startup GraphLab/Dato/Turi.",https://www.youtube.com/watch?v=7FeuUVp-8V4,"Computer Software, Information Technology & Service, Hospital & Health Care, Banking & Financial Services, Insurance
",,TMLS 2024,7FeuUVp-8V4
Aishwarya Naresh Reganti,Amazon,Applied Scientist,Evaluating LLM-Judge Evaluations: Best Practices,"The use of LLM-based judges has become common for evaluating scenarios where labeled data is not available or where a straightforward test set evaluation isn't feasible. However, this approach brings the challenge of ensuring that your LLM judge is properly calibrated and aligns with your evaluation goals. In this talk, I will discuss some best practices to prevent what I call the ""AI Collusion Problem,"" where multiple AI entities collaborate to produce seemingly good metrics but end up reinforcing each other's biases or errors. This creates a ripple effect.","- Gain insight into what LLM judges are and the components that make them effective tools for evaluating complex use cases.
- Understand the AI Collusion problem in context of evaluation and how it can create a ripple effect of errors.
- Explore additional components and calibration techniques that help maintain the integrity and accuracy of evaluations.",,Advanced Technical/Research,5.0,"Ethics, governance compliance ","LLM judges, AI Collusion problem, Calibration techniques","Aishwarya is an Applied Scientist in the Amazon Search Science and AI Org. She works on developing large scale graph-based ML techniques that improve Amazon Search Quality, Trust and Recommendations. She obtained my Master's degree in Computer Science (MCDS) from Carnegie Mellon's Language Technology Institute, Pittsburgh. Aishwarya has over 6+ years of hands-on Machine Learning experience and 20+ publications in top-tier conferences like AAAI, ACL, CVPR, NeurIPS, EACL e.t.c. She has worked on a wide spectrum of problems that involve Large Scale Graph Neural Networks, Machine Translation, Multimodal Summarization, Social Media and Social Networks, Human Centric ML, Artificial Social Intelligence, Code-Mixing e.t.c. She has also mentored several Masters and PhD students in the aforementioned areas. Aishwarya serves as a reviewer in various NLP and Graph ML conferences like ACL, EMNLP, AAAI, LoG e.t.c. She has had the opportunity of working with some of the best minds in both academia and industry through collaborations and internships in Microsoft Research, University of Michigan, NTU Singapore, IIIT-Delhi, NTNU-Norway, University of South Carolina e.t.c. ",https://www.youtube.com/watch?v=ruwz_OA4XTo,Computer Software,"This session presents the concept of """"AI Collusion"""" specifically within the context of LLM judges, highlighting a unique and often overlooked challenge in AI evaluation. I will explain how this problem emerges and why it's essential to address it.
- Drawing from my expertise, I’ll provide actionable strategies to mitigate AI collusion, focusing on prompting methods, calibration techniques, and additional components.",MLOps & GenAI World 2024,ruwz_OA4XTo
Susan Chang,Elasticsearch,Principal Data Scientist,Growing your ML career via technical writing and speaking: Tips and lessons,"This talk goes through the process of how I started writing about technical topics, leading to fast career growth, building an audience, speaking and keynoting at conferences, and eventually even a book deal. The talk aims to show you how to start, and how to gain career growth opportunities with writing and speaking.

This talk is based on the personal experience of the author of the O'Reilly book, Machine Learning Interviews.","-How to start writing technical content, such as a blog
-How to build an audience through speaking and writing
-Finding opportunities to speak and be published",,Applied Case Studies,2.0,Business and stakeholder alignment,"Technical Writing, Career Growth, Public Speaking
","Susan Shu Chang is a principal data scientist at Elastic, which powers search around the world. Previously, she built machine learning at scale in the fintech, social, and telecom industries. She is the author of Machine Learning Interviews, published by O’Reilly.",https://www.youtube.com/watch?v=QTY8m6IjQRQ,"Computer Software, Information Technology & Service, Other
",,TMLS 2024,QTY8m6IjQRQ
Kallie Levy,Superwise,Software Engineer,A Guide to Putting Together a Continuous ML Stack,We'll take a hands-on dive into implementing the 1st level of MLOps maturity and performing continuous training of the model by automating our ML pipeline. We'll start with the ML pipeline and see how we can detect performance degradation and data drift in order to trigger the pipeline and create a new model based on fresh data.,"See an example of an ML pipeline implementation using Flyte
Deploy model to an endpoint
Define monitoring policies (include some best practices)
Trigger ML pipeline to create a new model based on fresh data ",,Workshop,4.0,Infrastructure and scalability,,"Kallie Levy is an ML and data engineer. She started out working on a data-intensive, near-real-time system for the Israeli Defense Forces. Her greatest dev passions are around high-scale data ingestion and handling data lake and warehouse architecture. Currently, she works as a software engineer at Superwise, an end-to-end machine learning observability platform, and currently works on the development of the system’s entire data lake infrastructure. 
In her free time, she likes to play sports, especially soccer!",https://www.youtube.com/watch?v=N9HjsgK8oI8,,we leverage hands on experience and real life examples from organizations similar to those in the audience,,N9HjsgK8oI8
Arsene Fansi Tchango,Mila - Quebec Institute in Artificial Intelligence,Senior Applied Research Scientist,Towards Trustworthy Automatic Diagnosis Systems by Emulating Doctors’ Reasoning with Deep Reinforcement Learning,"The automation of the medical evidence acquisition and diagnosis process has recently attracted increasing attention in order to reduce the workload of doctors and democratize access to medical care. However, most works proposed in the machine learning literature focus solely on improving the prediction accuracy of a patient's pathology. We argue that this objective is insufficient to ensure doctors' acceptability of such systems. In their initial interaction with patients, doctors do not only focus on identifying the pathology a patient is suffering from; they instead generate a differential diagnosis (in the form of a short list of plausible diseases) because the medical evidence collected from patients is often insufficient to establish a final diagnosis. Moreover, doctors explicitly explore severe pathologies before potentially ruling them out from the differential, especially in acute care settings. Finally, for doctors to trust a system's recommendations, they need to understand how the gathered evidences led to the predicted diseases. In particular, interactions between a system and a patient need to emulate the reasoning of doctors. In this talk, we propose to model the evidence acquisition and automatic diagnosis tasks using a deep reinforcement learning based approach that considers three essential aspects of a doctor's reasoning, namely generating a differential diagnosis using an exploration-confirmation approach while prioritizing severe pathologies. We discuss metrics for evaluating interaction quality based on these three aspects, and demonstrate that the proposed solution performs better than existing models while maintaining competitive pathology prediction accuracy.",Integrating doctor's reasoning in a Machine Learning system,,Technical / Research,4.0,"Model dev, training, arch.","Deep Reinforcement Learning, Medical Evidence Acquisition, Differential Diagnosis","Arsene Fansi Tchango is a senior applied AI scientist at Mila, the Quebec AI Institute. His topics of interest are reinforcement learning, natural languange processing, and the application of deep learning techniques on graphs. Before joining Mila, Arsene obtained his PhD at INRIA, the French National Institute for Research in Digital Science and Technology and spent more than 5 years working in the industry.",https://www.youtube.com/watch?v=7qTRtdJHwHk,"Hospital & Health Care, Information Technology & Service
",Take into account the perspectives of the end-users when designing ML solutions,,7qTRtdJHwHk
Niklas Nielsen,Log10,"CTO, Co-founder",Scale Expert Review by 10x to Ship AI Apps at Lightning Speed,"The time and expense of subject matter expert (SME) review is a major barrier to developing generative AI applications, especially for high-risk use cases such as healthcare, finance, insurance, and more. Log10 scales SME review by 10x or more to accelerate deployment to production.

Our AutoFeedback system customizes domain-specific evaluation models that review LLM completions in real time with near-human accuracy, leveraging proprietary Latent Space Readout technology that needs 90% less data than fine-tuned evaluation model approaches.  With as few as 20 SME-labeled examples, dev teams can rapidly assess and enhance the accuracy of their generative AI app. 

We’ll demo AutoFeedback in a summarization use case, generating scores that assess the quality of CNN news summaries in real time. We’ll show that Latent Space Readout delivers superior accuracy to LLM-as-a-judge, and is cheaper and faster to use than fine tuning an evaluation model with comparable accuracy.

",,,Furture of AI,,Performance optimization and efficiency,"AutoFeedback, Latent Space Readout, SME review","Niklas Nielsen is CTO & Co-founder of Log10.io, a platform that rapidly measures and improves the accuracy of LLM applications by scaling your subject matter experts. Nik previously was Head of Product at MosaicML (acq. Databricks). Prior to that he worked at Intel and Mesosphere on building Distributed Systems deployed at large scale at companies such as Twitter and Apple, and at Adobe on the Virtual Machines and Compilers team. He co-founded CustomerDB, a startup applying AI to product management.",https://www.youtube.com/watch?v=Hg_kuDL5Sfs,,,MLOps & GenAI World 2024,Hg_kuDL5Sfs
Amit Kesarwani,LakeFS,"Director, Solution Engineering",From Chaos to Control: Mastering ML Reproducibility at scale,"Machine learning workflows are not linear, where experimentation is an iterative & repetitive to and fro process between different components. What this often involves is experimentation with different data labeling techniques, data cleaning, preprocessing and feature selection methods during model training, just to arrive at an accurate model.

Quality ML at scale is only possible when we can reproduce a specific iteration of the ML experiment–and this is where data is key. This means capturing the version of training data, ML code and model artifacts at each iteration is mandatory. However, to efficiently version ML experiments without duplicating code, data and models, data versioning tools are critical. Open-source tools like lakeFS make it possible to version all components of ML experiments without the need to keep multiple copies, and as an added benefit, save you storage costs as well."," In this talk, you will learn how to use a data versioning engine to intuitively and easily version your ML experiments and reproduce any specific iteration of the experiment.

This talk will demo through a live code example:
• Creating a basic ML experimentation framework with lakeFS (on Jupyter notebook)
• Reproducing ML components from a specific iteration of an experiment
• Building intuitive, zero-maintenance experiments infrastructure",,Workshop,6.0,Performance optimization and efficiency,"ML reproducibility, data versioning, lakeFS.
","Amit heads the solution architecture group at Treeverse, the company behind lakeFS, an open-source platform that delivers a Git-like experience to object-storage based data lakes. 
Amit has 30+ years of experience as a technologist working with Fortune 100 companies as well as start-ups. Designing and implementing technical solutions for complicated business problems. 
As an entrepreneur, he launched a cloud offering to provide Data Warehouse as a Service. Amit holds a Master’s certificate in Project Management from George Washington University and a bachelor’s degree in Computer Science and Technology from Indian Institute of Technology (IIT), India. He is the inventor of the patent: System and Method for Managing and Controlling Data",https://www.youtube.com/watch?v=MZGSwF4HRlM,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Telecommunications
",,TMLS 2024,MZGSwF4HRlM
Aniket Maurya,Lightning AI,Developer Advocate,Finetuning a large language model on a custom dataset,"This is a hands-on workshop for finetuning large language models using custom dataset. By the end of this workshop, you will learn about parameter efficient finetuning, optimised inference and tricks to finetune models at scale. ",Parameter efficient finetuning and LLM optimisations for very large models.,"Python, PyTorch basics",Workshop,5.0,"Model dev, training, arch.","Finetuning, Large language models (LLMs), Custom dataset





",Aniket is a Developer advocate at Lightning AI. He is an open source enthusiast and contributor to some popular repos like Lit-GPT and Gradsflow.,https://www.youtube.com/watch?v=OsD0KkyYXoY,,,MLOps & GenAI World 2023,OsD0KkyYXoY
Niv Hertz,Aporia,Director Of ML Engineering,Spend Less Time Troubleshooting ML Production Issues,"Business stakeholders are unhappy with the model decisions again? Manual triage takes up a lot of bandwidth from your team every single time? In this workshop, you’ll learn how ML leaders identify and troubleshoot ML issues in production faster than ever. By being more proactive about common types of ML-specific production issues such as model drift, you’ll be able to spend significantly less time troubleshooting and gain peace of mind to focus on cooler, mission-critical projects.","Spend less time and resources on troubleshooting ML production issues. 
Model drift awareness and monitoring 
Improving model decision processes","Basic understanding of machine learning concepts including model development and deployment.
Experience with ML production environments and awareness of common issues such as model drift.
Familiarity with data science research processes and techniques.
Ability to apply analytical and problem-solving skills to identify and address production issues proactively.",,4.0,Performance optimization and efficiency,"ML production issues, Model drift, Troubleshooting
","Niv Hertz is the Director of ML Engineering at Aporia, where he customizes ML observability to support any use case or production need. Niv started his career as a software engineer and cyber-security expert, and now focuses on helping ML teams gain immediate impact from their data and ML pipelines. When off the clock, Niv enjoys hiking and playing or watching basketball.",https://www.youtube.com/watch?v=hTNqD0wi98o,,,MLOps & GenAI World 2023,hTNqD0wi98o
Dan Adamson,Armilla AI,CEO and Cofounder,Testing for fairness in AI HR systems: hidden dangers and real-world lessons on how to detect and prevent bias,"Testing for fairness in AI HR systems: hidden dangers and real-world lessons on how to detect and prevent biasAbstract: HR systems can perpetuate biases and represent a significant risk to organizations and harms to candidates. In this tutorial, we will review how to detect bias issues in HR systems, including resume screening and promotion models with Armilla, a QA for ML tool that is being used for formal assessments, including those under the new New York City bias law. We'll look at hidden biases and common motifs that can cause these systems to fail, as well as suggestions for making these systems more robust.",,,Workshop,3.0,"Ethics, governance compliance ",,"Dan Adamson is the Co-Founder and CEO of Armilla.AI, a company helping institutions create trust in their AI. He co-founded PointChain Technologies, an AI-based neo-banking platform for high-risk industries and was Founder/CEO of OutsideIQ until its acquisition by Exiger, where he remained as their President overseeing product and cognitive computing research. OutsideIQ deployed AML and anti-fraud models to over 100 global financial institutions and built AI solutions for the HR and Insurance industries. He also previously served as Chief Architect at Medstory, a vertical search start-up acquired by Microsoft. Adamson holds several search algorithm and cognitive computing patents, and holds a Master of Science degree from U.C. Berkeley.",https://www.youtube.com/watch?v=mbPo2lTvIK0,,,TMLS 2022,mbPo2lTvIK0
Jinen Setpal,DagsHub,Machine Learning Engineer,Interpretability Tools are Feedback Loops,"Fundamentally - Machine Learning as a field is designed to emulate the way humans think; hence, *neural* networks. When we train our models, we use optimizers and loss functions to measure their success. While these functions make sense mathematically, they are far from intuitive or explaining what happened behind the scenes. It’s hard to pick the correct functions, and performing huge grid searches to hyperparameter tune at scale is as logical as bruteforcing an SHA-256 hash. 

On the other hand - Interpretability techniques can’t really be used in a training context but are intuitive in helping us understand how a given model interprets a set of data. 

This talk aims to bridge the gap between the two, connecting them within a single training loop to maximize training effectiveness without disproportionately increasing compute or training time. Making training intuitive to how humans learn should help develop models that actually work, without resorting to “useless” training.

I aim to showcase - with a practical demonstration - learning techniques to build feedback loops wherein interpretability is used to better optimize a training sequence. I also aim to discuss how this carries forward to complex architectures, and a potential approach for their relevant implementation.

Structurally: the talk would provide an overview on machine interpretability, provide a brief overview on optimizers and loss functions before jumping into the implementation walkthrough of a case study. The case study uses TensorFlow, but can generally be applied to any desired framework.","If, by the end my presentation, attendees are able to identify techniques for applying the proposed approach within their internal systems, or find themselves motivated to further research the ideas presented, I'd consider the talk a success.",,Technical,6.0,"Model dev, training, arch.",,"""I'm a second-year undergraduate studying Data Science at Purdue University. I also work part-time as a Machine Learning Engineer @ DagsHub.

I love research - especially within academia! My interests lie firmly within Machine Vision, NLP & Cybersecurity; so far, I've published some peer-reviewed papers and have pending patents within these domains.""",https://www.youtube.com/watch?v=NeRUtKW-RCo,,"I work extensively on ML Reproducibility. In fact, it is the fundamental work done within my research grant funded by Google. Time and again, I find that papers tend to document the end-result and the problem statement extensively, but not everything in between. That's where the real learning takes place, understanding what DIDN'T work and WHY.

This is rarely ever documented. Papers list working parameters, both functions and hyperparameters, but fail to explain why. I've struggled a lot with this and hope to relay potential solutions through the extent of my presentation.",TMLS 2022,NeRUtKW-RCo
Margo Wu,Georgian,Lead Investor,GenAI Investing in 2024,"An updated overview on the Gen Ai market landscape and investment activity, along with investor insights for fundraising.","An updated overview on the Gen Ai landscape and investment activity, as well as some investor insights for fundraising.",,Business Strategy or Ethics,1.0,Future trends,"GenAI Landscape, Investment Activity, Fundraising Insights
","Margo Wu is a Lead Investor at Georgian, a growth stage investment fund focused on B2B software companies leveraging applied machine learning and artificial intelligence. In her role, she is involved in deal selection, due diligence, post investment support and board governance. Prior to joining Georgian, Margo was a Senior Product Manager at Amazon and previous to this, she co-founded a biotech company called Uma Bioseed and served as the Chief Operating Officer at OneSpout. She started her career in enterprise software consulting at Accenture. Margo completed a double degree in Environment and Business and Chemistry at the University of Waterloo and also earned an MBA at Cornell Johnston Graduate School of Management.",https://www.youtube.com/watch?v=QBNX72rckbc,"Banking & Financial Services
",,TMLS 2024,QBNX72rckbc
"Shazia Akbar, Ali Madani, Santosh Hariharan, Shiva Amiri, Javier Diaz-Mejia","Altis Labs, Cyclica, Pfizer, Pivotal Life Sciences, Phenomic AI","Lead Machine Learning Engineer, Director of Machine Learning, Principal Scientist VP, Head of AI and Data Intelligence, Head of Data Science","Panel: ""Bias and Relevance in Application of ML in Healthcare""","Machine Learning (ML) technologies learn how to accomplish tasks and identify patterns from available data. This data-based learning has been critical in helping the healthcare institute, biotechnology and pharmaceutical companies in developing new technologies to improve processes like disease diagnosis from radiological and pathological images and drug design. In this panel, we will discuss how companies design ML technologies that are not only in-line with their business model but eventually will impact patients and healthcare systems.  We will also discuss the technological and sociological biases that need to be taken into account in the design of such technologies.",Examples of application of ML in healthcare; Impact of ML in healthcare technologies on patients; Potential biases in ML in Healthcare technologies,,Business,4.0,"Ethics, governance compliance ",,"Dr. Shazia Akbar is the lead machine learning engineer at Altis Labs, a Toronto-based startup which leverages deep learning technologies to gain prognostic insight from medical imaging data. Since joining Altis Labs, Shazia has designed and developed artificial intelligent systems which ingest millions of imaging data to predict patient outcomes. Some of the applications she has developed to date include a fully automated model to quantify mortality risk in early stage lung cancer patients, and an x-ray model which determines in-patient admission risk of hospital patients diagnosed with community acquired pneumonia. 

Shazia gained her PhD from the University of Dundee, UK, after which she joined the department of Radiology at New York University, US. In 2018, Shazia completed her postdoctoral fellowship at Sunnybrook Research Institute and the Vector Institute, designing novel deep learning algorithms for digital pathology. Her research interests include explainable AI, weakly supervised learning and applications of AI in healthcare.

Ali Madani leads ML technology development at Cyclica, a leading Canadian biotechnology company focused on AI based drug discovery. He is also editor of special Topic Artificial Intelligence in Cancer Diagnosis and Therapy at MDPI and work as an AI educator with companies like WeCloudData. Ali is a PhD graduate of University of Toronto; an alumni of University of Waterloo School of Engineering; and, attained a master of mathematics from the University of Waterloo. He is an active member of the machine learning community in Toronto and speaks in world and Canada wide conferences, webinars and workshops about technology development, machine learning, drug discovery and cancer therapeutics. He has also published more than 20 scientific articles in high impact factor journals on these subjects. 

Santosh Hariharan is a Principal Scientist at Pfizer, committed to curing disease and improving patient lives. He enjoys solving complex biological problems using simple blocks with a motto of ""Seeing is Believing"". He develops and analyzes complex biology by looking at individual cells, evaluating their response to drugs/genetic perturbation and developing predictive models using AI and machine learning (Phenotyping). 

Shiva Amiri is the VP, Head of AI and Data Intelligence at Pivotal Life Sciences, working at the intersection of computing and biology with experience in large-scale, multi-stakeholder technology development in data science and biology with a focus on computational biology, bioinformatics, machine learning, and big data systems. Shiva is a team builder and entrepreneurial in cutting edge computational methods in biology, digital health and medical research with a track record in big data/data science and program execution and strategy.

Javier is a data scientist with 15+ years of experience in projects aiming to solve problems of relevance for human health. He has experience in the academic, non-profit and industry sectors in Mexico, USA and Canada. Javier's role involves identifying organization data science needs, building teams to implement solutions addressing those needs, and serving as a bridge between technical and executive stakeholders. Javier made his PhD studies in Mexico, postdoctoral training in Toronto and currently, works as Head of Data Science at Phenomic AI, a biotech startup developing machine learning tools to fight cancer.",https://www.youtube.com/watch?v=ytRvsM8aX54,,,TMLS 2022,ytRvsM8aX54
Dan Adamson,AutoAlign AI,Interim Chief Executive Officer and Co-Founder,Robustness with Sidecars: Weak-To-Strong Supervision For Making Generative AI Robust For Enterprise," Many enterprise pilots with GenAI are stalling because of a lack of consistent performance as well as compliance, safety and security concerns. Comprehensive GenAI safety must continually evolve to mitigate critical issues such as hallucinations, jailbreaks, data leakage, biased content, and more.

Learn how AutoAlign CEO and co-founder Dan Adamson leveraged over two decades building regulated AI solutions to launch Sidecar — to ensure models are powerful AND safe. Learn how weak-to-strong controls work to put decisions directly in users' hands — improving model power while ensuring Generative AI is safe to use."," During this session, participants will have the opportunity to learn about common approaches to protect GenAI against jailbreaks, bias, data leakage and hallucinations and other harms. We’ll discuss the unique requirements of bringing LLMs to production in real-world applications, the critical importance of ensuring a high level of robustness and safety, and tools for solving these problems.

We’ll then discuss a new approach: weak supervision with a sidecar that can not only increase safety but can also make models more powerful. Finally, we’ll show some of our latest benchmarks around accuracy and discuss these state-of-the-art results.",,Business Strategy or Ethics,2.0,"Ethics, governance compliance ","Robustness, Safety, Weak supervision","Dan Adamson is a co-founder of AutoAlign, a company focused on AI safety and performance. He has also co-founded PointChain (developing a neo-banking platform using AI for high-risk and underserved industries) and Armilla AI (a company helping enterprises manage AI risk with risk transfer solutions). He previously founded OutsideIQ, deploying AI-based AML and anti-fraud solutions to over 100 global financial institutions. He also previously served as the Chief Architect at Medstory, a vertical search start-up acquired by Microsoft. Adamson holds several search algorithm and AI patents in addition to numerous academic awards and holding an M.Sc. from U.C. Berkeley and B.Sc. from McGill. He also serves on the McGill Faculty of Science Advisory Board.",https://www.youtube.com/watch?v=REPvyezAUvw,"Automotive, Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology Services, Insurance, Marketing & Advertising, Telecommunications",This session is unique because Dan is an AI veteran with 20+ years of experience in AI and successful AI startup exits including to Microsoft and Exiger. During this session he’ll discuss a new approach to AI safety: weak supervision with a sidecar that can not only increase safety but can also make models more powerful. Dan will present our latest research with findings of AI safety that ALSO improves model performance.,MLOps & GenAI World 2024,REPvyezAUvw
Tom Smoker,WhyHow.AI,Technical Founder,Multi-Graph Multi-Agent systems - Determinism through Structured Representations," As multi-agent systems increasingly get adopted, the range of unstructured information that agents need to process in structured ways, both to return to a user, but also to return to an agent system will increase. We explore the increasing trend of multi-graph multi-agent systems to allow for deterministic information representation and retrieval look like."," Why structured knowledge representations are important, how structured knowledge representation requirements have changed and will change in an increasingly agentic-driven world with complex multi-agent systems.",,Applied Case Studies,4.0,Future trends,"Multi-agent systems, Structured knowledge representation, Deterministic information retrieval





",Co-Founder @ WhyHow.AI,https://www.youtube.com/watch?v=wup9vlT03H8,,,MLOps & GenAI World 2024,wup9vlT03H8
Jesse Cresswell,Layer 6 AI @ TD,Senior Machine Learning Scientist,Navigating the Tradeoff Between Privacy and Fairness in ML,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies often worsens unfair tendencies in models. In this talk we address the intersection of privacy and fairness in machine learning, and offer research-based solutions for navigating the tradeoffs.",Applying privacy enhancing technologies can increase bias and unfairness in ML models. Practitioners need to consider the intersection of these important ethical ideas.,,Technical,5.0,"Ethics, governance compliance ",,"Jesse is a Senior Machine Learning Scientist at Layer 6 AI within TD, and is the Team Lead for Credit Risk. His applied work centers on building machine learning models in high risk and highly regulated domains. Jesse leads research on privacy enhancing technologies for machine learning including topics of Federated Learning and Differential Privacy.",https://www.youtube.com/watch?v=BE9DbfM0XgI,,,TMLS 2022,BE9DbfM0XgI
Charles Frye,THE FULL STACK,Deep Learning Educator,Chat with MLOpsWorld: Engineering an LLM Application,"We've all seen the impressive demos of large language model-powered software, from Pull Request bots and flight booking assistants to HustleGPT. But most of that demoware never makes it to production. What can engineers do to help get LLM applications over the line from concept to product? In this workshop, we'll build a simple chatbot over past MLOps World content and then layer on the tooling and techniques required to take it to the next level.","Building an actually good LLM application is not plug-and-play, but it is possible.",,Workshop,5.0,Deployment and integration,"Building LLM Applications, MLOps Techniques, Production Engineering
","Charles teaches people about AI on the internet. Since completing a PhD on neural network optimization at the University of California, Berkeley, he has taught machine learning fundamentals and MLOps with Weights & Biases and Full Stack Deep Learning and delivered talks and trainings at venues from NVIDIA GPU Technology Conference to DeveloperWeek.",https://www.youtube.com/watch?v=Ck7uBz88Q_w,"Computer Software, Information Technology & Service
",,MLOps & GenAI World 2023,Ck7uBz88Q_w
Patrick Halina,Pinterest,Engineering Manager,Web Extraction With LLMs,Since the dawn of the internet people have scraped websites to build up datasets. How has that changed with the advent of LLMs? This talk will discuss our learnings in applying state of the art approaches to understanding webpages and extracting information. We'll share lessons from parsing over 1 billion webpages per day at Pinterest.," -Overview of state of the art systems for extracting data from webpages
-Our experience in testing out different approaches, from GPT to open source LLMs to simple models
-Results from research into our own internal approaches to web extraction",,Applied Case Studies,3.0,Deployment and integration,"Web Extraction, State-of-the-Art Systems, LLM Applications
",I lead the Content Mining team at Pinterest. We use ML to understand the webpages and extract useful information for our users.,https://www.youtube.com/watch?v=olf7hs0c0zk,"Information Technology & Service
",,TMLS 2024,olf7hs0c0zk
Remi Cadene,Hugging Face,ML for Robotics,LeRobot: Democratizing Robotics,"Learn about how LeRobot aims to lower the barrier of entry to robotics, and how you can get started!"," 1. What LeRobot's mission is.
2. Ways in which LeRobot aims to lower the barrier of entry to robotics.
3. How you can get started with you own robot.
4. How you can get involved in LeRobot's development.",,Virtual Talk,3.0,Introduction to MLOps and GenAI,"Democratizing robotics, Accessibility , LeRobot development","I build next-gen robots at Hugging Face. Before, I was a research scientist at Tesla on Autopilot and Optimus. Academically, I did some postdoctoral studies at Brown University and my PhD at Sorbonne.

My scientific interest lies in understanding the underlying mechanisms of intelligence. My research is focused on learning human behaviors with neural networks. I am working on novel architectures, learning approaches, theoritical frameworks and explainability methods. I like to contribute to open-source projects and to read about neuroscience!",https://www.youtube.com/watch?v=apCGauhyq_c,Computer Software,LeRobot is positioned to be one of the most impactful open source libraries for robotics due to its support from the Hugging Face ecosystem and focus on end-to-end machine learning techniques.,MLOps & GenAI World 2024,apCGauhyq_c
"Winston Li, Nikita Medvedev","Arima, Coca Cola","Founder, Director of Advanced Analytics",The Application of Mobile Location Data for Vending Machine Site Selection and Revenue Optimization.,"In this presentation, we present an innovative approach to utilizing mobility data to optimize the placement of vending machines in Canada. Coca-Cola has more than 10k vending machines in various locations and their ROI heavily depends on the amount of foot traffic next to them as well as who those people are. For this use case, we’ll be concentrating on using the super detailed mobility data to understand the difference between our best machines and worst at scale, and optimizing their location based on the mobility data to increase the ROI. In addition to the practical and business application, we’ll also be able to share the algorithms used and the tech stack with the audience.",Mobility data as an alternative data source for consumer related analytics and its recency and granularity and really drive measurable business outcomes.,,Case Study,4.0,Business and stakeholder alignment,,"Winston is the founder of Arima, a Canadian based startup that provides consumer data to its users. Our flagship product, the Synthetic Society, is a privacy-by-design, individual level database that mirrors the real society. Built using trusted sources like census, market research, mobility and purchase patterns, it contains 10k+ attributes across North America and enables advanced modelling at the most granular level.

Prior to founding Arima, Winston was the Director of Data Science at PwC and Omnicom. Winston is also a part-time faculty member at Northeastern University Toronto and sits on the advisory board of the Master of Analytics program.


Nikita is the Director of Advanced Analytics at Coca-Cola Canada Bottling Limited. Together with his team he is transforming terabytes of business operations data into actionable insights to drive growth and innovate in the Consumer Packaged Goods industry. He loves finding novel solutions to old problems and is obsessed with driving real lasting change through better use of data.

Nikita has over 10 years of experience in the Retail and Consumer Packaged Goods industries, working for companies like Loblaw and Sears. He is also an alumnus of the Master of Management Analytics program from Queen’s University, and holds a Bachelor of Finance & Economics degree from University of Toronto.",https://www.youtube.com/watch?v=E3TC1-dmlNE,,"Arima is building a Canadian version of the synthetic population data so we have first hand experience on why this problem is important and why people need synthetic data. Furthermore, our case studies come from real industry projects with our partners, which can be useful lessons for others to explore.
",TMLS 2022,E3TC1-dmlNE
D. Sculley,Kaggle,CEO,Is it too much to ask for a stable baseline?,"Evaluation and monitoring are the heart of any reliable machine learning system.  But finding a stable reference point, a reliable comparison baseline, or even a decent performance metric can be surprisingly difficult in a world that is beset by changing conditions, feedback loops, and shifting distributions.  In this talk, we will look at some of the ways that these conditions show up in more traditional settings like click through prediction, and then see how they might reappear in the emerging world of productionized LLMs and generative models.  ","Evaluation is hard, but not impossible, and with enough care we can probably say something useful about our models.",,Advanced Technical/Research,3.0,Performance optimization and efficiency,"Evaluation, Monitoring, Baseline
","D. is currently CEO of Kaggle and GM of 3P ML Ecosystems at Google. Prior to this role, he was a director of engineering in the Google Brain team, leading research teams working on robust, responsible, reliable and efficient ML and AI. During his 15 years at Google, he has worked on nearly every aspect of machine learning, and have led both product and research teams including those on some of the most challenging business problems.  His work on machine learning and technical debt helped lay the foundation for the field of MLOps, and the book Reliable Machine Learning was named Best MLOps Book of 2022.",https://www.youtube.com/watch?v=qM8b4NqSNdM,This is broadly targeted,,,qM8b4NqSNdM
Kshetrajna Raghavan,Shopify,Senior Staff ML Engineer,Multimodal LLMs for product taxonomy at Shopify," At Shopify we fine-tune and deploy large vision language models in production to make millions of predictions a day, and leverage different open source tooling to achieve this.
In this talk we walkthrough how we went about doing it for a generative ai use case at Shopify's scale."," a. Getting to Know Vision Language Models:

The Basics: We'll kick things off with a quick rundown of what vision language models are and how they work.
Cool Uses: Dive into some awesome ways these models are being used in e-commerce, especially at Shopify.
b. Fine-Tuning and Deployment:

Tweaking the Models: Learn the ins and outs of fine-tuning these big models for specific tasks.
Going Live: Tips and tricks for deploying these models so they can handle millions of predictions every day without breaking a sweat.
c. Open Source Tools:

Tool Talk: How to pick the right open-source tools for different stages of your model journey.
Smooth Integration: Real-life examples of how we fit these tools into our workflows at Shopify.
d. Scaling Up and Speeding Up:

Scaling Challenges: The hurdles we faced when scaling these models and how we jumped over them.
Speed Boosts: Techniques to keep things running fast and smooth in a production setting.
e. Generative AI Case Study:

Deep Dive: A step-by-step look at a specific generative AI project we tackled at Shopify, from start to finish.
Key Takeaways: What we learned along the way and how you can apply these lessons to your own projects.",,case study & advanced technical,4.0,"Model dev, training, arch.","Vision language models, Fine-tuning, Generative AI"," With over 12 years of industry experience spanning healthcare, ad tech, and retail, Kshetrajna Raghavan has spent the last four years at Shopify building cutting-edge machine learning products that make life easier for merchants. From Product Taxonomy Classification to Image Search and Financial Forecasting, Kshetrajna has tackled a variety of impactful projects. Their favorite? The Product Taxonomy Classification model, a game-changer for Shopify’s data infrastructure and merchant tools.

Armed with a Master’s in Operations Research from Florida Institute of Technology, Kshetrajna brings a robust technical background to the table.

When not diving into data, Kshetrajna loves jamming on guitars, tinkering with electric guitar upgrades, hanging out with two large dogs, and conquering video game worlds.",https://www.youtube.com/watch?v=DNPLu3qNUN8,"Computer Software, Information Technology Services","Real-World Action:

This isn’t just theory – we’re talking about real-world applications of vision language models at a massive scale. You’ll get practical insights and tips you can actually use.
b. Big Scale, Big Challenges:

We’re dealing with the kind of scale that involves making millions of predictions daily. Not many companies face this, so the insights here are pretty special.
c. Open Source Love:

We’re all about open-source tools, which means you get to learn about cost-effective solutions that are accessible to everyone.
d. From Start to Finish:

We’ll walk you through the whole process, from fine-tuning to deployment and performance tweaks. You’ll get the full picture.
e. Fun and Interactive:

Expect practical examples, maybe some code snippets, and who knows, possibly a live demo. We’re keeping it engaging and hands-on.
f. E-Commerce Insights:

Since we’re in the e-commerce game, you’ll get industry-specific insights that are super relevant if you’re in the same boat
",MLOps & GenAI World 2024,DNPLu3qNUN8
Nadia Rauch,Nylas,Sr. Eng. Manager Intelligence,Evolving with AI: Insights from Nyla's Generative AI Journey,"In today's rapidly evolving technological landscape, the integration of cutting-edge technologies such as Generative Artificial Intelligence (AI) presents both unprecedented opportunities and challenges for businesses. This presentation delves into the journey of Nylas in crafting and executing a strategic roadmap for the adoption of Generative AI within our organization.

Drawing upon real-world experiences and insights gleaned from our implementation process, we offer a firsthand account of the strategies, methodologies, and best practices that enabled us to seamlessly integrate Generative AI into our workflows while concurrently fulfilling existing customer commitments (in just 3 months!).",,,case study & advanced technical,3.0,Business and stakeholder alignment,"Generative AI, Strategic roadmap, Business integration","Nadia is an experienced leader managing Nylas’ Machine Learning and Data Engineering teams, where she joined in 2020. With 14+ years of experience building products and working with data in multiple roles, she brings a wealth of expertise to her current position. Nadia holds a Bachelor's and Master's degree in Computer Engineering from the University of Florence and specializes in Knowledge Bases and Ontologies. During her doctoral studies, she developed a pioneering Smart City system, an innovative project which showcased her ability to harness data from diverse sources to create actionable insights for urban planning and management. Additionally, Nadia is a dedicated advocate for diversity in tech, serving as a committee member for TMLS - Women x AI, where she empowers young women entering the field.",https://www.youtube.com/watch?v=ktatsqlAjHs,"Information Technology Services, Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Insurance, Marketing & Advertising, Telecommunications",,MLOps & GenAI World 2024,ktatsqlAjHs
Christian P. Calderon,Zapata AI,MLOps & Deployment Engineer,AI-ready Data Infrastructure for Real-time Sensor Data Analytics on the Edge,"AI and ML use cases involving real-time sensor data in edge environments present numerous challenges, including real-time data cleaning and transformation, merging with historical data, and running power-hungry models on-premises. Using Zapata AI’s race strategy analytics work with Andretti Global as a case study, Christian Picón Calderón will share practical lessons in building the data architecture necessary to support real-time data analytics use cases on the edge, exploring parallel use cases across industries.",How to build an AI-ready data infrastructure to overcome the challenges in deploying real-time sensor data analytics applications to the edge.,,MLOps & Infrastructure,5.0,Introduction to MLOps and GenAI,,"Christian Picón Calderón is an accomplished MLOps Engineer at Zapata Computing, Inc. with over 3 years of experience in designing and implementing machine learning platforms based on open-source technology. Christian has been instrumental in designing and architecting systems for machine learning applications for various client projects. Christian's responsibilities also include deploying models into production, managing core aspects like retraining, monitoring, exposure as a service, drift detection, version control, and auditability. He collaborates closely with data engineers to define data pipelines and requirements for the models and takes ownership of the technical interview component for Software, Machine Learning, and Data Engineering positions, as well as technical onboarding.",https://www.youtube.com/watch?v=QAghBkZX7sM,"Automotive, Banking & Financial Services, Telecommunications, Other
",,TMLS 2024,QAghBkZX7sM
Jekaterina Novikova,AI Risk and Vulnerability Alliance,Science Lead,The Dual Nature of Consistency in Foundation Models: Challenges and Opportunities," Consistency is an important factor that needs to be present in any trustworthy model. In this talk, I will speak about consistency in LLMs and foundation models, how to measure it, what are the mitigation practices of its negative consequences, and what are the ways to use it to the advantage.","It is still problematic for LLMs and foundation models to generate consistent outputs. This problem may have important negative consequences and needs to be properly addressed. However, observed inconsistencies may also be used to the advantage and I will present several examples of this.",,Research or Advanced Technical,6.0,"Model dev, training, arch.","Consistency in LLMs, Foundation Models, Mitigation Practices
","Jekaterina Novikova is a Science Lead at the AI Risk and Vulnerability Alliance, where she leads research efforts towards developing responsible and trustworthy AI systems. She has 10+ years of experience working in both research industry and academia, specializing in NLP evaluation and machine learning for health, and focusing on user-focused approaches. Jekaterina has PhD in Computer Science from the University of Bath/UK, and a strong track record of publications in top-level conferences",https://www.youtube.com/watch?v=eQh3B7QBcEU,"Computer Software, Information Technology & Service, Other
",,TMLS 2024,eQh3B7QBcEU
Bowen Yang,Cohere,Member of Technical Staff,"Unraveling Long Context: Existing Methods, Challenges, and Future Directions"," We will discuss how to scale transformer models to longer context, some of the challenges we are facing from modelling and framework perspective and some directions we can explore.","This panel talk will discuss methods and analysis on long context models and long context extrapolation. We will explore a range of approaches, from data and modelling to framework level, and delve into the current challenges and solutions in the field. We will cover topics including training, evaluation and inference.",,Research or Advanced Technical,6.0,"Model dev, training, arch.","Long Context Models, Transformer Models, Extrapolation.
","Work on LLMs, pretraining, long-context and deep-learning engineering.",https://www.youtube.com/watch?v=jT0g7-GUMGY,"Computer Software, Information Technology & Service
",,TMLS 2024,jT0g7-GUMGY
Steven Waslander,University of Toronto,"Professor, Institute for Aerospace Studies / Director, Toronto Robotics and AI Laboratory",Where's the Road? The Challenge of Autonomous Driving Perception in Winter,"Autonomous driving solutions are steadily progressing toward real-world deployments, but most companies are focused on driving in clear weather days in benign climates.  Our work on exposing the challenges of Canadian winters for perception tasks has led to the Canadian Adverse Driving Conditions Dataset, and to multiple advances in all-weather autonomy that set the stage for more complete dominion of robotics systems over sensor degradation due to precipitation and accumulation.  In this talk, I'll highlight some of the worst problems that arise for autonomous systems in Canada, and lay out our plans for WinTOR, a new University of Toronto research program aimed at helping self-driving vehicles extend their range to our roadways year round.","That winter is harder than clear weather, but that we can still build safe self-driving cars for any weather condition, if we take the time to work through the added challenges.",,Technical,6.0,Future trends,,"Prof. Steven Waslander is a leading authority on autonomous aerial and ground vehicles, including multirotor drones and autonomous driving vehicles. He received his B.Sc.E.in 1998 from Queen’s University, his M.S. in 2002 and his Ph.D. in 2007, both from Stanford University in Aeronautics and Astronautics.  He founded and directed the Waterloo Autonomous Vehicle Laboratory (WAVELab) in 2008-2018, and the Toronto Robotics and Artificial Intelligence Laboratory (TRAILab) at the University of Toronto from 2018 onward.
Prof. Waslander’s work on autonomous vehicles has resulted in the Autonomoose, the first autonomous vehicle created at a Canadian University to drive on public roads. His insights into autonomous driving have been featured in the Globe and Mail, Toronto Star, National Post, and the Rick Mercer Report. He has over 160 publications and host the Self-Driving Car Specialization on Coursera, which has accumulated over 150,000 learners worldwide since 2019. ",https://www.youtube.com/watch?v=pSnbiGee-bQ,,,TMLS 2022,pSnbiGee-bQ
"Kevin Laven, Alex Gobolos","Deloitte, Dataiku","Partner, Data Science Leader, Solutions Engineer",Creating an ML model as a business user,"Opportunity for business users and executives to be exposed to the following topics: How to validate use cases, Approaches to building models, Business case for deployment ",High-level steps on creating an ML model as a business user,Basic knowledge of ML,Workshop,4.0,Business and stakeholder alignment,"Creating ML Models, Business Use Cases, Model Deployment","Kevin Laven leads the Energy, Resources, and Industrial sector team within Deloitte Canada’s Artificial Intelligence practice. Kevin’s AI experience started in 2003 with a Masters degree at the University of Toronto Machine Learning Lab, and includes dozens of AI models for ER&I applications.

Alex is a Solutions Engineer at Dataiku. He works with customers to get value from all things data and analytics, from data access and exploration, to machine learning and AI. Alex has worked in the analytics space for the last 15+ years including project delivery, consulting, and solutions engineering across industries such as banking and insurance, healthcare, and manufacturing.",https://www.youtube.com/watch?v=pU9-XV0DCUI,,,TMLS 2023,pU9-XV0DCUI
John Jewell,Vector Institute,Applied Machine Learning,FL4Health: Private and Personal Clinical Modeling," It is well-established that the robustness and generalizability of machine-learning models typically grow with access to larger quantities of representative training data. However, in the healthcare domain and other industries with highly sensitive data, the vast majority of data exists in silos across different institutions. Centralizing the data is often discouraged, if not impossible, due to strong regulations governing data sharing. This is a fundamental barrier in the development of performant machine learning models in healthcare, and other domains. Fortunately, Federated learning (FL) provides an avenue for training models in distributed data settings without requiring training data transfer. In this talk, we'll provide an overview of FL and its application in healthcare. This will include a discussion of common challenges arising in distributed data settings, such as data drift and heterogeneity, along with modern approaches aimed at addressing these issues. We'll introduce the FL4Health library developed at the Vector Institute, which can be leveraged to easily train models on distributed clinical datasets. Finally, we'll consider some noteworthy experimental results, obtained using the library, demonstrating the utility of FL in training high-performing models in challenging clinical settings."," Attendees will have the opportunity to learn about FL and how it is used to train performant models on distributed datasets, with a specific focus on clinical tasks. In doing so, attendees will become familiar with common challenges that arise in FL, state-of-the-art techniques to address those challenges and helpful tools to get started.",,Research or Advanced Technical,6.0,Introduction to MLOps and GenAI,"Federated Learning (FL), Healthcare Data Privacy, Distributed Data Challenges
","John Jewell is an Applied Machine Learning Specialist at Vector Institute where he is currently focused on building FL4Health - a Python Package to jointly train machine learning models on distributed datasets in the healthcare domain. Prior to joining Vector Institute, John received his Master's in Computer Science from Western University under the supervision of Vector Institute Faculty Member Yalda Mohsenzadeh. During this time, he was fortunate enough to make strong contributions to the Anomaly Detection literature, an area he is very much still interested in.",https://www.youtube.com/watch?v=YSY_gaFlFhk,"Hospital & Health Care, Banking & Financial Services
",,TMLS 2024,YSY_gaFlFhk
"Roy Derks, Alex Seymour",IBM watsonx.ai,Technical Product Manager,Building AI Applications as a Developer," In today’s world, developers are essential for creating exciting AI applications. They build powerful applications and APIs that use Large Language Models (LLMs), relying on open-source frameworks or tools from LLM providers. In this session, you’ll learn how to build your own AI applications using the watsonx and watsonx.ai ecosystem, including use cases such as Retrieval-Augmented Generation (RAG) and Agents. Through live, hands-on demos, we’ll explore the watsonx.ai developer toolkit and the watsonx.ai Flows Engine. Join us to gain practical skills and unlock new possibilities in AI development!"," By attending this session, you'll acquire essential skills for effectively leveraging Large Language Models (LLMs) in your projects. You'll learn to use LLMs via APIs and SDKs, integrate them with your own data, and understand Retrieval-Augmented Generation (RAG) concepts while building RAG systems using watsonx.ai. Additionally, this session will cover Agentic workflows, guiding you through their creation with watsonx.ai. Finally, you'll explore how to work with various LLMs, including Granite, LLama, and Mistral, equipping you with the versatility needed to optimize AI applications in your development work.",,Virtual Workshop,5.0,"Model dev, training, arch.","LLM development, Retrieval-Augmented Generation (RAG), watsonx.ai","Roy Derks is a lifelong software developer, author and public speaker from the Netherlands. His mission is to make the world a better place through technology by inspiring developers all over the world. Before jumping into Developer Advocacy and joining IBM, he founded and worked at multiple startups. His personal mission is making the world better through technology.",https://www.youtube.com/watch?v=kDLnlZ8-9hQ,Computer Software,"This session uniquely emphasizes empowering developers of all skill levels to kickstart their AI application-building journey by leveraging familiar paradigms. You'll discover how to apply existing knowledge and frameworks to integrate AI seamlessly into your projects, making the transition into AI development both intuitive and efficient. This practical approach ensures that you can quickly harness the power of AI without having to navigate a steep learning curve.",MLOps & GenAI World 2024,kDLnlZ8-9hQ
Bandish Shah,MosaicML/Databricks,"Engineering Manager,",Training large language models: lessons from the trenches,"Training large AI language models is a challenging task that requires a deep understanding of natural language processing, machine learning, and distributed computing. In this talk, we will go over lessons learned from training models with billions of parameters across hundreds of GPUs. We will discuss the challenges of handling massive amounts of data, designing effective model architectures, optimizing training procedures, and managing computational resources. This talk is suitable for ML researchers, practitioners, and anyone curious about the ""sausage making"" behind training large language models.","- The challenges in training models with billions of parameters across hundreds of GPUs
- Tips and tricks to make the training work and the model to achieve high accuracy",,Case Study,7.0,"Model dev, training, arch.","Training large language models, Distributed computing, Natural language processing
","Bandish Shah is an Engineering Manager at MosaicML/Databricks, where he focuses on making generative AI training and inference efficient, fast, and accessible by bridging the gap between deep learning, large scale distributed systems and performance computing. Bandish has over a decade of experience building systems for machine learning and enterprise applications. Prior to MosaicML, Bandish held engineering and development roles at SambaNova Systems where he helped develop and ship the first RDU systems from the ground up and Oracle where he worked as an ASIC engineer for SPARC-based enterprise servers.",https://www.youtube.com/watch?v=TTCyZjcGqf0,,,MLOps & GenAI World 2023,TTCyZjcGqf0
"Josh Peters, Kiarash Shamsi",Wealthsimple,"Data Science Manager, ML Researcher",LLMs for Revolutionizing Credit Risk Assessment," The session on leveraging Large Language Models (LLMs) in revolutionizing credit risk assessment will commence with an introduction to the potential impact of LLMs on the finance industry. This will be followed by an exploration of the key benefits of LLM integration, including the enhancement of risk assessment accuracy, the utilization of alternative data sources, and the automation of credit processes. The discussion will delve into real-life case studies and examples to illustrate the practical applications of LLMs in credit risk assessment. Additionally, the session will address potential challenges and ethical considerations surrounding the use of LLMs in this context. The talk will conclude with insights on the future of credit risk assessment with LLMs, leaving room for engaging discussions and Q&A."," Improved Risk Assessment
LLMs can analyze vast amounts of unstructured data, such as financial records, transaction histories, and market trends, to provide more comprehensive and accurate risk assessments. By processing and generating human-like text, LLMs can uncover insights and patterns that traditional credit risk models may miss.

Enhanced Contextual Understanding
LLMs can provide a deeper contextual understanding of borrower profiles and financial data. They can analyze text-based information, like loan applications and customer interactions, to gain a more holistic view of a borrower's creditworthiness.

Handling Nonlinear Relationships
LLMs can capture complex nonlinear relationships within credit data, enabling them to make more accurate credit risk predictions compared to traditional linear models.

Improved Fraud Detection
LLMs can analyze transaction patterns and identify anomalies that may indicate fraudulent activities, enhancing an institution's ability to detect and prevent fraud.

Automating Credit Risk Processes
LLMs can automate the credit risk analysis process, generating credit approvals, pricing recommendations, and repayment terms. This can lead to faster decision-making, reduced manual effort, and minimized human error.

Leveraging Alternative Data
LLMs can integrate alternative data sources, such as social media profiles and online behavior, to assess credit risk for borrowers with limited or no credit history. This allows for more comprehensive and inclusive credit risk evaluations.

Enhancing Portfolio Management
By analyzing market trends and customer behavior, LLMs can assist in optimizing credit portfolios, improving risk management, and enhancing overall lending strategies.
Overall, the integration of LLMs in credit risk assessment has the potential to revolutionize the industry by providing more accurate, efficient, and inclusive credit risk evaluations, ultimately leading to better lending decisions and improved financial outcomes.",,Applied Case Studies,4.0,Business and stakeholder alignment,"LLMs, Credit Risk Assessment, Alternative Data.

","Josh: Josh is a Data Science Manager at Wealthsimple. For the last 2 years, he has led the development of the company's first credit risk models and created the data pipelines to support new credit products. 

Prior to Wealthsimple, Josh spent 7+ years working on Data Science problems in the insurance, banking and fraud spaces through his time at Accenture and Airbnb. 

Josh's educational background is in Finance, Statistics and Computer Science.

Kiarash: Kiarash Shamsi is a Ph.D. student at the University of Manitoba, and currently working as financial ML researcher at Wealthsimple. He has published as a first author in conferences such as NeurIPS, ICLR, and ICBC. His research interests are Large language models, temporal graph learning, graph neural networks, topological data analysis, and blockchain data analysis and systems.",https://www.youtube.com/watch?v=Qi0SGJqsMJA,"Banking & Financial Services
",,TMLS 2024,Qi0SGJqsMJA
Mandy  Gu,Wealthsimple,Engineering Manager,Deploying a Machine Learning Model in under 15 Minutes at Wealthsimple,"From powering money movement to fraud detection, machine learning models are critical to Wealthsimple's core business process. This year, we built our next generation Machine Learning platform with a simple goal in mind: deploy new ML models within HOURS. This is how we scoped, designed and built our platform in just under 3 months.",,,Ignite Lighting Talk,,Deployment and integration,"Machine Learning Platform, Deployment, Wealthsimple","Mandy leads the Machine Learning Platform and the Data Platform teams at Wealthsimple. Prior to working on ML-ops and infrastructure,  she was a NLP researcher in two separate conversational AI roles and a data scientist building models for the operations and client experience spaces.",https://www.youtube.com/watch?v=kceNJ8f_gMs,,"The speech is unique to Wealthsimple's experience with ML infrastructure, tooling and deployment. When it came to designing our platform, we drew inspiration from the Rails Doctrine to accelerate ML by convention over configuration and putting the developer experience on a pedestal. Our goal is to build something revolutionary, just like how Ruby on Rails transformed web development 20 years ago.
",TMLS 2022,kceNJ8f_gMs
Vipul Raheja,Grammarly,Applied Research Scientist,Intelligent Writing Assistants in the era of Large Language Models,"Text revision involves a multifaceted and iterative process, which can be daunting for human writers. They need to balance numerous factors such as content coverage, linguistic norms, tone, and discourse conventions. Intelligent writing assistants have recently transformed significantly, thanks to advances in large language models and human feedback-driven learning. They've shifted from prescriptive systems to more collaborative ones. However, they still face shortcomings in terms of quality, personalization, and usability, which limit their overall value to users. In this talk, I will share my research, challenges, and insights into the development of intelligent and interactive writing assistants designed to enhance effective communication and address quality, personalization, and usability concerns.","* How LLMs have changed the writing assistance landscape
* How to effectively leverage LLMs for building writing assistants
* Challenges of building intelligent writing assistants ",-,,5.0,"Model dev, training, arch.","Intelligent writing assistants, Large language models (LLMs), Quality and usability
","Vipul Raheja is an Applied Research Scientist at Grammarly. He works on developing robust and scalable approaches centered around improving the quality of written communication, leveraging Natural Language Processing and Machine Learning. His research interests lie at the intersection of large language models and controllable text generation methods for text revision. He has authored multiple papers at top-tier NLP conferences such as ACL, NAACL, and EMNLP. He also co-organizes the Workshop on Intelligent and Interactive Writing Assistants (In2Writing).",https://www.youtube.com/watch?v=P1mJ-s8IwzQ,Computer Software,,,P1mJ-s8IwzQ
"Suchita Venugopal, Irena Grabovitch-Zuyev",PagerDuty,"Senior Machine Learning Engineer, Staff Applied Scientist",Rapid Deployment of LLMs into Production: Strategies and Insights,"In the fast-paced domain of generative AI, the deployment of Large Language Models (LLMs) into production settings introduces a distinctive blend of challenges and opportunities. This presentation will detail our experience in incorporating LLMs into our product line within a challenging two-month period, a move motivated by the transformative potential of generative AI for enhancing our offerings. We navigated through various obstacles, such as constrained planning timelines, shifting requirements, the management of diverse stakeholder expectations, adaptation to emerging technologies, and the coordination of simultaneous workflows. These hurdles highlighted the pivotal role of data science and machine learning engineering teams in facilitating LLM integration, emphasizing the importance of security, testing, monitoring, and the pursuit of alternative solutions.

We will share the systematic approach we employed for identifying LLM use cases, validating their feasibility, engineering effective prompts, and crafting a comprehensive testing strategy. Additionally, we will introduce the LLM Service, a custom solution designed to ensure secure and efficient LLM access. This service underscores the significance of robust security protocols, the protection of customer data, the flexibility to switch LLM models to optimize performance for specific use cases, and the provision of redundancy in case of provider outages. Our discussion aims to illuminate how our expedited LLM deployment signifies the dawn of a new era in AI-driven product innovation.","In this talk, you will learn the effective processes and strategies that enabled the rapid deployment of Large Language Models (LLMs) into our production environment. We will share key takeaways from our journey, highlighting what aspects are non-negotiable, such as robust security measures and the protection of customer data, alongside insights into what strategies yielded the best outcomes. Additionally, we will openly discuss the mistakes we encountered along the way, offering valuable lessons to help you avoid similar pitfalls in your own LLM deployment projects. This session promises a candid look into the challenges and triumphs of integrating generative AI into product offerings at speed.",,Applied Case Studies,4.0,Deployment and integration,"LLM Deployment, Generative AI, Security and Testing.
","Suchita: Suchita Venugopal is a Senior Machine Learning Engineer at PagerDuty, where she specializes in implementing Generative AI features and leveraging Large Language Models (LLMs). She holds a Master of Science in Big Data from Simon Fraser University in Vancouver, Canada.

In her current role at PagerDuty, Suchita is instrumental in integrating LLM-based features, such as the PagerDuty Copilot assistant and customer support chatbots that utilize Retrieval-Augmented Generation (RAG). She also contributes to the development of Machine Learning (ML) models used in PagerDuty AIOps, helping to automate and optimize IT operations.

Irena: Irena Grabovitch-Zuyev is a Staff Applied Scientist at PagerDuty, specializing in Data Mining, Machine Learning, and Information Retrieval. She earned her Master of Science in Computer Science from the Technion - Israel Institute of Technology. Her thesis, titled ""Entity Search in Facebook,"" delved into the realm of Information Retrieval in Social Networks.

In her current role, Irena plays a significant role in developing the PagerDuty Copilot Assistant, leveraging Generative AI to streamline PagerDuty Operation Cloud. Additionally, she has contributed to the development of the Auto-Pause Incident Notifications feature, an important component of AIOps aimed at noise reduction. This feature employs a prediction model to automatically pause notifications for transient alerts, resolving them within minutes.

Before joining PagerDuty, Irena spent five years at Yahoo Research as a senior member of the Mail Mining Team. During this time, she focused on Automatic Extraction and Classification using machine learning algorithms. Her work was deployed in production within Yahoo’s mail backend, processing hundreds of millions of messages daily.

In addition to her professional accomplishments, which include presenting papers at top conferences and filing patents, Irena finds immense fulfillment in her role as a mother to her three children.",https://www.youtube.com/watch?v=ptqJ4prSHiA,,,TMLS 2024,ptqJ4prSHiA
Amish Popli,SpotHero,Data Scientist,Marketplace experimentation at SpotHero,"SpotHero is the biggest and fastest growing off-street reservation platform in North America. It is a two sided marketplace involving drivers and parking garage owners. The data science team at SpotHero is working on many interesting problems in the areas of dynamic pricing, marketing, ranking etc. One of the key challenges that we face is how we test our machine learning models in production and make sure that the changes we make lead to an improvement in our KPI’s. In this talk, I will focus on how SpotHero runs experiments whenever we make improvements or create a new model to generate prices for our parking spots. I will cover why the general A/B test framework will not work in our scenario, various approaches that we considered and introduce switchback experimentation as an alternative. I will discuss our experiment design and conclude the talk with a result from one of our experiments and our technical architecture.  ","Experimentation is a very nebulous topic. There is a lot of companies, articles and research available on  google, but each company has its own unique way of running and measuring impact from experiments.",,Case Study,4.0,Deployment and integration,," Amish Popli is passionate about solving challenging business problems using data science and machine learning. He supports multiple departments at SpotHero including, but not limited to, marketing, sales, and product development. He likes data, manipulating it, making it (simulation), modelling it, visualizing it, and yes, even cleaning it. He works with different PMs and engineers in different domains and has brought many successful products from discovery to production.",https://www.youtube.com/watch?v=je_ZtHFM-q4,,In my knowledge there is no company in North America doing data science/ML in parking industry. The problems we are solving are present in other industries but parking adds another layer of complexity on top of it.,,je_ZtHFM-q4
"Angela Xu, Kemi Borisade",CIBC,"Director, Risk Control and Fraud Analytics, Senior Fraud Data Analyst",Revolutionizing Fraud Prevention: Harnessing AI and ML to Safeguard Banking from Fraud,"In 2023, the Canadian Anti-Fraud Centre reported staggering losses of over CAD $550 million due to fraudulent activities, underscoring the urgent need for advanced security measures. At CIBC, we confront the dynamic challenges of this evolving landscape head-on by embracing cutting-edge tools, technologies, and methodologies.
Our journey is marked by formidable obstacles, including the limitations of rule-based fraud strategies, the delicate balance between sales and risk mitigation, inadequate tools for documentation validation, and the pressing demand for rapid fraud assessment. To address these challenges, our team embarked on a transformative path, leveraging next-generation self-learning Machine Learning models supplemented with custom thresholds. This approach enhances fraud detection capabilities, minimizes false positives, optimizes sales strategies, and fortifies client protection.
Furthermore, through strategic partnerships, we've embraced solutions such as Optical Character Recognition (OCR) to streamline documentation validation processes. Exploring the integration of graph databases, Natural Language Processing (NLP), and foundational models, we aim to unlock new frontiers in fraud prevention.
The culmination of our efforts heralds a new era in security, where the synergy of advanced AI and ML technologies promises unparalleled efficiency and efficacy in combating fraud. Join us as we unveil the future of fraud prevention in Canadian banking.","After attending this presentation, you will gain a comprehensive understanding of the prevailing fraud challenges within the financial industry. You will also acquire foundational knowledge of next-generation near real-time self-learning Machine Learning models, along with insights into their fundamental concepts. Additionally, you'll explore advanced cutting-edge technologies utilized in fraud detection, equipping you with valuable insights into the evolving landscape of financial security.",,Business Strategy or Ethics,3.0,Security and Privacy,"Fraud Prevention, AI and ML, Financial Security
","Angela: Angela Xu brings over 15 years of strategic data analytics experience in premier financial institutions to the Toronto Machine Learning Summit Conference. As a seasoned technical expert and strategic thinker, Angela has demonstrated success in developing and implementing innovative strategies. With a Master's degree in Statistics from the Georgia Institute of Technology in Atlanta, USA, and another Master's degree in Computer Science from China, Angela possesses a diverse skill set that she leverages to drive initiatives to tangible results.
Currently leading the Risk Control & Fraud Analytics team at CIBC, Angela focuses on regulatory breach reporting and fraud strategies for secured and unsecured lending products such as mortgages, loans, and lines of credit. Her leadership is characterized by a commitment to generating innovative ideas, influencing stakeholders, and delivering real value to both her organization and its clients.
Passionate about leveraging cutting-edge technologies to solve complex problems, Angela is dedicated to applying the latest advancements in machine learning and data analytics to add value to her company and enhance the experiences of its clients.

Kemi: A data professional with expertise in data manipulation, visualization, and machine learning libraries. Specialization in extracting valuable insights from complex data, automating processes to deliver insightful reports, and ultimately enhancing business decision-making. 
A BEng degree in Computer Hardware and Software Engineering and an MSc in Robotics and Autonomous Systems. 
Currently as a senior fraud data analyst, I utilize my skills in extracting and analyzing data as well as building machine learning models to detect high risk fraudulent applications.",https://www.youtube.com/watch?v=bZ1NP8htwYA,,,TMLS 2024,bZ1NP8htwYA
Pablo  Salvador Lopez,Microsoft,Principal AI Architect,Demystifying Multi-Agent Patterns,How to successfully build and productionalize a multi-agent architecture with semantic-kernel and Autogen.,The audience will learn how to build a multi-agent architecture following best practices using open-source technology like Semantic-Kernel and Autogen. This session will accelerate the journey from single-agent to multi-agent systems and how to productionize these systems to scale using best practices for LLMs in production.,,Workshop,5.0,Deployment and integration,"Multi-agent architecture, Semantic-Kernel, Productionalize"," As a seasoned engineer with extensive experience in AI and machine learning, I possess a unique blend of skills in full-stack data science, machine learning, and software engineering, complemented by a solid foundation in mathematics. My expertise lies in designing, deploying, and monitoring AI/ML software products at scale, adhering to MLOps/LLMOps and best practices in software engineering.

Having previously led the MLOps practice at Concentrix Catalyst and the ML Engineering global team at Levi Strauss & Co., I have developed a profound understanding of implementing real-time and batch time ML solutions for several Fortune 500 enterprises. This experience has significantly enhanced my ability to manage big data and leverage cloud engineering, particularly with Azure's AI, GCP and AWS.

Currently, at Microsoft, as a Principal Technical Member of the prestigious AI Global Black Belt team, I am dedicated to empowering the world's largest enterprises with cutting-edge generative AI and machine learning solutions. My role involves driving transformative outcomes through the adoption of the latest AI technologies and demystifying the most complex architectural and development patterns. Additionally, I am actively involved in shaping the industry’s direction in LLMOps and contributing to open source by publishing impactful software and AI solutions.",https://www.youtube.com/watch?v=cyeQ98jLQP0,"Hospital & Health Care, Information Technology Services, Computer Software, Banking & Financial Services, Enviromental Services, Automotive, Food & Beverage, Insurance, Marketing & Advertising, Telecommunications","Learning the latest patterns from an AI GBB at Microsoft. I lead the implementation of the latest technology at Microsoft (focus on Azure AI) for the largest customer in the world, so my perspective is practical and hands-on. I'll chime in on the best and worst practices for implementing these systems.",MLOps & GenAI World 2024,cyeQ98jLQP0
Jaime Tatis,TELUS,VP-Chief Insights Architect,How Is GenAI Reshaping the Business?,"Generative AI offers transformative advantages across all sectors with unlimited possibilities. Audience will learn about AI application and how it enhances efficiency, fosters innovation, and elevates problem-solving with real examples.",,,Business Strategy or Ethics,2.0,Business and stakeholder alignment,"Generative AI, Business Transformation, Efficiency
","Jaime Tatis is a visionary and technology thought leader with strong business acumen and a proven track record of collaborating with both technical and non-technical teams to drive critical initiatives. Jaime is passionate about building and developing diversely skilled high-performance teams, growing future leaders and driving business efficiency through continuous improvement and innovation.

As the Chief Insights and Analytics Officer at TELUS, a world-leading technology company, Jaime works with partners across the TELUS family of companies leading the advancement of data, AI and analytics strategy and the company’s cultural shift to create cutting-edge customer technology solutions. By thoughtfully providing data insights and analytics, along with next-generation cloud-based architecture to enable world-class Artificial Intelligence and Machine Learning capabilities, Jaime is improving business outcomes and providing best-in-class customer experiences for TELUS.",https://www.youtube.com/watch?v=M8QBzroN3WA,"Telecommunications
",,TMLS 2024,M8QBzroN3WA
"Nicolás Venegas Oliva, Sarah Sun, David Scharbach","LATAM Airlines, Scotiabank, TMLS","Technical Lead of Advanced Analytics, Director Data Science, Executive Director",Data FAILS,"We like to talk about the successes...but why don't we ever talk about the FAILS across the data world?  Join us as we swap stories crossing data, industrial, and even geographical boundaries.  We may have failed...but maybe amongst all the tales there's a lesson or two to be learnt across networking, recruiting, planning, model building, engineering....you name it ;) ","Lessons to be learnt in the experiences shared on what not to do.  In addition, we like to talk about the wins, but the reality is fails are more frequent, but also more lessons to be learnt! ",,Business Strategy,1.0,Business and stakeholder alignment,,"Nicolas Matias Venegas Oliva has 2 years of experience in backend development, 2+ years in data processing and the last 3+ years as Advanced Analytics technical leader at LATAM Airlines. During this time the team has grown from 9 to 48 highly trained professionals. It has also become the team with the highest impact generation within the company and a reference in the region in terms of MLOps and measured business impact through data products.

Sarah Sun - A decade in data has taken Sarah across multiple industries, including banking, technology, and natural resources.  While specializing in data strategy, she was trained as a data scientist and has worked across the industry in innovation, governance, AI, and also a stint as CEO of a startup.  Working in data has taught Sarah some value lessons - everything from seizing opportunities, the important of mental health, and the power of sharing stories.  Sarah was named one of the Women Executive Network's Top 100 Most Powerful Women in 2019 .",https://www.youtube.com/watch?v=S-dEDX0G0qE,,,TMLS 2022,S-dEDX0G0qE
"Naiel Samaan, Valmir Bucaj","
Ford Motor Company",AI Platform Product Owner,From Model T to Machine Learning: A Glimpse into Ford's MLOps and Hybrid Infrastructure Strategy,"
In this talk we will discuss Ford’s journey in building an AI/ML platform from inception to present day. Specifically, we will share the many lessons learned around starting a platform within an organization with deeply rooted traditions. This includes making decisions for product strategy around customer persona, technical stack, buy vs. build, on-prem vs. cloud, and centralized vs. decentralized. We will discuss the reasons behind our evolving product strategy, the challenges in finding the right product-market fit, and the many organizational challenges.","The audience will learn how to navigate organizational challenges in starting a AI/ML ecosystem at a large company with deeply rooted traditions. This will include pitfalls to avoid, building for your customer, and navigating team topology that works best for your ecosystem.",,Business Strategy,3.0,Business and stakeholder alignment,"AI/ML Platform, Organizational Challenges, Product Strategy
","

Naiel- As a seasoned product, people, and project manager, I thrive on tackling complex problems with a combination of data, analysis, leadership, and creativity. Currently, I lead a team of data scientists, tech leads, and software developers in building an enterprise machine learning operations platform for Ford Motor Company.

My expertise lies in leveraging data and machine learning, along with innovative ideas, to solve business challenges. I've successfully managed every aspect of the process, from evaluating customer needs to informing product development to leading cross-functional teams to success.

My passion for developing teams, combined with my entrepreneurial drive and expertise in data and technology, make me the ideal candidate for any company looking to solve complex challenges and achieve growth.

-

Valamir - Valmir Bucaj is a technical leader and product owner of Ford's AI/ML Platform. He has 3+ years of experienced in leading cross-functional teams, who specialize in building multi-cloud enterprise MLOps frameworks to help ML Engineers and Data Scientists scale and productionize their machine learning projects faster and more efficiently. Valmir is passionate in building AI/ML products that customers both need and love. He previously used to work as a machine learning engineer, focusing on graph neural networks for social recommendations. 

Valmir used to also work as an assistant professor of mathematics at West Point, where among other things, he designed and taught the Academy's first Data Science course for their newly established major. 

Valmir holds a PhD in Mathematics, from Rice University.""",https://www.youtube.com/watch?v=-sXK2M7lndQ,Automotive,,,-sXK2M7lndQ
Jim Olsen,ModelOp,Chief Technology Officer,"Building Automated Model Life Cycles to Show Data Science Business Contribution, Minimize the Impact of Regulation & Governance Requirements,& Keep the Freedom of Innovation","In this session, ModelOp CTO Jim Olsen shows you how to design and build a model life cycle, including how to incorporate Industry best practices as well as provides considerations for creating the model life cycle, who should be involved, and the types of issues that must be considered.","Basics of a model life cycle: What makes up a model life cycle and how do you design one
Governance: Developing an automated governance workflow
Monitoring: How to monitor models post-deployment in a flexible manner
Remediation: Creating remediation workflows that track and accelerate time to resolution","Will teach all skills, but some understanding of flow charts is helpful",Workshop,4.0,Deployment and integration,,Jim Olsen serves as Chief Technology Officer at ModelOp where he leads the technical innovation and design of the ModelOp Center platform. Jim is also integral to advising ModelOp customer CIOs and CTOs on requirements to better support their IT operations as they execute on digital business strategies that often strain technology infrastructure.,https://www.youtube.com/watch?v=FMr_JW_smQY,,"A concrete way to measure AI ROI across the organization and impelment AI Governance, as result of case studies and experience with our customers (public customers are large enterprises such as Royal Bank of Canada, etc) 
",TMLS 2022,FMr_JW_smQY
Aurimas Griciūnas,neptune.ai,Head of Product,Building an end-to-end MLOps Pipeline,"The talk will be about MLOps and the lifecycle of ML projects. I will go through stages involved in the ML project lifecycle and some key highlights from each of them. I will also explain how CI/CD is different in Machine Learning project when compared to regular software and highlight how it evolves with the maturity of MLOps processes in the organisation. I will also ground the explanations with real life examples of building out MLOps capabilities, successes and failures.",,,Ignite Lightning Talk,,Deployment and integration,"MLOps Pipeline, ML Project Lifecycle, CI/CD in ML
","Aurimas has over a decade of work experience in various data-related fields: Data Analytics, Data Science, Machine Learning, Data Engineering, and Cloud Engineering. For a few years he also led teams working with Data and Infrastructure. Today, Aurimas is Head of Product at neptune.ai.",https://www.youtube.com/watch?v=PvPDpO4IjOI,,,MLOps & GenAI World 2023,PvPDpO4IjOI
Eddie Mattia,Outerbounds,Data Scientist,MLOps Template for Time Series in Production,"In this session, we will build a complete MLOps platform that periodically retrains models and computes predictions in a batch inference pipeline. We'll show how to build a time series forecasting machine with these properties and how the entire system can be deployed in the cloud."," How to frame time series forecasting problems for XGBoost.
How to build an end-to-end MLOps system.
How to trigger workflows in the cloud based on exogenous system events.",,Workshop,5.0,Deployment and integration,"MLOps platform, Time series forecasting, XGBoost","Data scientist at Outerbounds

Building AI developer tools and many applications on top of them!",https://www.youtube.com/watch?v=6-wcagI37jw,"Telecommunications, Automotive, Banking & Financial Services, Enviromental Services, Food & Beverage, Information Technology Services, Insurance, Marketing & Advertising, Hospital & Health Care, Computer Software","We will go from zero to a battle-hardened MLOps framework in an hour. You'll be able to scale like Netflix and we'll provide all infrastructure for participants, as the content will be embedded in trial UX of the Outerbounds product.",MLOps & GenAI World 2024,6-wcagI37jw
Bhavani  Rao,Pachyderm,Technical Product Marketing Manager,What To Look For In Your Next ML Pipeline,"MLOps is not the same as DevOps. Iteration is a common theme to both methodologies but the requirements are different. Your pipelines need to version the code AND the data for easy reproducibility and rollback. Given the enormous size of datasets, data pipelines need to scale to petabytes, automatically trigger and process only the new data, rather than executing a complete run every time. Join us for this lightning talk as we discuss: what are data pipelines and how to leverage pipelines to quickly converge on a ML model.",,,Ignite Lighting Talk,,Introduction to MLOps and GenAI,"Data Pipelines, MLOps, Reproducibility","Bhavani Rao is a Technical Product Marketing Manager, responsible for product messaging and positioning at Pachyderm, a leader in data pipelining and MLOps. He has a diverse background, working with customers in Data Ops, DevOps, CI/CD, relational and NoSQL databases. A recent convert to the potential of AI/ML, Bhavani is passionate about technology and how it can be leveraged to solve customer problems. Throughout his career, Bhavani has promoted these learnings and best practices at numerous industry gatherings. He has a B.S. degree in Operations Research from Indiana University and an MBA from Columbia University.",https://www.youtube.com/watch?v=rbjIpJYUym8,,"The biggest challenges to creating real innovation and fresh competition is data and adhering to the privacy regulations that come with it. Synthetic data tackles both issues head on. By making it easy to create synthetic data, developers everywhere are free to build. Synthetic data is a complete game-changer for a world that thrives on data. 
",TMLS 2022,rbjIpJYUym8
"Narcisse Torshizi, Andres Villegas",Scotiabank,"Data Scientist/ Data Science Manager, Data Scientist Manager",AI for AI-Scotiabank's Award-Winning ML Models,"A brief overview of four innovative models that power and improve a chatbot solution 

Last year, Scotiabank was awarded the 2023 Digital Transformation Award by IT World Canada for our customer support chatbot. This achievement was made possible through the implementation of auxiliary AI models that helped the team develop the chatbot (""AI for AI""). These auxiliary models enabled the automation of the conversations review, supported NLU training, and allowed for scalability as the adoption of the chatbot increased. Besides, we have recently leveraged LLMs for summarizing chatbot interactions when a chatbot session is handed over to an agent (when the chatbot cannot fulfil the customer’s request). 

The chatbot solutions that we have developed and deployed is a result of combining various machine learning and statistical models. These models handle distinct aspects of natural language understanding, processing, and evaluation. Launching a new chatbot with no previous data puts immense pressure on the sustaining teams to detect, classify, and fix issues in the chatbot. In the absence of out of the box solutions the team came up with the concept of building auxiliary AI models to sustain the chatbot (AI for AI). We will describe the major features and achievements of four models that sustain our award-winning chatbot: Luigi, EVA, Peach and GenAI summarization. 

Luigi is a machine learning model that takes the confidence threshold of the chatbot's answers as either correct or incorrect. It uses a supervised learning approach to learn from the feedback of human reviewers and adjust the threshold accordingly. EVA is a machine learning classification model that processes customer inputs to predict their intent. It works in conjunction with Google Dialogflow. Peach is a natural language understanding model focused on similarity analysis. It supports AI trainers by evaluating whether training utterances positively influence the performance of the Dialogflow machine learning model. Finally, Our First GenAI feature helps summarization of the chat and capturing key details of each conversation, including account information and transaction specifics. This information is then sent to an agent, reducing the initial workload by an impressive 71%. On average, summaries are a mere 48 words, compared to the original 166-word conversations.

By utilizing these models, the team tapped into a database of curated data, reducing manual labor by thousands of hours in maintaining the organization's chatbot. This enabled the chatbot to rapidly enhance its performance after launch, resulting in improved call containment, customer satisfaction, and ultimately, recognition with the 2023 Digital Transformation Award. 
These models handle different aspects of natural language processing and evaluation and work together to provide a seamless and satisfying customer experience. ","Launching a new AI product with no previous data puts immense pressure on the sustaining teams to detect, classify, and fix issues in the model. In the absence of out of the box solutions the teams can came up with the concept of building auxiliary AI models to sustain the conversational AI product (AI for AI).",,Applied Case Studies,5.0,"Model dev, training, arch.","AI for AI, chatbot solutions, natural language processing.






","Narcisse Torshizi is an NLP and AI Data Scientist at Scotiabank, who has a PhD in Neurolinguistics. She has 10 years of experience in data and analytics and is specialized in the development of AI products and the related LLM trainings.

Andres: Trilingual (English, French, Spanish) Conversational AI Expert with over 7 years of experience in designing voice and chatbot interactions across various industries. With a Master’s degree in Engineering and a Professional Development Certificate in Data Science and Machine Learning, I have led multiple successful projects. Currently, I am part of the global Machine Learning and Artificial Intelligence group at Scotia Bank, where I have implemented the first customer-facing Gen AI feature and conducted extensive analytics to optimize chatbot performance. I am passionate about leveraging NLP, UX design, and automaton to drive digital transformation and enhance user interactions.  ",https://www.youtube.com/watch?v=Fhtig6TrZdk,"Banking & Financial Services, Computer Software, Information Technology & Service
",,TMLS 2024,Fhtig6TrZdk
Xin Liang,Canva,Senior Machine Learning Engineer,LLM evaluation to craft delightful content from messy inputs,"Large language models (LLMs) have been widely adopted in many applications across different business. Crafting delightful content from diverse and messy inputs is one of the use cases that can be enhanced by LLMs. However, due to the subjectivity and unstructured nature of the space, it appears to be challenging to objectively evaluate the quality of the LLM outcomes on specific tasks, such as providing detailed summaries on design inputs based on design types.
This talk explores an evaluation framework on the quality of LLM outputs for transforming diverse and messy textual inputs into delightful outputs with certain criteria. It not only considers the general LLM evaluation metrics such as relevance, fluency and coherence, but also specific metrics such as overall information preservation rate, accuracy of titles / headings understanding, and key information extraction score etc. This evaluation framework with measurable metrics can be generalised with similar LLM tasks.","It's important to define the quality criteria, ideally with measurable metrics, for the output expected from LLMs before configuring prompts for your LLM tasks.",-,,4.0,Performance optimization and efficiency,"LLM evaluation, Quality metrics for LLM outputs, Transforming messy inputs
","Xin is a Senior Machine Learning Engineer at Canva. Xin works with machine learning techniques including natural language processing, computer vision and MLOps. Driven by her deep passion for Artificial Intelligence,  she applies her engineering expertise and leadership skills to architect and deliver machine learning solutions. Additionally, she specialises in ML consultancy, system optimisation, and AI product development.",https://www.youtube.com/watch?v=pC-3VhW4RFE,Computer Software,,,pC-3VhW4RFE
Patrick Tammer,Scale AI,Senior Investment Director,Successfully integrating AI in your strategy and business operations – Lessons learnt from investing," Drawing from a portfolio of over 100 AI and big data projects, I aim to share actionable guidance on how businesses can harness AI to drive innovation, efficiency, and competitive advantage. Attendees will learn how to:
1. Navigate the AI Landscape: I will present findings from Scale AI’s flagship report ""The State of AI in Canada"" (https://www.scaleai.ca/aiatscale-2023/) to provide a comprehensive overview of how Canada compares globally in AI advancements.
2. Identify and Collaborate with Ecosystem Partners: I will provide strategies for identifying the right partners across academia, startups, and AI solution providers to foster innovation and growth.
3. Structure Successful AI Initiatives: Sharing lessons learned from Scale AI’s extensive project portfolio, I will outline how to effectively structure internal AI initiatives for maximum impact.
4. Develop AI Talent: Insights on crafting a forward-thinking AI talent strategy will be discussed, enabling organizations to build essential in-house capabilities.
5. Access Non-Dilutive Funding: Information on leveraging government non-dilutive funding to de-risk investments in AI technologies will be highlighted, offering a pathway to innovative project financing.

Additional notes

While our project portfolio is cross-industry, I am happy to tailor my presentation to specific industries of interest"," Why? What's the knowledge gap:
The session addresses the critical gap of integrating cutting-edge AI and big data technologies into mainstream business operations. It aims to equip leaders with the knowledge and tools necessary to navigate the complexities of AI adoption and to leverage these technologies for strategic advantage.

Learning Format and Audience Engagement Details:
The session is designed to be a concise, high-impact presentation lasting 15-30 minutes. It will include a combination of case study insights, strategic frameworks, and interactive Q&A, crafted to engage a diverse audience of C-suite executives, IT professionals, and strategic decision-makers.

Target Audience:
Tailored for senior decision-makers, this presentation will benefit those looking to effectively deploy AI and big data technologies to reshape their business landscapes. It promises valuable insights for anyone involved in technology strategy and implementation.",,Business Strategy or Ethics,2.0,Business and stakeholder alignment,"AI Integration, Ecosystem Partnerships, Structuring AI Initiatives
","Patrick Tammer is a Senior Investment Director and Policy Advisor to the Canadian Government at Scale AI, Canada's global AI innovation cluster. He currently manages a $125M portfolio of AI industry innovation projects. He is also an AI researcher at the Harvard Kennedy School. Prior to his current role, he spent 4 years as a strategy consultant with BCG. LinkedIn: https://www.linkedin.com/in/patricktammer/",https://www.youtube.com/watch?v=8gWE-YACCKo,"Other, Automotive, Banking & Financial Services, Computer Software, Food & Beverages, Hospital & Health Care, Environmental Services, Information Technology & Service, Insurance, Telecommunications
",,TMLS 2024,8gWE-YACCKo
Andrew Willson,Seldon,Head of Customer Success,Production ML Serving & Monitoring in Kubernetes,"This talk offers a practical guide to building a state-of-the-art MLOps deployment platform using Kubernetes, with a focus on deploying deep learning models. Attendees will gain insights into the integration of key technologies including NVIDIA Triton Inference Server, Seldon Core v2, Kafka, Prometheus, and Grafana. The session covers an end-to-end workflow for serving complex models like transformers and CNNs and configuring monitoring on top. The knowledge shared will be valuable for those looking to enhance model performance, reduce costs, and unlock new use cases in machine learning.","By attending this talk, the audience will learn how to set up a state-of-the art MLOps deployment platform on top of Kubernetes.  Once it is up and running, they will learn how to serve deep learning models in a robust and scalable way.  As inference is being performed, the audience will see how they can configure rich monitoring to observe their models in production.

Along the way, the audience will learn about each of the technologies being presented, including: Kubernetes, PyTorch, NVIDIA Triton Inference Server, Seldon Core v2, Kafka, Prometheus, and Grafana.  More importantly, they will see how these tools integrate with one another to form a fully fledged MLOps deployment platform.

MLOps deployment platforms are very challenging systems to build, deploy, and operate in the real-world.  As more and more business value is being driven by large, complex, deep learning (e.g. transformer, diffusion, CNN) models, it is crucial to be able to serve these in a scalable way.  This type of a platform creates business value by: (1) reducing inference costs through efficient serving, (2) improving performance through continuous monitoring and deployment, (3) enhancing the user experience with low latency, (4) shortening the time to bring use cases to market, and (5) eliminating the up-front development costs of building a deployment platform.

Aside from benefitting the business, engineers can benefit knowing that they are using best-in-breed tools, that they have a consistent way of deploying ML models, and that they have monitoring and logging built in.  Enabling engineers to build complex, data-centric pipelines opens up a new set of use cases, including advanced recommendation engines, explainable machine vision models, and even LLM question-and-answer applications.
",,Workshop,5.0,Deployment and integration,"Kubernetes, Deep Learning Models, MLOps Deployment
","Andrew is the Head of Customer Success at Seldon, enabling enterprise customers to get the most out of the Seldon ecosystem of tools. Andrew has spent the last 10 years in technical, client-facing roles with a focus on data, machine learning, and architecture. He loves transforming complex and ambiguous topics into clear actions that drive value within an organization. Outside of work, you might find Andrew riding his bike around the streets of London or snowboarding somewhere in The Alps.",https://www.youtube.com/watch?v=m1KI3rFqlPI,,It's an interesting personal problem I wanted to tackle and shows how to overcome lots of technical challenges the audience can relate to.,,m1KI3rFqlPI
Chinar Movsisyan,Feedback Intelligence,CEO,"Optimizing LLM apps through usage: Implicit feedback, given explicitly","In the rapidly evolving landscape of LLM-powered applications, AI teams face a unique challenge: gathering actionable insights from real-world usage to continuously improve app performance. This presentation will explore the common obstacles teams encounter when attempting to optimize their applications based on user interactions. We will dive into how implicit feedback—often hidden within everyday user behavior—can be harnessed effectively to drive measurable improvements. By sharing our approach to extracting and leveraging this data, we'll demonstrate how it accelerates the development of smarter, more responsive LLM applications.",,,Furture of AI,,Performance optimization and efficiency,"Implicit feedback, LLM optimization, User behavior insights","Chinar Movsisyan is the founder and CEO of Feedback Intelligence, an MLOps company based in San Francisco that enables enterprises to make sure that LLM-based products are reliable and that the output is aligned with end-user expectations. With over eight years of experience in deep learning, spanning from research labs to venture-backed startups, Chinar has led AI projects in mission-critical applications such as healthcare, drones, and satellites. ",https://www.youtube.com/watch?v=M7B02XA_COQ,,,MLOps & GenAI World 2024,M7B02XA_COQ
Yoshua Bengio,Mila / U. Montreal,Scientific Director / Full Professor,Large Neural Nets for Amortized Probabilistic Inference for Highly Multimodal Distributions and Model-Based ML,"Current large language models and other large-scale neural nets directly fit data, thus learning to imitate its distribution. Is it possible to do better? Consider the possibility, which we claim is actually the typical case, where a world model that captures causal structure in the world would require substantially fewer bits than the code that performs the kinds of inferences that we may desire from that knowledge. For example, the rules of the game of Go or the rules of logical reasoning are fairly compact, whereas inference (e.g., playing Go at Champion level, or being able to discover or prove theorems) may require a lot more computation (i.e., huge neural nets). In fact, exact inference for many problems (in science, computing, or in coming up with Bayesian posteriors) is often intractable and so we resort to approximate solutions. With amortized probabilistic inference, we expend computation upfront to train a neural net that learns to answer questions (for example sample latent variables or parameters) in a way that is as consistent as possible with the world model, making run-time inference quick. This also makes it possible to exploit the generalization ability of the approximate inference generative learner to guess where good answers (e.g., modes of a highly multimodal posterior distribution) might be. It also allows to decouple the capacity needed for inference from the capacity needed to describe the world model. This is unlike current state-of-the-art in deep learning, where inference and world model are confounded,  yielding overfitting of the world knowledge and underfitting of the inference machine. We have recently introduced a novel framework for achieving this kind of model-based ML, with generative flow networks (or GFlowNets), which have relations to reinforcement learning, variational inference and generative models. We'll highlight some of the advances achieved with GFlowNets and close with our research programme to exploit such probabilistic inference machinery to incorporate in ML inductive biases inspired by high-level human cognition and build AI systems that focus on understanding the world in a Bayesian and causal way and generating probabilistically truthful statements.",,,,7.0,"Model dev, training, arch.","Amortized Probabilistic Inference, GFlowNets, Model-Based Machine Learning





","Recognized worldwide as one of the leading experts in artificial intelligence, Yoshua Bengio is most known for his pioneering work in deep learning, earning him the 2018 A.M. Turing Award, “the Nobel Prize of Computing,” with Geoffrey Hinton and Yann LeCun.

He is a Full Professor at Université de Montréal, and the Founder and Scientific Director of Mila – Quebec AI Institute. He co-directs the CIFAR Learning in Machines & Brains program as Senior Fellow and acts as Scientific Director of IVADO.

In 2019, he was awarded the prestigious Killam Prize and in 2022, became the computer scientist with the highest h-index in the world. He is a Fellow of both the Royal Society of London and Canada, Knight of the Legion of Honor of France and Officer of the Order of Canada.

Concerned about the social impact of AI and the objective that AI benefits all, he actively contributed to the Montreal Declaration for the Responsible Development of Artificial Intelligence.",https://www.youtube.com/watch?v=OiMZI-xMjbU,AI Research,,,OiMZI-xMjbU
Zahra  Kharal,Venterra Realty,"ML Engineer, Data Scientist (Principal)",Leveraging ML in Multi-Family Real Estate Industry: From Predicting Rentals to Streamlining Operations ,"The multifamily real estate industry has witnessed an increased adoption of machine learning (ML) in recent years; allowing it to recover from its initial relatively slow adoption of ML technology and catch up to other industries in the field. The use of ML started to move into the multifamily real estate industry, offering new and innovative ways to analyze and operate properties, providing greater efficiency and optimization to investors, owners, and property managers. This presentation will showcase a collection of innovative ML case studies and application projects in multi-family real estate at Venterra Realty. 

One case study at Venterra where ML has shown its value was in predicting renter trends and tenure tendencies.  At Venterra, we started 20 years ago, capturing and structuring data, that we thought would eventually be useful for this purpose. By utilizing the 20 years of data that Venterra had, including 70+ features, Venterra built ML algorithms and pipelines that can more accurately help in identifying tenant behavior patterns, such as rent-paying habits and the likelihood of renewing a lease. This accumulative data was then used to make occupancy forecasts months in advance. The results of these models were used to understand tenant behavior better, improve retention rates and help planning and budgeting. An interesting question arose from this model: If we give the model results to the property managers, how will it influence their behavior in regard to tenant dealings.

Another interesting application of ML at Venterra was identifying maintenance needs and developing models for preventative maintenance. By analyzing data from millions of maintenance requests and several other sources, over many years, Venterra used NLP and classification techniques to predict request categories and thus assign these requests to the correct department in almost real time. Additionally, using deep-learning time-series models Venterra was able to predict when repairs will be necessary and schedule them proactively, while also predicting the budget required for these repairs. 

Through these case studies and application projects, attendees will gain an understanding of how ML is influencing operations at Venterra. They will see to succeed in ML endeavours, data strategies and proper data collection methodologies needs to be made decades in advance, and learn how pipelines need to be developed to successfully put the models in production. The benefits that Venterra has received since then will also be briefly discussed. Ultimately, this presentation will inspire traditionally non-tech industries to explore the potential of AI and ML in their own operations, and will prepare them for what’s needed in order to succeed. 

A bit about Venterra: Venterra specializes in the identification, development, finance, acquisition, and management of multi-family residential communities in the United States. We manage a portfolio of multi-family real estate assets totaling over $4.7 billion in value and have completed more than $8.7 billion of real estate transactions. Venterra has differentiated itself from its competitors by deploying industry leading technology to the benefit of both residents and investors. This technology includes a comprehensive suite of entirely web-enabled applications covering rent management, pricing optimization, purchase order management, utility billing, work order and capital expenditure management; an entire suite of back-end financial applications and customized workflows; and reporting and more recently the adoption of ML. 
","Through the case studies and application projects at Venterra Realty, attendees will gain an understanding of how ML is transforming the operations at Venterra and the multi-family real estate industry at large. They will learn specifically what data problems are encountered in the real estate domain, how the models were developed and what pipelines needed to be developed to successfully put the models in production. Issues encountered, unique to this industry, will also be discussed.",,Case Study,3.0,"Model dev, training, arch.","Machine Learning, Multi-Family Real Estate, Case Study





","Selected as a women in engineering rising star, Zahra has worked on solving problems related to data and modeling across industries, from engineering, sustainability and climate change, for University of Toronto and NRC, to more recently real estate. In her time at Venterra Realty, she has been responsible for implementing the Data Science and AI/ML projects; in just a few years, the Venterra AI/ML team has successfully developed various models, pipelines and in-house AI apps to help solve several business needs. From her research, she has published 12 journal papers and more than 20 conference papers. She is a member of the American Concrete Institute (ACI), American Society for Civil Engineering (ASCE), Canadian Association of Earthquake Engineering (CAEE) and Canadian Society for Civil Engineering (CSCE).",https://www.youtube.com/watch?v=qr0-RHG4U18,,"Real Estate multi-family industry has been behind in adopting ML, as the industry is traditionally tech slow. Moreover, for the most part, real estate companies use third parties to buy the software or technology. Venterra is unique in the sense of developing technology, applications and ML models in-house. Thus, the audience will get a unique glimpse into this industries data issues, its problems and how ML is being used to solve them. Additionally, rarely does real estate industry participate in technology summits or conferences such as this, so this will be an interesting glimpse in the world of proptech.
This presentation aims to inspire not only those data scientists and product engineers, but also real estate dabblers, property managers, investors, and other traditionally non-tech industries to explore the potential of AI and ML in their own operations. ",TMLS 2023,qr0-RHG4U18
Ashley Varghese,Canadian National Railways (CN),Data Scientist,Automated Inspection for rail cars using computer vision and machine learning,"CN is one of the organizations which realized the importance of AI very early and adopted some of the latest technologies in railroad operations. We at CN have been evaluating and adopting some of the latest approaches to carry out automated inspections of railcars that improve overall operational efficiency. The automated inspection of the railcars is achieved by leveraging computer vision and machine learning techniques. For automated inspection of railcars, the train passes through the portals which have equipped with two generations of cameras and are positioned to capture all sides of the railcar including the undercarriage. Once these images are captured, they are sent to the inference engine to detect the defects. However, developing machine learning pipelines and training robust models comes with its own challenges. In this talk, I would be covering how computer vision and machine learning techniques are used for developing some of the use cases, and various challenges associated with developing these machine learning pipelines. One of the key challenges is selecting the right data for training which must be representative of the actual data from the portal. Self-supervised learning-based techniques are adopted to identify the unique samples from the pool of unlabeled datasets. Another challenge associated with the model performance is image quality. The quality of the image captured at the portals is affected by different weather and lighting conditions. In addition to these, there are several other challenges such as subjectivity in defect classification, lack of samples for the defective class and imbalanced datasets, etc. I will pick one of the use cases we developed and then go over discussing some challenges faced and how we tackled these challenges to improve the performance of the use cases. By choosing the right data and computer vision approaches, it is possible to develop effective solutions for the automated inspection of cars. ","This talk provides insight into how computer vision and machine learning are applied in the railroad domain for the inspection of rail cars. It includes the challenges, strategies, and practical considerations that are associated with developing an ML-based automated inspection pipeline for a real-world, safety-critical application.",,Case Study,4.0,Deployment and integration,"Computer Vision, Machine Learning, Automated Inspection





","Ashley is a data scientist at Canadian National Railway. She works in the automated inspection program overseeing the development and retraining aspects for the inspection of rail cars. She has over 12 years of research experience in computer vision and deep learning. Her research papers were published in multiple international conferences and journals. She has previously worked as an AI Scientist with Qii.AI and as a researcher with TCS Innovation Lab. She holds an MTech in Computer Science from the International Institute of Information Technology, Bangalore.",https://www.youtube.com/watch?v=EzQLEea-4wU,Railroad,"This talk is based on my first-hand experience in developing and deploying ML pipelines for a real-world inspection program. This provides insight into real-world data, the challenges associated with training a robust model, and evaluate the performance of the model in production, and improve the model performance with retraining.
",TMLS 2023,EzQLEea-4wU
Niv Hertz,Solutions Architect,Aporia,Monitoring LLMs: Where to Begin? What's Important?,"Dive into the world of monitoring Large Language Models (LLMs). We'll cover key points like tracking drift and measuring model performance, and highlight useful tools and alert systems. This talk aims to give data scientists and ML engineers a basic but solid starting point on their monitoring journey to ensure high performance of their LLMs, contributing to the development of next-gen models.",,,Ignite Lighting Talk,,"Model dev, training, arch.","LLMs, Monitoring, Performance
","Niv Hertz is a Solutions Architect at Aporia, where he customizes ML monitoring to support any use case or need. Before joining Aporia, Niv worked as a Software Engineer and Cyber Security expert at other startups and in the elite 81 intelligence unit of the Israel Defense Forces. When he is not in front of a computer, Niv also enjoys hiking and basketball.",https://www.youtube.com/watch?v=Qje3tWudbMQ,,,TMLS 2023,Qje3tWudbMQ
Aarushi Kansal,AutoGPT,AI Engineer,From Paper to Production in 30 Minutes: Implementing code-less Gen AI Research,"There are new research papers in the Gen AI space, about prompting, RAG, different models, different ways to finetune almost every other day these days. Often they come with no code and in this talk we're going to go through and implement research papers in 30 minutes.",In this talk the audience will learn how to take a research paper and quickly implement (within 30 minutes) and then how to actually evaluate if it's useful for their work or not.,,case study & advanced technical,5.0,Future trends,"Gen AI, Research papers, Implementation"," Aarushi is a passionate and seasoned AI engineer, currently working at AutoGPT - one of the most popular projects on GitHubm aiming to democratize AI. Previously she has initiated and lead Generative AI at Bumble as a principal engineer. She has also been a software engineer at iconic companies such as ThoughtWorks, Deliveroo and Tier Mobility.",https://www.youtube.com/watch?v=oUl6NEzHMDY,Information Technology Services,"It's unique because there's not real many frameworks, tools or guides out their on how to implement research papers and for most people in industry, theory alone is not enough. ",MLOps & GenAI World 2024,oUl6NEzHMDY
Ankit Pat,Genesys,Lead Machine Learning Applied Scientist,"Exploring the Frontier of Graph Neural Networks: Key Concepts, Architectures, and Trends","In today's data-driven world, the relationships and connections within data are as crucial as the data itself. Graph Neural Networks (GNNs) have emerged as a groundbreaking technology that leverages these relationships to uncover insights and drive innovation across various domains, from social network analysis to drug discovery. This talk, will delve into the fundamentals of GNNs, exploring their unique ability and versatility to model complex data structures through graph representations. We will discuss the core principles, architectures, and applications of GNNs, providing a comprehensive overview of how they can transform your approach to data analysis and problem-solving. Whether you're a data scientist, researcher, or industry professional, this talk will equip you with the knowledge to harness the full potential of GNNs in your work.","- Basics of Graph Structures and Graph Theory
- Fundamental Concepts, Key Components, and Architecture of GNNs
- Real-World Applications of GNNs Across Various Domains
- Advanced GNN Techniques: Including Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and more.
- Emerging Trends and Future Directions in Graph Neural Networks",,Research or Advanced Technical,5.0,"Model dev, training, arch.","Graph Neural Networks, Key Concepts, Architectures
","Ankit Pat is a Lead Machine Learning Applied Scientist at Genesys, specializing in leading and contributing to applied Machine Learning research with a strong emphasis on product-centric approaches. He has over 9 years of industry experience and 4 years of academic research in Machine Learning (ML) and Artificial Intelligence (AI). He holds a Master's degree in Computer Science, specializing in AI, from the University of Waterloo, and both a Bachelor's and Master's in Mathematics and Computing from the Indian Institute of Technology, Kharagpur.

Ankit has authored over 10 patents and published 4 research papers at leading international conferences, including AAAI.",https://www.youtube.com/watch?v=ieY2dpQJA0U,"Computer Software, Information Technology & Service, Other
",,TMLS 2024,ieY2dpQJA0U
Michael Havey,Amazon Web Services,Senior Solutions Architect,Ask the Graph: How Knowledge Graphs Helps Generative AI Models Answer Questions," Generative AI has taken the world by storm. The Retrieval Augmented Generation (RAG) pattern has emerged as an effective way to incorporate your organization's data to provide current, accurate answers to questions that users ask a Large Language Model (LLM) Knowledge Graphs make RAG even more accurate and helpful. The secret sauce: relationships! I describe what a Knowledge Graph is, why it has long been a great database to answer questions, and how it can help an LLM using a pattern called Graph RAG. I present examples of Graph RAG in action from industries such as finance and healthcare."," You will learn how a Large Language Model (LLM) benefits from Retrieval Augmented Generation (RAG) to provide current, accurate answers to user questions grounded in your organization's data. A graph database helps make RAG more accurate because it maintains relationships between business objects. You will learn what a Knowledge Graph is and how to build Graph RAG on a Knowledge Graph. As a takeway, you will see the business benefit of Graph RAG, the value of more accurate, helpful answers!",,Applied Case Studies,3.0,Deployment and integration,"Knowledge Graphs, Retrieval Augmented Generation (RAG), Generative AI",Mike Havey is a Senior Solutions Architect for AWS with over 25 years of experience building enterprise applications. Mike is the author of two books and numerous articles.,https://www.youtube.com/watch?v=cdqIHS29HLA,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Marketing & Advertising, Telecommunications
",,TMLS 2024,cdqIHS29HLA
Aayush Mudgal,Pinterest,"Senior Machine Learning Engineer,",Evolution of ML Training and Serving Infrastructure at Pinterest,"Join us for an insightful talk as we delve into the fascinating evolution of training and serving infrastructure at Pinterest Ads over the past 5+ years. Witness the remarkable progression from logistic regression-based models to the cutting-edge implementation of large transformer-based models, efficiently served using GPU technology. Throughout this transformative journey, we encountered numerous challenges and invaluable lessons that have shaped the very core of this critical business undertaking. Prepare to be inspired by our experiences as we share the triumphs and tribulations that ultimately led to a revolution in Pinterest Ads' capabilities.","1. How to best structure training and serving infrastruture. 
2. How to balance infrastructure costs and performance
3. Learn from real industrial system serving users at scale and the design choices that were made.",,Case Study,5.0,"Model dev, training, arch.","Training Infrastructure, Serving Infrastructure, Transformer Models","Aayush Mudgal is a Senior Machine Learning Engineer at Pinterest, currently leading the efforts around Privacy Aware Conversion Modeling. He has a successful track record of starting and executing 0 to 1 projects, including conversion optimization, video ads ranking, landing page optimization, and evolving the ads ranking from GBDT to DNN stack. His expertise is in large-scale recommendation systems, personalization, and ads marketplaces. Before entering the industry, Aayush conducted research on intelligent tutoring systems, developing data-driven feedback to aid students in learning computer programming. He holds a Master's in Computer Science from Columbia University and a Bachelor of Technology in Computer Science from Indian Institute of Technology Kanpur.
",https://www.youtube.com/watch?v=5kbAIU4Xgpo,"Computer Software, Information Technology & Service
",Learnings from the evolution and deep dive into lesosns and challenges which are not found online easily.,,5kbAIU4Xgpo
"Shiming Ren, Chen Liu",Amazon/Twitch,"Sr. Engineering Manager - Safety, MLOps and Infrastructure, Twitch Sr. Engineering Manager on Personalization and ML Infra",From silo to collaboration - building tooling to support distributed ML teams at Twitch,"[High level intro]

In this talk, we will cover Twitch’s current ML team structure and challenges of it. Then we dive deep into some solutions we have built to support ML development at Twitch, including what they are and how they will benefit the situation. We close with a discussion of Twitch’s distributed ML team style and how we collaborate using Conductor as an example.

[Actual abstract]

ML has been playing a more and more important role in Twitch’s products (e.g. Recommendation, Safety). In order  to allow products to iterate fast, we keep ML practitioners in the product teams and empower the teams to work independently. Undoubtedly, there are common challenges in ML development regardless of product areas. So we are striving to develop tooling and infrastructures for general ML development in order to reduce duplicate work across ML teams. We will dive into those efforts we made in this presentation. For example, Twitch machine learning feature store is developed to have a single control plane serving as feature registry but facilitates distributed feature ownership (e.g. storage, pipelines). Conductor, a in-house ML orchestration system, promotes best practices in pipeline management with templated process control flow and distributed infrastructure management. Meanwhile, we are promoting collaborative ML culture among Twitch engineering teams. It is similar to community owned open source projects where teams share the same interests and encourage cross team contribution and development. ",Twitch's strategy of scaling our ML infra and MLOps tooling has never been discussed online. And we aim to help audience figure out the best strategy to utilize ML tooling for enhancing collaborations between ML teams and boost scientists self-service / efficiency. This is a good lesson if companies are seeking to start MLOps from stratch.,,Case Study,4.0,Introduction to MLOps and GenAI,,"I worked as a Software Engineer Manager at Twitch about MLOps and Tooling in Safety team. Here is my linkedin https://www.linkedin.com/in/shimingr/ I spoke at Meta's At Scale about Scaling ML Workflows for Real-Time Moderation Challenges at Twitch, I also spoke at TwitchCon about Integrating Data into Twitch at Scale. I worked in engineering leadership role for 5 years and our team made several company wide MLOps tooling such as orchstration and feature store.

Chen:
Chen is currently supporting teams working on personalization and ML infrastructures at Twitch. He is passionate about building scalable ML products and democratizing ML in the organization.",https://www.youtube.com/watch?v=72UBwLb7fEQ,,"We aim to use examples how Twitch build in house feature store, realtime inference and orchstration system to demonstrate from technology perspective about MLOps collaborations in a company. This is more like a hybrid tech and management talk which will benefit both engineer and leadership groups.
",TMLS 2022,72UBwLb7fEQ
"Bharat Venkitesh is a senior machine learning (ML) engineer at Cohere, focused on model compression, model efficiency, and inference optimizations. Previously, Bharat was a ML research engineer at Huawei, Noah’s Ark Lab, Montreal, where he worked on scaling up model training and model compression for natural language processing and speech recognition applications on edge devices. He has a master’s degree from University of Waterloo and a bachelor of technology degree from Indian Insitute of Technology Hyderabad. Venkitesh",Cohere,Senior Machine Learning Engineer,Efficient Inference of Extremely Large Transformer Models,"The rise of transformer-based language models has seen a boom in model sizes, since these models’ performance scales extremely well with size. With this comes the challenge to develop solutions to make inference on these models more efficient. We'll show how these behemoth multi-billion-parameter models are optimized for production and how the inference tech stack is established. We'll cover the key ingredients in making these models faster, smaller, and more cost-effective, including model compression, efficient attention, and optimal model parallelism.",,,Technical / Research,5.0,Performance optimization and efficiency,"Transformer Models, Efficient Inference, Model Compression","Bharat Venkitesh is a senior machine learning (ML) engineer at Cohere, focused on model compression, model efficiency, and inference optimizations. Previously, Bharat was a ML research engineer at Huawei, Noah’s Ark Lab, Montreal, where he worked on scaling up model training and model compression for natural language processing and speech recognition applications on edge devices. He has a master’s degree from University of Waterloo and a bachelor of technology degree from Indian Insitute of Technology Hyderabad.",https://www.youtube.com/watch?v=mzMiKydilZQ,"Information Technology & Service
",,TMLS 2023,mzMiKydilZQ
Savin Goyal,Outerbounds,Co-founder & CTO,Fast and Reproducible: Taming AI/ML dependencies," Careful management of software dependencies is one of the most underrated parts of ML and AI systems despite being critically important for the stability of production deployments as well as for the speed of development. For the past many years, we have worked with the wider Python package management community (pip, conda, rattler, uv, and many more) and multiple organizations (Netflix, Amazon, Goldman Sachs, and many more) to advance the state of the art in dependency management for ML/AI platforms, including our open-source framework Metaflow.

In this talk, we'll explore common pitfalls in dependency management and their impact on ML projects, from unexpected results due to package changes to the challenges of reproducing environments across different machines. We'll cover issues ranging from the complexities of scaling dependencies in distributed cloud environments to performance regressions from seemingly innocuous updates, highlighting why robust dependency management is crucial for production ML systems. 

We'll share our learnings and demonstrate how we address key challenges in building robust and maintainable ML systems, such as: 
Creating fast, stable, and reproducible environments for quick experimentation
Scaling cross-platform execution to the cloud with automatic dependency handling
Auditing the full software supply chain for security and compliance

We’ll also demo some of our recent work which enables baking very large container images in just a few seconds, significantly accelerating the prototyping and experimentation cycle for ML practitioners."," This talk explores the critical yet often overlooked role of software dependency management in ML and AI systems. Drawing from years of collaboration with the Python package management community and major organizations, the speakers will share insights on common pitfalls in dependency management and their impact on ML projects and recent innovations in rapid container image creation for accelerated ML experimentation.",,Advanced Technical/Research,3.0,Performance optimization and efficiency,"Dependency management, Metaflow, Reproducibility","Savin is the co-founder and CTO of Outerbounds - where his team is building the modern ML stack to accelerate the impact of data science. Previously, he was at Netflix, where he built and open-sourced Metaflow, a full stack framework for data science.",https://www.youtube.com/watch?v=GOHfDHe9dCQ,"Telecommunications, Marketing & Advertising, Insurance, Information Technology Services, Food & Beverage, Hospital & Health Care, Enviromental Services, Computer Software, Banking & Financial Services, Automotive, Other",This talk draws from real-world enterprise use cases over multiple years on the importance of a principled approach to dependency management. This talk will get into details of what works and what doesn't work - a rare view into the guts of how leading organisations manage some of the most overlooked yet fundamentally important parts of the AI/ML stack,MLOps & GenAI World 2024,GOHfDHe9dCQ
Laila Paszti,Kirkland & Ellis LLP,Partner,"Goose, Chewbacca, and AI: Safely flying the generative AI coding skies",Goose and Chewbacca – two trusty co-pilots supporting their starship captains. Generative AI coding tools promise much to support developers navigating through the complexity of code development while spurring innovation. But can you trust your generative coding co-pilot? Does your organization? Let’s discuss the risks arounds generative AI coding tools and how you and your organizations can safely manage and minimize risk.,"Generative AI coding tools can be leveraged safely within an organization if there is a practical and appropriate set of guardrails in place. This includes a mix of technical, operational and administrative safeguards (e.g. usage policies, auditing tools, optimal selection of generative AI coding tools)",,Business Strategy,4.0,Security and Privacy,"Generative AI coding tools, risk management, safeguards
","A machine learning engineer turned lawyer, Laila Paszti is a technology & IP transactions partner in the Bay Area office of Kirkland & Ellis LLP. Laila leverages her significant experience in technology, intellectual property and privacy to help clients navigate risk and protect their commercial interests, including when using and developing AI systems.

Laila’s practice focuses on technology and related aspects of mergers and acquisitions, corporate finance, licensing and other commercial transactions. She has acted on both sides of multinational mergers and acquisitions, patent purchases and sales, and licensing and IP transactions. Laila routinely advises clients on evolving multijurisdictional privacy regimes, cybersecurity, big data platform diligence and IP matters. She is sought out for her guidance on artificial intelligence and commercialization, including on patent strategy.
Before law school, she held positions as a software applications engineer at ExxonMobil and Capstone Technology where she designed and implemented neural-network based control systems to optimize industrial processes. Laila holds a B.A.Sc. in Chemical Engineering from the University of Toronto, a M.A.Sc. in Chemical Engineering from the University of Waterloo and a J.D. from the University of Toronto, where she was a law review editor. She is admitted to practice law in California, New York, and Ontario (Canada).  Laila is a Certified Information Privacy Professional (Europe and Canada) (CIPP/E and CIPP/C).
",https://www.youtube.com/watch?v=QrprmA94UgY,"Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology & Service, Marketing & Advertising, Telecommunications
",,MLOps & GenAI World 2023,QrprmA94UgY
Sarah Sun,Scotiabank,Director Data Science,Model Lifecycle in Banking,"Everything you ever wanted to know about model building at a bank, from conception to implementation, and then some!!",,,Ignite Lighting Talk,,Introduction to MLOps and GenAI,"A decade in data has taken Sarah across multiple industries, including banking, technology, and natural resources. While specializing in data strategy, she was trained as a data scientist and has worked across the industry in innovation, governance, AI, and also a stint as CEO of a startup. Working in data has taught Sarah some value lessons - everything from seizing opportunities, the important of mental health, and the power of sharing stories. Sarah was named one of the Women Executive Network's Top 100 Most Powerful Women in 2019 .",,https://www.youtube.com/watch?v=zBpwEae2Xmg,,,TMLS 2022,zBpwEae2Xmg
Nikunj  Bajaj,TrueFoundry,CEO & Cofounder,LLM economics : The Cost of leveraging Large Language Models,"Most of us are using LLMs and some of us are getting to the point where LLMs are going to production. Honeymoon phase is going to get over soon and practical realities like cost & maintainability are going to become mainstream.
However, the cost of running LLMs is not well understood or often not put in perspective. In this talk we will dive deep into what type of costs are involved in building LLM based apps. How do these compare when you run RAG vs Fine tuning, what happens when you use Open Source vs Commercial LLMs?
Spoiler- If you wanted to summarize the entire Wikipedia to half its size using GPT-4 8k context window, it would cost a whopping $360K!","The highlighted takeaways from our session provide valuable insights specific to LLM pricing. By understanding the economics of different models, businesses and LLM users can make informed decisions tailored to their needs. Comparing costs across models, such as GPT-4, Anthropic Claude V1, InstructGPT-DaVinci, Curie, and self-hosted 7B models, reveals the significant variations in pricing. This knowledge enables organizations to evaluate the trade-off between cost and performance effectively, ensuring optimal resource allocation.

Additionally, we'll provide the breakdown of pricing levers, including input and output costs based on the number of tokens, which allows businesses or individuals to analyze their usage patterns and manage expenses efficiently. Understanding the cost implications of fine-tuning models empowers organizations to make strategic decisions based on their specific requirements and budget.

The introduction of TrueFoundry's solutions, such as the compression API for reducing OpenAI costs and simplified deployment of open-source LLMs, offers tangible strategies for optimizing expenses. These unique approaches provide actionable steps that businesses can implement to reduce LLM-related costs while maintaining desired performance levels.
",-,,3.0,Performance optimization and efficiency,"LLM economics, Cost of LLMs, Fine-tuning vs. RAG
","Nikunj is the co-founder and CEO of TrueFoundry, a platform empowering ML developers to deploy and optimize Language Models. Prior to this role, he served as a Tech Lead for Conversational AI at Meta, where he spearheaded the development of proactive virtual assistants. His team also put Meta's first deep learning model on-device. Nikunj also led the Machine Learning team at Reflektion, where he built an AI platform to enhance search and recommendations for over 600 million users across numerous eCommerce websites. Fun Fact about Nikunj? He learnt scuba diving and swimming in parallel. His instructor laughed at him saying- ""You don't know how to swim? And you thought it would be just fine to jump in the middle of Pacific with 70 lb gear on your back""",https://www.youtube.com/watch?v=TBGRFoNOlrs,"Automotive, Banking & Financial Services, Computer Software, Hospital & Health Care, Information Technology & Service, Insurance, Marketing & Advertising, GenAI powered companies
","Our session stands out in the MLops world because we provide exclusive insights into the often misunderstood economics of Large Language Models (LLMs) pricing. While there is ample information available online about LLMs and their performances, our session focuses solely on the math-intensive aspect of understanding LLM pricing. We delve into the cost analysis of running popular LLMs, comparing their pricing for a specific task of summarizing Wikipedia. Moreover, we offer valuable knowledge on the levers of pricing in OpenAI and 3rd-party APIs, as well as the costs associated with self-hosted models and fine-tuning. Additionally, we introduce TrueFoundry's innovative solutions, such as a compression API for reducing OpenAI costs and simplified deployment of open-source LLMs through our Model Catalogue and Drop-in APIs. By attending our session, participants gain unique and actionable insights that cannot be easily found online.
",MLOps & GenAI World 2023,TBGRFoNOlrs
Marija Stanojevic,EudAImonia Science,Lead Applied Machine Learning Scientist," Machine Unlearning: Addressing Bias, Privacy, and Regulation in LLMs and Multimodal Models"," This talk discusses machine unlearning for large language models (LLMs) and multimodal models (MMs) handling sensitive data. As these AI models gain traction, ensuring adaptable and ethical practices is paramount, especially in domains handling healthcare, finance, and personal information. Here, we explore the intricacies of machine unlearning dynamics and their impact on bias mitigation, data privacy, legal compliance, and model robustness.

The talk sheds light on recent advancements and seminal research in machine unlearning. Given the growing prevalence of AI regulations and concerns around test data leaks during massive training, machine unlearning emerges as an essential component for ensuring unbiased, compliant, and well-evaluated AI systems. We discuss techniques for identifying unwanted data within models and for removing it while preserving model performance. Additionally, the talk explores methods for evaluating the success of machine unlearning, guaranteeing that the model forgets the targeted data without compromising its overall behavior and performance on other data.

Machine unlearning empowers stakeholders, including customers and data owners, with the ability to withdraw their data and fosters trust in the responsible development and deployment of LLMs and MM models."," * The importance of machine unlearning in responsible AI: You'll be able to explain why machine unlearning is crucial for ensuring ethical and adaptable AI practices, particularly for models handling sensitive data.
* The impact of machine unlearning on key aspects of AI development: The talk will investigate how machine unlearning can mitigate bias, enhance data privacy, ensure legal compliance, and improve model robustness.
* Recent advancements and research in machine unlearning: You'll understand the latest developments and significant research findings in the field of machine unlearning.
* Techniques for identifying and removing data from models: The talk will explore practical methods for determining if specific data resides within a model and how to remove it while maintaining the model's performance. 
* Evaluating the success of machine unlearning: You'll learn techniques to assess whether the machine unlearning process has been successful, ensuring the model forgets the targeted data without impacting its overall functionality.",,Research or Advanced Technical,4.0,"Ethics, governance compliance ","Machine Unlearning, Bias Mitigation, Data Privacy.
","Marija Stanojevic16, Ph.D. is a Lead Applied Machine Learning Scientist at EudAImonia Science and Ellipsis Health. She focuses on representation learning, multimodal, multilingual, and transfer learning for healthcare. She was a virtual chair of ICLR 2021 and ICML 2021, general chair of the Machine Learning for Cognitive and Mental Health workshop at AAAI 2024, and main organizer of the 9th Mid-Atlantic Student Colloquium on Speech, Language, and Learning
(MASC-SLL 2022). General chair. She worked at Meta, Cambridge Cognition, Winterlight Labs, and LinkedIn. ",https://www.youtube.com/watch?v=qrJGtMjA8PU,"Banking & Financial Services, Hospital & Health Care, Insurance, Marketing & Advertising, Telecommunications
",,TMLS 2024,qrJGtMjA8PU
Hamza Farooq,Traversaal.ai,CEO & Founder,LLMs for Leaders & Senior Product Managers,Become a leader in Gen AI Transformation. Learn from real-world case studies on how you can drive innovation within your company through LLM," In this workshop, participants will get a deep understanding of what Gen AI can do for them. We will do small exercises which provide with the knowledge and skills to leverage Large Language Models (LLMs) to build an AI strategy for your larger organization.

The course will take you through the entire process of:

💡 How to identify GenAI opportunities, within existing or brand-new products

🧪 Testing your idea through user research, quickly and without wasting a ton of $$

🏗️ Building your MVP, just enough so that you can get early traction

💰 Selling your idea, across customers, VCs, and internal stakeholders"," None, just basic knowledge of Chat GPT",Workshop,1.0,Introduction to MLOps and GenAI,"Large Language Models (LLMs), AI strategy, Gen AI transformation.
",,https://www.youtube.com/watch?v=RJawFx4kSMM,"Banking & Financial Services, Computer Software, Food & Beverages, Information Technology & Service, Marketing & Advertising
",,TMLS 2024,RJawFx4kSMM
Marinela Profi,SAS,Global AI & GenAI Lead,LLMs Alone Do Not Solve Business Problems,"The rapid rise of generative AI in 2023 sparked widespread experimentation, with companies across industries eager to leverage its potential for content generation, task automation, and customer experience transformation. However, not all organizations have found success, and by 2025, a clear divide will emerge between those excelling with AI-driven innovation and those struggling to keep up. In this session we will explore the key principles that are differentiating organizations who are winning the generative AI wave and successful use cases. "," Join me to learn the reasons behind this divide, mistakes to avoid and key factors and use cases driving success for companies leveling up with AI, so you can win too.
",,Business Strategy,4.0,Business and stakeholder alignment,"Generative AI, Business success, Use cases","Marinela Profi is the Global AI & GenAI Lead at SAS. Leveraging her extensive background in data science, Marinela brings a unique perspective that bridges the realms of technology and marketing. She drives AI implementation within Banking, Manufacturing, Insurance, Government and Energy sectors. Marinela has a Bachelor’s in Econometrics, a Master of Science in Statistics and Machine Learning and Master’s in Business Administration (MBA). She enjoys sharing her journey on LinkedIn, and on the main stage, to help those interested in a career in data and tech.",https://www.youtube.com/watch?v=OK-7qZpEADc,Computer Software,"SAS serves 90% of Fortune 100 companies (or their affiliates) and is the leader in analytics software and services, and the largest independent vendor in the business intelligence market. We can speak to cross-industry solutions and how we're helping Fortune 100 companies win in AI.",MLOps & GenAI World 2024,OK-7qZpEADc
"Gayathri Srinivasan, Abhimanyu Anand",Wattpad,"Senior AI/ML Product Manager, Data Scientist",Optimizing Recommendations on Wattpad Home,"At Wattpad, the world's leading online storytelling platform, recommendation systems are pivotal to our mission of connecting readers with the stories they love. The Home Page is the primary gateway to Wattpad's diverse content and experiences. As the platform has evolved, we've introduced new content types and classes of stories to meet various business objectives, such as user engagement, merchandising, and marketing. This evolution necessitated recalibrating our homepage recommender system to effectively balance multiple business goals. In this talk, we will discuss how we integrated these objectives into the home recommender stack using probabilistic algorithms derived from the domain of reinforcement learning. Additionally, we will share the challenges we encountered during this transition, such as data sparsity and the cold start problem, along with insights into our development of novel graph neural network architectures tailored for recommendation systems and the new datasets we developed to overcome these hurdles."," The audience will learn about: 
1. How Recommendation systems are used at scale for content recommendation. 
2. Challenges associated with recommendation systems like data sparsity, balancing multiple objectives, etc.. 
3. How we solved these problems at Wattpad using novel graph-based models, multi-objective ranker, etc..",,Applied Case Studies,4.0,"Model dev, training, arch.","Recommendation Systems, Reinforcement Learning, Graph Neural Networks
","Gayathri: Gayathri Srinivasan is an accomplished AI product manager at Wattpad, specializing in personalized rankings and recommendations. With over 7 years of diverse product management experience across various industries, including startups, scale-ups, and enterprises, she brings a wealth of knowledge and expertise to her role.

Abhimanyu: Abhimanyu is a Data Scientist at Wattpad, an online social storytelling platform, where he leads the development of recommender systems for content recommendations. He holds an M.Sc. in Big Data Analytics from Trent University, with a specialization in natural language processing. He has developed and implemented robust AI solutions throughout his career across diverse domains, including internet-scale platforms, metals and mining, oil and gas, and e-commerce.",https://www.youtube.com/watch?v=KckAQQWTMRI,,,TMLS 2024,KckAQQWTMRI
Niels Bantilan,Union.ai,Chief ML Engineer,Learn Your Codebase: Fine-tuning CodeLlama with Flyte… to Learn Flyte,"Today, foundation LLMs can only be trained by a handful of organizations possessing the compute resources required to pre-train models with more than a hundred billion parameters over internet-scale data. These foundation models are then fine-tuned by the wider ML community for specific applications. Even though fine-tuning can be more compute and memory efficient than full parameter tuning, a significant challenge to fine-tuning is provisioning the appropriate infrastructure.

In this session, Niels will demonstrate how to use Flyte, a Linux Foundation open-source orchestration platform to fine-tune a LLM on the Flyte codebase itself 🤯. Flyte allows for the declarative specification of the infrastructure needed for a broad range of ML workloads, including fine-tuning LLMs with limited resources by leveraging multi-node, multi-gpu distributed training.","Attendees will gain hands-on experience using Flyte to leverage state-of-the-art deep learning tools such as `torchrun` distributed training, LoRA, 4/8-bit quantization, and FSDP, while benefiting from Flyte's reproducibility, versioning, and cost management capabilities. At the end of this talk, you’ll be able to take the code and adapt it to learn your own code base to help to answer user-support questions, create boilerplate starter code, or whatever downstream task you’re interested in!","Intermediate Python
Intermediate Machine Learning
Familiarity with Command-line Tools",Workshop,5.0,"Model dev, training, arch.","Flyte, Fine-tuning, LLMs
","Niels is the Chief Machine Learning Engineer at Union.ai, and core maintainer of Flyte, an open source workflow orchestration tool, author of UnionML, an MLOps framework for machine learning microservices, and creator of Pandera, a statistical typing and data testing tool for scientific data containers. His mission is to help data science and machine learning practitioners be more productive.

He has a Masters in Public Health with a specialization in sociomedical science and public health informatics, and prior to that a background in developmental biology and immunology. His research interests include reinforcement learning, AutoML, creative machine learning, and fairness, accountability, and transparency in automated systems.
",https://www.youtube.com/watch?v=VjIVPmow31A,,"Everything can be found online, I wouldn't suggest that any presentation I could write wouldn't be something someone can learn via google or chatGPT :)
",MLOps & GenAI World 2023,VjIVPmow31A
Dre Olgiati,LinkedIn,"Distinguished Engineer, AI/ML",Large Language Model Training and Serving at LinkedIn,"In this talk, Dre will describe some of the fundamental challenges and solutions faced by the LinkedIn team as they build innovative products based on LLMs and agents.",How do I build scalable training and serving solutions for large language models (LLMs)? What are the challenges in scaling LLM training and serving?,,case study & advanced technical,4.0,Deployment and integration,"LLM training and serving, Scalable solutions, LinkedIn AI products"," Dre is a Distinguished Engineer at LinkedIn, where he leads wide-ranging initiatives relevant to large model training, serving, MLOps and more.",https://www.youtube.com/watch?v=yx_BKcAPoQs,Computer Software,"The application of LLMs to complex topics such as text completion, ranking, etc",MLOps & GenAI World 2024,yx_BKcAPoQs
Alik Sokolov,Responsibli,Co-Founder and CEO,The Natural Language Processing Revolution in Investment Management: Advice and Responsible Investing,"This talk will discuss the revolution of natural language processing in recent years, and how it applies to two diverse areas of investment management: investment advice, and the evolving landscape of responsible investing. Our ability to work with unstructured text data, which is abundant in investment management, has undergone several evolutions from the late 2010's: from sequence-to-sequence models for machine translation, to the advent of transformers and transfer learning, to the recent breakthroughs achieved by Large Language Models like Chat GPT. These changes will have a profound impact on investment management, and we will examine two case studies in investment management and responsible investing. We will also examine their long-term future, and how the technical approaches to both are likely to intersect.
","A unique, hands-on industry perspective on the rapidly evolving world of NLP / LLM's, presented in an executive-accessible format",,Business/Strategy,2.0,Business and stakeholder alignment,"Natural Language Processing, Investment Management, Large Language Models





","Alik’s professional background is in AI consulting as a machine learning and data science team leader, and venture capital as a research associate in one of Peter Thiel's funds. Alik is also a seasoned AI project leader and educator in the machine learning field, having taught and developed the machine learning course at the University of Toronto Master's of Mathematical Finance program, as well as many workshops and classes around the world. Alik is also a PhD candidate and Vanier Scholar at the University of Toronto, studying applications of machine learning in quantitative finance and he has several publications at the intersection of quantitative finance, AI, and responsible investing.",https://www.youtube.com/watch?v=NPlI_znQEHk,"Banking & Financial Services
","I have a very unique mix of skills at the industry of machine learning, finance and academia. I have my CFA, work with some very prominent asset managers to help them integrate AI into their day-to-day process today, as well as do research on these topics as a University of Toronto graduate researcher.
",TMLS 2023,NPlI_znQEHk
"Greg Loughnane, Chris Alexiuk",AI Makerspace,"Co-Founder, Co-Founder & CTO",Building Agentic and Multi-Agent Systems with LangGraph,"2024 is the year of agents, agentic RAG, and multi-agent systems! 

This year, people and companies aim to build more complex LLM applications and models; namely, ones that are ever-more capable of leveraging context and reasoning.  For applications to leverage context well, they must provide useful input to the context window (e.g., [in-context learning](https://openai.com/index/language-models-are-few-shot-learners/)), through direct prompting or search and retrieval (e.g., [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401), or RAG).  To leverage reasoning is to leverage the Reasoning-Action ([ReAct](https://arxiv.org/abs/2210.03629)) pattern, and to be “agentic” or “agent-like.”  Another way to think about agents is that they enhance search and retrieval through the intelligent use of tools or services. 

The best practice tool in the industry for building complex LLM applications is LangChain.  To build agents as part of the LangChain framework, we leverage LangGraph, which allows us to bake in cyclical reasoning loops to our application logic.  LangChain v0.2, the latest version of the leading [infrastructure orchestration tooling](https://github.com/a16z-infra/llm-app-stack?tab=readme-ov-file#orchestrators), incorporates LangGraph directly, the engine that powers stateful (and even fully autonomous) agent cycles.

In this session, we'll break down all the concepts and code you need to understand and build the industry-standard agentic and multi-agent systems, from soup to nuts."," - A review of the basic prototyping patterns of GenAI, including Prompt Engineering, RAG, Fine-Tuning, and Agents
- The core ideas and constructs to build agentic and multi-agent applications with LangGraph
- ⛓️ Build custom agent applications with LangGraph
- 🤖 Develop multi-agent workflows with LangGraph",,Workshop,4.0,"Model dev, training, arch.","LangGraph, Agentic systems, Multi-agent workflow
","[Dr. Greg Loughnane](https://www.linkedin.com/in/gregloughnane/) is the Co-Founder & CEO of AI Makerspace, where he is an instructor for their [AI Engineering Bootcamp](https://maven.com/aimakerspace/ai-eng-bootcamp). Since 2021 he has built and led industry-leading Machine Learning education programs.  Previously, he worked as an AI product manager, a university professor teaching AI, an AI consultant and startup advisor, and an ML researcher.  He loves trail running and is based in Dayton, Ohio.

[Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/) is the Co-Founder & CTO at AI Makerspace, where he is an instructor for their [AI Engineering Bootcamp](https://maven.com/aimakerspace/ai-eng-bootcamp). Previously, he was a Founding Machine Learning Engineer, Data Scientist, and ML curriculum developer and instructor. He’s a YouTube content creator YouTube who’s motto is “Build, build, build!” He loves Dungeons & Dragons and is based in Toronto, Canada.",https://www.youtube.com/watch?v=5qsEIdUlBLY,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Insurance, Information Technology Services, Marketing & Advertising, Telecommunications, Other","In just three hours, you can go from not knowing agents, LangChain, or LangGraph, to building robust, production-grade multi-agent applications with industry-leading open-source tooling.  The only thing you'll have to do is figure out what to build, and why... ",MLOps & GenAI World 2024,5qsEIdUlBLY
Geoffrey  Hunter,SpotHero,Lead Data Scientist,7 Questions for Data Scientists,Geoffrey will share the 7 Questions I ask on a daily basis to rapidly qualify and frame new Data Science problems. This framework can be applied to understand new opportunities as well as to existing problems to help eliminate noise and focus one's efforts.,,,Ignite Lighting Talk,,Business and stakeholder alignment,"Data Science Framework, Problem Qualification, Opportunity Identification","Geoffrey is passionate about forming end-to-end, product-focused Data Science teams that deliver high impact results. After his post doc, he was a Data Science consultant at different companies and then moved onto leading Data Science teams. He acts to contextualize Data Science opportunities for senior leadership and then mobilizes and mentors the data science teams to focus on understanding and solving problems. ",https://www.youtube.com/watch?v=uIsR0BXBqfw,,,TMLS 2022,uIsR0BXBqfw
Nikita Namjoshi,Google Cloud,Generative AI Developer Advocate,Era of Multimodal AI & Reasoning,"The future of AI is multimodal. In this session, you will learn about a variety of multimodal use cases. You will also explore the importance of large context windows for effective reasoning over multi-modalities and learn how caching mechanisms can enhance performance.",,,Ignite Lighting Talk,,"Model dev, training, arch.","Multimodal AI, Reasoning, Performance optimization","Nikita helps developers build generative AI applications with Google Cloud’s machine learning tools. She’s the instructor of popular Coursera courses on generative AI and carbon aware computing. You can find her blogging about tech, explaining AI concepts on Youtube, and doing live demos at Google events.",https://www.youtube.com/watch?v=EmAVZvsopYw,,,MLOps & GenAI World 2024,EmAVZvsopYw
Shashank Shekhar,Dice Health,Co-Founder,A Practitioner's Guide To Safeguarding Your LLM Applications,"In this workshop, participants will learn essential techniques for enhancing the reliability and security of their Large Language Model (LLM) applications. Despite their powerful capabilities, LLMs often face challenges such as generating inconsistent outputs, straying off-topic, exposing sensitive data, etc. This workshop is tailored to give practitioners a broad understanding of current LLM limitations, as well as providing them with tools to address these limitations by generating structured outputs, ensuring topic relevance, mitigating hallucinations, and safeguarding company data.

This workshop will be tailored towards data scientists, ML engineers, and anyone involved in developing or managing LLM applications in the real world who is looking to enhance the robustness of their LLM systems. There will be hands-on programming components using open-source tools to reinforce the concepts covered during the workshop."," Participants of this workshop will gain a comprehensive understanding of:

Generating Structured Outputs for LLMs: Learn to generate, validate, and, if necessary, regenerate outputs that align with interoperability requirements, ensuring that LLM applications interact seamlessly with existing codebases.

Topic Relevance: Master techniques to ensure that LLMs consistently produce content that is relevant and on-topic, adheres to company brand guidelines, and focuses on delivering the desired user experience.

Hallucination Mitigation: Develop strategies to reduce the risk of LLMs generating inaccurate or misleading information. This includes setting programmable guardrails and providing a reliable ground truth data source for content generation.

Data Leakage Prevention: Understand and implement best practices to protect sensitive information, such as health records and financial details, from being inadvertently exposed by LLMs.

Safety Guardrails Implementation: Learn to establish robust safety guardrails to minimize risks like unauthorized model behavior (""jailbreaks""), ensure safe interactions with third-party applications, and manage operational costs related to LLM use."," Basic knowledge of large language models either via APIs such as OpenAI ChatGPT, or Anthropic Claude, or via local models such as Meta Llama, or Mixtral.",Workshop,6.0,Security and Privacy,"LLM security, structured outputs, hallucination mitigation.
",,https://www.youtube.com/watch?v=NMYIj1U8DRM,"Banking & Financial Services, Hospital & Health Care, Information Technology & Service, Insurance, Computer Software
",,TMLS 2024,NMYIj1U8DRM
Adam Probst,ZenML,CEO & Co-Creator,MLOps isn't that hard: Create a Modular Stack with Open-Source Tools,"Picture this: a machine learning system that's less of a riddle and more of an open book.

With the inherent intricacies of code, models, and data, these systems are a unique challenge. Building, managing, and deploying require a dance between processes, practices, and tools that span across the machine learning solutions lifecycle.

This talk is taking it back to basics and breaking down what it takes to build an end-to-end ML pipeline from the ground up. Listeners can expect to take away a blueprint for creating a MLOps pipeline for their own use-case, starting local all the way to scaling up to a production-ready stack.",,,Future of AI Talk,,Introduction to MLOps and GenAI,"MLOps pipeline, Open-source tools, Modular stack","Adam is a Mechanical Engineer and previously worked in the automotive industry. In 2017, he co-founded a company with Hamza Tahir, aiming to implement Machine Learning in a specific automotive use case. After several years of project work and recognizing similar MLOps challenges across various companies, they decided to launch ZenML, an open-source MLOps framework, in 2021. This initiative attracted investments from notable investors such as Crane from London and AIX Ventures from Palo Alto, who have also invested in companies like HuggingFace, H2o.ai, and Weights&Biases.",https://www.youtube.com/watch?v=6UCob-4508Q,,,MLOps & GenAI World 2023,6UCob-4508Q
"Yatong Li, Ryan Shannon, Vik Pant, PhD, Michelle Yu
","Sixty Degree Capital, Radical Ventures, Synthetic Intelligence Forum, Georgian, Maverix Private Equity","Managing Director, Investor, Founder, Investor, Founder & Managing Partner",Looking into AI/ML from a Venture Capital lens,"As a Toronto based Canadian VC focusing on global investments, Sixty Degree Capital invests in software that’s transforming industries, and the digital infrastructure that supports it. We have invested in lots of exciting portfolio companies including Arctic Wolf in the Cybersecurity space, DataGrail innovating on Data Privacy, Paperspace providing MLOps platform, MacroMeta solving Edge Computing bottlenecks, Pragma work in the Gaming Infrastructure space, Tact.ai and Radius Agent as Vertical SaaS products etc.). We believe AI/ML is a fundamental building block for next generation software, so Sixty Degree has spent lots of time to research, source, invest in the sub sectors. While we're a direct VC fund, we've allocated capital to become LPs of notable venture funds such as Lightspeed, a16z, NEA, Tiger, NfX, Initialized, Foundation Capital and Upfront Ventures, which gives us a special information advantage on what are the top tier Silicon Valley VCs are investing.",Insider information on how investors are allocating capital in AI/ML now,,Business Strategy  - Panel,3.0,Business and stakeholder alignment,"AI/ML Venture Capital, Investment Strategies, Software Transformation





","With over 10 years of experience in technology, finance and investment banking, Yatong leads Sixty Degree Capital’s investment in the technology space. He joined Sixty Degree Capital as an Associate in 2017 and quickly rose through the ranks to become a Managing Director on the technology investment team where he focuses on enterprise software opportunities. During his six year tenure at the firm, he managed to build the firm’s portfolio in the tech space with over 12 portfolio companies — including GrubMarket, MioVision, Pragma, DataGrail, Arctic Wolf, MacroMeta, Tact.AI, Lime, and Schrödinger.

Ryan Shannon is a member of the investment team at Radical Ventures.

Prior to joining Radical, Ryan was a Private Equity Investor at TPG Capital in San Francisco, where he focused on Leveraged Buyouts, Corporate Carve-outs, and Take-privates of North American businesses. Previously, Ryan worked as an Investment Banker in the Financial Sponsors group at Barclays in Los Angeles. 

Ryan received an HBA from the Ivey Business School at Western University, where he graduated as an Ivey Scholar, and an MBA from Harvard Business School.

Vik is the Founder of the Synthetic Intelligence Forum, a global community of practice focused on Data Science and applied AI. SIF is a professional networking, knowledge sharing, and collaborative learning hub for professional data scientists, AI researchers, and ML engineers.
He is an Adjunct Professor in the Faculty of Information (iSchool) at the University of Toronto and the Department of Geography, Environment, and Geomatics at the University of Ottawa.
Vik earned a doctorate from the Faculty of Information (iSchool) in the University of Toronto where his thesis was unanimously accepted by the examination committee As-Is and without any changes. His scholarship is focused on game-theoretic optimization of strategic coopetition in complex multi-agent systems. His research has been published in numerous peer-reviewed scholarly journals and he has also presented his academic research at refereed scholarly conferences and juried workshops.

Michelle is an Investor at Georgian, a growth-stage firm investing in high growth technology companies that harness the power of data in a trustworthy way. Michelle focuses on sourcing investment opportunities, investment due diligence, and advising companies post-investment on go-to-market, market analysis and product strategy. Prior to joining Georgian, Michelle worked on the investment team at Vectr Ventures in Hong Kong and for BDC Capital in Montreal. Michelle has a Bachelor of Commerce with a Double Major in Finance & International Management, with a Minor in Political Science, from McGill University.  
",https://www.youtube.com/watch?v=6X0sqZ7Metg,"Computer Software, Information Technology & Service, Venture Capital & Private Equity
",Not many investors have access to stay in close touch with top tier US VCs as we do.,,6X0sqZ7Metg
"Adam Kerr, Lyndon Quadros",Bell Canada,"Senior Machine Learning Engineer
Senior Manager, Artificial Intelligence",Modular Solutions for Knowledge Management at scale in RAG Systems," An important component of any RAG system or application is the underlying knowledge base that the bot or application uses. 

At Bell, we have built and adopted modular document embedding pipelines that allow some level of customization in the processing, ingestion and indexing of documents so that the peculiarities of various use cases and their raw documents can be efficiently indexed and used in their RAG applications. These pipelines also support both batch and incremental updates to the knowledge bases, with capabilities for automatically updating the indexes when documents are added to or removed from their source location. The modular nature also enables these pipelines to integrate with various document sources. These are supplemented with processes and conventions to ensure the efficient management and governance of the indexes at scale, providing a standardized framework for large-scale RAG applications at an enterprise level. ",How to approach Embedding Pipelines and Document Management for RAGs in a hybrid batch / incremental fashion,,MLOps & Infrastructure,6.0,Deployment and integration,"Modular document embedding, RAG systems, knowledge management.
","Adam: A senior machine learning engineer on Bell Canada's Customer Op's DEAI team. One of the key architects of Bell Canada's ML Platform, Maverick. His primary objective: develop an opinionated set of products and configurations to deploy end-to-end machine learning solutions using recommended infrastructure, targeted at teams starting out on their ML journeys.

Lyndon: Lyndon Quadros has lead teams that build and manage ML, AI and Data Engineering solutions on the cloud at an enterprise scale; and currently leads an MLOps and ML Engineering team at Bell. His current work focuses on Generative AI applications, AI infrastructure and MLOps standards and processes.",https://www.youtube.com/watch?v=w5FZh0R4JaQ,"Telecommunications, Information Technology & Service, Banking & Financial Services, Computer Software, Other
",,TMLS 2024,w5FZh0R4JaQ
Debadyuti Roy Chowdhury,InfinyOn,VP Products,Why real-time event streaming pattern is indispensable for an AI native future," Will our favourite favourite applications be able to deliver AI powered experiences without basic data quality and reliable infrastructure?

Distributed event streaming is the backbone of real-time analytics, insights, and intelligence. Stateful stream processing enables bounded and unbounded stream processing patterns that delivers rich datasets, streamline explainability, and delightful consumer experiences.

Event streaming patterns are useful for data collection, data enrichment, data profiling and aggregation, and measuring drift and explainability.

In this talk I will share my experience with AI use cases and event streaming."," Event driven architecture and event streaming patterns are a big power up for applied AI:
- Architecture patterns that are currently in use in IoT and B2B SaaS eCommerce and FinTech
- Tangible benefits of event driven architecture for AI in production in terms of:
- Operational Cost
- Infrastructure Overhead
- Developer Productivity and velocity",,Applied Case Studies,4.0,Deployment and integration,"Event Streaming, Real-time Analytics, AI Infrastructure.
","Deb leads product management at InfinyOn a distributed streaming infrastructure company. Deb's career since 2006 spans across IT, server administration, software and data engineering, leading data science and AI practices, and product management in HealthTech, Public Safety, Manufacturing, and Ecommerce.",https://www.youtube.com/watch?v=zCey3ZXpEZk,,,TMLS 2024,zCey3ZXpEZk
"Debadyuti RoyChowdhury, Sehyo Chang",InfinyOn,"VP Products, CTO",Hands-on Scalable Edge-to-Core ML Pipelines,"In this intensive workshop, participants will gain hands-on experience in designing, implementing, and troubleshooting a real-world distributed ML pipeline that spans from edge devices to core infrastructure. We'll tackle key MLOps challenges in building and managing complex, scalable systems for both operational analytics and AI/ML workflows.
Key topics covered:

Edge Computing: Simulating data ingestion from edge devices
Streaming Architecture: Implementing real-time data flows with open-source tools
Distributed Processing: Scaling ML workloads across heterogeneous environments
Model Deployment: Strategies for serving models at the edge and in the cloud
Observability and Monitoring: Setting up comprehensive monitoring for distributed ML systems
MLOps Best Practices: Applying DevOps principles to ML lifecycle management

Hands-on activities:

Participants will work in small groups to build a complete edge-to-core ML pipeline
Each team will deploy a pre-trained model for real-time inference at the edge
Groups will implement data validation and model monitoring across the pipeline
Participants will troubleshoot common issues in distributed ML systems"," Attendees will gain hands-on experience in designing, implementing, and troubleshooting a real-world distributed dataflow spanning operational analytics and AI/ML pipelines.

- Practical experience in designing scalable, distributed ML architectures
- Understanding of MLOps challenges in edge-to-core systems
- Hands-on skills in deploying and monitoring ML models across diverse environments
- Strategies for optimizing performance and resource usage in complex ML pipelines
- Best practices for maintaining data quality and model accuracy in production systems",,Workshop,7.0,Deployment and integration,"Edge-to-core pipeline, MLOps challenges, Real-time inference","Deb leads product management at InfinyOn a distributed streaming infrastructure company. Deb's career since 2006 spans across IT, server administration, software and data engineering, leading data science and AI practices, and product management in HealthTech, Public Safety, Manufacturing, and Ecommerce.

Sehyo Chang is the CTO and Co-founder of InfinyOn. He is also the creator of the Fluvio open-source project. He dabbled in WASM technology at an early stage and spearheaded InfinyOn to join Bytecode Alliance. He is a veteran of the open-source business model. Previously he was at NGINX, where he developed nginmesh and Rust binding for NGINX.",https://www.youtube.com/watch?v=S3o0iooG0r8,"Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Information Technology Services, Marketing & Advertising, Telecommunications, Insurance, Hospital & Health Care, Food & Beverage","It uses the leanest high performance streaming engine which runs in ARM64 edge devices and scales to large cloud servers.

It also uses a single paradigm for production data processing with modular components, intuitive development experience, eliminating the need to glue together a number of disjointed tools.",MLOps & GenAI World 2024,S3o0iooG0r8
Estefania Barreto,Recursion Pharmaceuticals,ML Engineer,Industrializing ML Workflows in Drug Discovery,"Recursion is committed to industrialize drug discovery by addressing the complexities of Machine Learning (ML) workflows head-on. A critical step in the drug discovery process is predicting compounds’ properties such as Absorption, Distribution, Metabolism, and Excretion (ADME), Potency and Toxicity among others, which allows the evaluation of a drug candidate for safety and efficacy, crucial for regulatory approval. In order to leverage its large volume of diverse and regularly updated chemical assays datasets, Recursion has engineered standardized and automated solutions to train and deploy predictive models on a weekly basis, thus accelerating the drug discovery process in early stages. In this talk, we will offer a comprehensive overview of our industrialized workflows to develop and deploy ML compound property predictors. Insights into Recusion’s strategy for data management, model training and deployment using both cloud and supercomputing resources will be shared."," During this presentation, attendees will gain understanding of our structured approach for creating and implementing machine learning models to predict compound properties in an industrial setting. We will explore Recusion's approach to managing data, training models, and deploying them utilizing a combination of cloud services and supercomputing resources",,Research or Advanced Technical,5.0,Deployment and integration,"Industrializing ML Workflows, Drug Discovery, Predictive Models
","Estefania Barreto-Ojeda is an ML Engineer at Recursion, where she builds and automates machine learning pipelines for drug discovery. A physicist by training, she has a PhD in Biophysical Chemistry from the University of Calgary where she participated in Google Summer of Code as an open source software developer . She has given talks at several major data conferences, including PyData. Estefania is a full time automation fan, an occasional open-source contributor, and a seasonal bicycle lover.",https://www.youtube.com/watch?v=Te02lHSsxr4,"Hospital & Health Care, Computer Software, Other
",,TMLS 2024,Te02lHSsxr4
Rokshana Stephny  Geread,Biosymetrics,Data Scientist - Computer Vision,A machine learning-driven phenotyping platform for rapid in vivo target validation and precision medicine development,"Zebrafish are a popular model organism in developmental biology, molecular genetics, and toxicology studies due to their small size, low breeding costs, transparent embryos, morphological identification, and similarity of their genome to the human genome. They are particularly well suited for large scale phenotypic screening in drug discovery studies, where observable characteristics (phenotypes) can be monitored due to biological effects from a small molecule or gene modification (via CRISPR gene editing). Using imaging and computer vision analysis, some zebrafish phenotypes that can be monitored include morphology, heart rate, and ejection fraction (the amount of blood the heart pumps). To scale up these experiments, some logistical challenges need to be addressed. First, standardizing the positioning of live zebrafish embryos is necessary to produce consistent imaging data and maximizing the value of experiments. Second, up to thousands of images need to be labelled for specific organs to analyze specific phenotypes, a time-consuming task if performed manually. To address these challenges, we have developed a comprehensive zebrafish phenotyping assay that uses custom 3D-printed mounting platforms to affix embryos in a standardized orientation for consistent image and video acquisition. We then developed a computer vision analysis pipeline that couples a fine-tuned instance segmentation model with downstream phenotyping analysis, providing quantitative. The model, based on the Detectron2 segmentation model architecture, is trained to automatically detect individual zebrafish embryos and its organs from images generated by our screening assay. We applied our approach to study several genes associated with cardiovascular disease using CRISPR injections, identifying subtle cardiac abnormalities. Our results demonstrate highly accurate segmentation on validation data, achieving average precision scores of 60% for the ventricle, and over 80% for other major organs. Our phenotyping platform is designed to close the gap between gene and phenotypes, thereby empowering and accelerating drug discovery programs with a higher probability of clinical translation. ",Using AI driven technology and image processing to analyze cardiac phenotypes in zebrafish images and videos.,,Case Study,4.0,"Model dev, training, arch.","Zebrafish Phenotyping, Computer Vision, AI-Driven Drug Discovery","Steph joined BioSymetrics in January 2021, with a background in biomedical ECE engineering (MASc) and electrical engineering (BEng). She has diverse work experience, starting with terminal web development for Bell Canada before building, validating, and delivering a production-ready machine learning model for breast cancer imaging diagnosis. This last project was a collaboration with University Health Network and St Michael's Hospital, Toronto, and resulted in three journal articles.

“I transitioned from the telecommunications industry to healthcare because I wanted to make a positive impact on patients’ quality of life. I wanted to contribute to bridging the gap between technology and healthcare, which has the potential to enhance the overall healthcare system.",https://www.youtube.com/watch?v=ot3eDZY2X-s,,"This speech will discuss how to leverage existing models, utilizing transfer learning to accurately predict of medical images. The power of segmentation in medicine, to accelerate phenotypic analysis in zebrafish models. 
",TMLS 2023,ot3eDZY2X-s
Prakash Putta,Instacart,Staff Software Engineer,Supercharging Search with LLMs: The Instacart Journey,"Discover how Instacart's search journey has been revolutionized through the implementation of Language Models (LLMs). By leveraging the power of LLMs, we have achieved significant enhancements, transforming the search experience for Instacart users. Join us to discover real-world use cases, gain insights into our seamless integration strategies, and witness how LLMs have empowered us to overcome challenges, deliver personalized recommendations, and elevate the overall search experience at Instacart.
[An example - https://techcrunch.com/2023/05/31/instacart-in-app-ai-search-tool-powered-by-chatgpt/]","The audience will gain valuable insights into the practical implementation of LLMs in real production use cases. While LLMs have garnered significant attention, their effective integration into live environments remains a challenge. By attending this talk, participants will learn firsthand about the successful utilization of LLMs to enhance Instacart's search journey. Through real-world examples, they will discover how LLMs can be harnessed to supercharge search capabilities and derive actionable knowledge for their own production scenarios.",,Case Study,4.0,Deployment and integration,"Generative AI search, LLM integration, real-world use cases
",https://www.linkedin.com/in/prakashreddyputta/,https://www.youtube.com/watch?v=q5LP1X3bhgc,,,MLOps & GenAI World 2023,q5LP1X3bhgc
Winston Li,Arima,Founder,Dynamic Huff's Gravity Model with Covariates for Site Visitation Prediction," Huff's Gravity Model is a widely used statistical analysis for predicting the probability of a consumer visiting a location, as a function of distance, attractiveness, and the available alternatives. First formulated by David Huff in 1963, it has been widely used in marketing, economics, retail research and urban planning.

In this presentation, we introduce Dynamic Huff's Gravity Model with Covariates, a technical enhancement to the traditional gravity model where additional covariates like mobility and population behavioural data are included to further increase model accuracy and explanability. We cover the model formulation, examples of additional datasets that can be included, and a case study with a Canadian retailer to demonstrate how the model can provide business value."," - What is the Huff Gravity Model
- What business problems can the model solve
- What companies/use cases are ideal for this technique",,Applied Case Studies,3.0,"Model dev, training, arch.","Dynamic Huff's Gravity Model, Covariates, Site Visitation Prediction.
","Winston is the founder of Arima, a Canadian based startup working on synthetic consumer data. Our flagship product, the Synthetic Society, is a privacy-by-design, individual level database that mirrors the real society. Built using trusted sources like census, market research, mobility and purchase patterns, it contains 50,000+ attributes for 40+335 million individuals across Canada & US and enables advanced modelling at the most granular level.

Aside from Arima, Winston is an avid researcher in data mining, where he publishes in outlier detection and synthetic data generation. Winston is the co-creator of PyOD, one of the most widely used open-source Python toolbox for data mining.",https://www.youtube.com/watch?v=8GmDu6lwHn4,"Banking & Financial Services, Food & Beverages, Hospital & Health Care, Insurance, Marketing & Advertising, Telecommunications, Other, Automotive
",,TMLS 2024,8GmDu6lwHn4
Sophia  Yang,Anaconda,Senior Data Scientist,Introduction to LangChain and Retrieval Augmented Generation (RAG),"LangChain is an open-source framework for developing applications powered by language models. In this introductory talk, we'll explore the core concepts behind LangChain including chains, memory, tools, agents, embeddings, and vector databases. Additionally, we'll use LangChain and Panel to build LLM-based applications including answering questions using information from documents, a technique known as Retrieval Augmented Generation. Join us to learn LangChain!",Use LangChain to develop LLM applications,,Advanced Technical/Research,4.0,Introduction to MLOps and GenAI,"LangChain, Retrieval Augmented Generation (RAG), LLM Applications
","Sophia Yang is a Senior Data Scientist and a Developer Advocate at Anaconda. She is passionate about the data science community and the Python open-source community. She is the author of multiple Python open-source libraries such as condastats, cranlogs, PyPowerUp, intake-stripe, and intake-salesforce. She serves on the Steering Committee and the Code of Conduct Committee of the Python open-source visualization system HoloViz. She also volunteers at NumFOCUS, PyData, and SciPy conferences. She holds an M.S. in Computer Science, an M.S. in Statistics, and a Ph.D. in Educational Psychology from The University of Texas at Austin.",https://www.youtube.com/watch?v=oL-SR1U9g0k,Information Technology & Service,,,oL-SR1U9g0k
Prashanth Rao,"Kùzu, Inc.",AI Engineer,"Kùzu - A fast, scalable graph database for analytical workloads"," In this session, we will introduce Kùzu, a highly scalable, extremely fast, easy-to-use, open source embedded graph database designed for analytical query workloads. Users who are familiar with DuckDB in the SQL world will find Kùzu to be a refreshingly familiar graph analogue. A number of state-of-the-art methods from graph database research are highlighted.

The workshop will include a practical component that showcases how simple and easy-to-use Kùzu is for data scientists and engineers. We will demonstrate popular use cases by transforming a relational dataset (in the form of tables) into a knowledge graph, run Cypher queries on the graph, analyze the dataset using graph algorithms, and train a simple graph neural network using PyTorch Geometric to compute node embeddings and store them in the graph database for downstream use cases. We will end by summarizing how these methods can help build advanced RAG systems that can be coupled with an LLM downstream.

Additional notes

In addition to the workshop where we go into the hands-on concepts of knowledge graphs and how to use them, we'd very much like to have a 30-minute talk that introduces the idea of Kùzu and how it's different from other graph databases, and the core innovations under the hood. If the organizers feel that the content is better separated into two parts (a separate talk on the main stage and the workshop with the practical component), that's perfectly fine as well. For this reason, I've opted for any of the available presentation times."," 1. What are knowledge graphs
2. The characteristics of competent graph database systems
3. How to work with graphs on real-world data
4. How to query a graph in Cypher
5. How to run graph algorithms for graph data science
6. How to do graph machine learning

The core message that attendees will take away is this: There are times when modeling tabular/relational data as a graph is necessary and useful, e.g., to obtain a more object-oriented model over your records or find indirect connections/paths between the entities in the data. In such cases, using an open source, embedded graph database like Kùzu is a simple and low-barrier-to-entry option to analyze the connected data at a much greater depth via graph data structures.","Basic Python programming skills (all the background for what knowledge graphs are, and how to work with a graph database will be provided to users who are new to the world of graphs).",Workshop,5.0,"Model dev, training, arch.","Knowledge graphs, graph database, Cypher queries.






",,https://www.youtube.com/watch?v=gsMW-cRvqfQ,"Computer Software
",,TMLS 2024,gsMW-cRvqfQ
Adhithya Ravichandran,Loblaw Digital,"Senior Engineer, ML Platform","ML in Production: Implementation, Tooling & Engineering, Data/ML Ops","At Loblaw Digital, ML models play a crucial role in every part of our business – from helping customers search for products they want, providing them with personalized shopping experiences, ensuring the orders they place get to them on time. Behind the scenes, our MLEs are constantly developing, and improving the models that power our business. Furthermore, more use-cases and teams of MLEs are being stood up. To facilitate a high volume of ML models throughout their lifecycle, we have invested in building an ML observability stack in our ML platform. In this talk, we discuss how our centralized observability stack – in-house inference logging, along with Snowplow analytics, which captures user behaviour data. This stack enables our MLEs to have visibility into how ML models influence user interaction, and eventually, revenue in our e-commerce sites. We further discuss how these tools help us enable model improvements and drive real world business outcomes. We also discuss ideas for future feature improvements and additions to our observability stack.","Investing in building and customizing model observability helps measure performance and ROI of ML, which in turn leads to continuous ML improvements and builds confidence and trust from stakeholders",,Case Study,5.0,Deployment and integration,"Observability, ML Models, Tooling","Adhithya (Adhi), is a Senior Engineer in the Machine Learning (ML) Platform team at Loblaw Digital (LD), where he helps build a self-service platform to provide the best development experience that empowers Data Scientists to productionalize ML services. Previously he has held a range of positions spanning analytics, Machine Learning, data strategy, and building a foundational data stack across a range of companies – early stage, Series B-C startups and Canada’s largest banks.  Outside of work he spends time practising Muay Thai and yoga, as well as advising a private equity firm in Latin America.",https://www.youtube.com/watch?v=VqdsKNvLrhE,"Banking & Financial Services, Insurance, Marketing & Advertising, Real Estate, Investment, Finance, Development, and any of the above industries that deal with automating maintenance and/or custormer retention.
","The breadth of ML models in production and the different in-house tools that were needed to be integrated while building this tool is unique. The ability to make this tool self-serve for multiple Data science teams poses unique challenges in system design, delivery and operation
",TMLS 2023,VqdsKNvLrhE
Hannes Hapke,Digits,Principal Machine Learning Engineer,Fine-tune LLMs or Integrate 3rd party APIs? A financial Case-study," ""Almost a year ago, with the introduction of ChatGPT and, subsequently GPT-4, the sphere of machine learning transformed completely. These advancements and LLMs unlocked the capability to address previously unsolvable problems. 

In this talk, Hannes explains how Digits’ machine learning team has adapted to the new world of LLMs, how their MLOps processes have changed, and the team’s learning around fine-tuning and deploying LLMs as part of a small, highly focused ML team. He will also discuss the key questions you must ask to determine if you should fine-tune open-source LLMs or integrate with a 3rd party API, the challenges and ethical concerns of using advanced language models via APIs, and how these risks in your projects can be mitigated through engineering.""","""* When to use model APIs and when to avoid them
* When to fine tune LLMs
* How to deploy LLMs effectively """,,Case Study,4.0,Deployment and integration,"Fine-tuning LLMs, Model APIs, Financial case study
","""As one of Digits' principal machine learning engineers, Hannes Hapke is developing innovative machine learning systems to give accountants and business owners real-time insights into their businesses. Before joining Digits, Hannes solved machine learning infrastructure problems in various industries, including healthcare, retail, recruiting, and renewable energies. 

Hannes is an active contributor to TensorFlow's TFX Addons project and has co-authored multiple machine learning publications, including the book """"Building Machine Learning Pipelines"""" by O'Reilly Media. He has also presented state-of-the-art ML work at conferences like Google Developer Connect. He is excited about the recent developments around Large Language Models and ML Engineering.""",https://www.youtube.com/watch?v=vDVNW90Zl_M,Banking & Financial Services,,,vDVNW90Zl_M
Will Jones,LanceDB,Software Engineer,"LanceDB: a unified storage layer for vector search, analytics, and model training ","AI is missing a storage layer. Traditional databases weren’t built for AI use cases. New vector databases mostly exist to store vectors, but what about the rest of your data? We’ll be looking at how LanceDB can unify vector search, analytics, and model training workloads under a single database.",,,Future of AI Talk,,Performance optimization and efficiency,"Unified storage layer, Vector search, Model training
","Will Jones is a software engineer working on LanceDB’s storage format, Lance. He also volunteers as a PMC member of the Apache Arrow project and contributor to the Rust Delta Lake project.",https://www.youtube.com/watch?v=s3L35puUWSA,,,MLOps & GenAI World 2023,s3L35puUWSA
Sophie Daly,Stripe,Staff Data Scientist,Lessons Learned Productionising LLMs for Stripe Support,"Large Language Models are an especially exciting opportunity for Support Operations: they excel at answering questions, completing sentences, and summarising text while requiring ~100x less training data than the previous generation of models. 

In this talk, Sophie discusses lessons learned productionising Stripe’s first application of Large Language Modelling - providing answers to user questions for Stripe Support.
",Three important lessons to keep top of mind when solving a business problem using LLMs.,-,,4.0,"Model dev, training, arch.","Productionising LLMs, Stripe Support, Large Language Models
","Sophie is a Staff Data Scientist working on improving user experience and efficiency for Stripe’s Support Operations team Stripe. Her favourite thing about being a data scientist is getting to work on a huge variety of business problems, and using analysis and machine learning to solve them. Her hobbies include trying to keep up with the impossibly fast-growing world of LLMs (inside work) and binge watching The Office (outside work).",https://www.youtube.com/watch?v=FQl6K160DKU,Support Operations,,,FQl6K160DKU
Simba Khadder,Featureform,Founder & CEO,"Feature Stores in Practice: Train and Deploy an End-to-End Fraud Detection Model with Featureform, Redis, and AWS.","In this workshop, we'll go through the process of building a fraud detection model from scratch using Featureform's open-source feature store alongside a handful of other tools like Redis and Sagemaker. We'll both train and deploy the model through this workshop. We'll deep dive into where feature stores fit into the MLOps stack, the value they provide, and how to use them in practice.","Participants will learn how to:
- Use Featureform to build, manage, and serve their model features from fraud detection data.
- Use Redis, Spark, and Sagemaker to train and deploy a random forest model.
- Use Terraform and other best practice DevOps, DataOps, and MLOps through the process.","Basics of cloud, networking, databases, and machine learning.",Workshop,4.0,Deployment and integration,"Feature Stores, Fraud detection, Featureform





","Simba Khadder is the founder & CEO of Featureform. He started his ML career in recommender systems where he architected a multi-modal personalization engine that powered 100s of millions of user's experiences. He later open-sourced and built a company around their feature store. Featureform is the virtual feature store. It enables data scientists to define, manage, and serve model features using a Python API. Simba is also a published astrophysicist, an avid surfer, and ran a marathon in basketball shoes.",https://www.youtube.com/watch?v=QQeSylVBuhU,,,MLOps & GenAI World 2023,QQeSylVBuhU
Mei Chen,MunichRE,Machine Learning Engineer,"The fairness of fairness: Evaluating, Monitoring, and Impacts of ML in Insurance ","This session explores the technical and ethical aspects of creating and deploying fair machine learning (ML) models in the insurance industry. This presentation emphasizes practical strategies for achieving both equitable and favourable outcomes through the effective implementation of ML.

Key topics covered in the presentation include:
- A comprehensive approach to calculating, interpreting, and monitoring bias metrics for assessing model fairness
- Strategies for optimizing ML models to achieve high fairness and accuracy simultaneously
- A review of the Federal Trade Commission's research on the impact of credit-based insurance scores on different racial and ethnic groups
- Valuable insights gleaned from the research and current best practices in fairness analysis

Attendees can expect to gain a deeper understanding of the challenges and opportunities surrounding fairness in ML for the insurance sector. By drawing from both research findings and real-world experiences, this presentation aims to equip participants with the knowledge and tools necessary to foster transparent, ethical, and effective ML solutions in the insurance industry.",Our distilled practice in integrating fairness knowledge into our model development process,,Case Study,4.0,"Ethics, governance compliance ","ML Fairness, Bias Metrics, Insurance Industry","Mei is a Machine Learning Engineer at MunichRE, working on ensuring fairness in insurance models throughout its lifecycle. She has previously launched profitable tech and product companies, published academic works in clinical and algorithmic biosignal processing, and produces music under the artist name ""Multiversity"".",https://www.youtube.com/watch?v=qoOe7N5z-0Q,"Insurance
","This talk goes deeper than surface level calculation of fairness metrics. It touches on the ethical issues, potential biases and their harm, real-life examples in research and in our day-to-day practice that have came up as challenges, and our current solutions to overcome them. 
",TMLS 2023,qoOe7N5z-0Q
Gautam Kamath,University of Waterlo / Vector Institute,"Assistant Professor, Faculty Member and Canada CIFAR AI Chair",Privacy Risks and Protections in Machine Learning Systems," Machine learning models are prone to leaking information about their training data. This can be problematic when the training data is privacy-sensitive information belonging to people. I will highlight several privacy risks of modern machine learning systems and discuss rigorous ways to protect against these vulnerabilities, preserving user privacy and maintaining their trust in the system.","Privacy risks in modern machine learning systems are increasingly significant. At the same time, there are rigorous protections that allow us to guard against these privacy risks being realized.",,Business Strategy or Ethics,4.0,Security and Privacy,"Privacy Risks, Machine Learning Systems, Privacy Protections","Gautam Kamath is an Assistant Professor at the University of Waterloo, and a Faculty Member and Canada CIFAR AI Chair at the Vector Institute for Artificial Intelligence. His research interests are in trustworthy algorithms, statistics, and machine learning, particularly focusing on considerations like data privacy and robustness. He has a B.S. from Cornell University and a Ph.D. from MIT. He is the recipient of the 2023 Golden Jubilee Research Excellence Award. His online course on differential privacy is the most popular resource for learning the topic, with his lecture videos having over 100,000 views. He is an Editor-in-Chief of Transactions on Machine Learning Research, and on the Executive Committee of the Learning Theory Alliance.",https://www.youtube.com/watch?v=qbZbhMyavxs,,,TMLS 2024,qbZbhMyavxs
Anu Reddy,Google,Senior Software Engineer,Optimizing AI/ML Workflows on Kubernetes: Advanced Techniques and Integration,"Explore advanced technical strategies for optimizing AI/ML workflows on Kubernetes, the world's leading open-source container orchestration platform. This session will cover techniques for integrating open-source AI tools, wide range of workflows, including training, inference, prompt engineering (RAG, agent), managing multi-cluster environments, and ensuring cost-effective resource utilization. Participants will gain deep insights into how Kubernetes supports flexible and scalable AI/ML infrastructure, with specific examples of using Kubernetes-native tools like Kueue for job queuing and Ray for distributed computing. The session will also highlight the use of NVIDIA GPUs, TPUs, and advanced workload management strategies, with Google Kubernetes Engine (GKE) as an illustrative example."," - Advanced techniques for optimizing AI/ML workflows on Kubernetes
- Integration of open-source AI tools within Kubernetes environments
- Strategies for managing multi-cluster AI/ML deployments and optimizing resource utilization",,Advanced Technical/Research,6.0,Deployment and integration,"Kubernetes AI/ML workflows, Multi-cluster deployments, Resource utilization optimization",Anu is a senior software engineer working on optimizing Google Kubernetes Engine for techniques like RAG and supporting popular AI/ML framework and tools such as Ray.,https://www.youtube.com/watch?v=grCvM9tkS7Q,,,MLOps & GenAI World 2024,grCvM9tkS7Q
Marcelo Litovsky,Aporia,Director of Sales Engineering,Building an MLOps Team," MLOps has become more recognized in businesses, bringing more attention to the teams working on it. However, creating a team skilled in MLOps is challenging due to the varied skills needed for handling AI/ML models in real-world settings. In this session, we’ll talk with Munich Re’s Magd Bayoumi about his experience in forming a successful MLOps team. We’ll discuss good hiring strategies and key aspects that make MLOps unique. With a strong MLOps team, you can clearly show the value you bring to your company. ",,,Ignite Lightning Talk,,Introduction to MLOps and GenAI,"Building an MLOps Team, Hiring Strategies, MLOps Skills","Marcelo Litovsky is an experienced IT professional with 30 years of a diverse background in Enterprise Architecture, AI, Systems and Database Management, and Programming. Throughout his career, he has worked in various industries, including Financial Services, Entertainment, and Information Technology. Currently, he serves as the Director of Sales Engineering at Aporia. His expertise aids Data Scientists, Machine Learning Engineers, and Business Users in collaborating to unlock and amplify the business value of their machine learning models. When he's not engaging with customers or writing Python code, you can find him at the gym or preparing healthy vegan meals.",https://www.youtube.com/watch?v=AnwBQKv-HRs,,,MLOps & GenAI World 2023,AnwBQKv-HRs
Dan Shiebler,Abnormal Security,Head of Machine Learning,How to Design and Build Resilient Machine Learning Systems,"
The real world is messy. Systems fail, pipelines break, services go down, engineers push bugs, and users behave erratically. Software is hard exactly because these problems always happen. Effective systems must gracefully handle these events and smoothly degrade without catastrophic failure. 

Unfortunately, ML systems are more likely to break than bend. Just like a boxer who only punches a bag will fail in the ring, an ML model that only learns with clean data may fail in production. Most ML models are trained with clean data, and when failures occur feature distributions can shift in ways that the model has never seen during training. This can cause strange and unexpected behavior.

In this talk we will explore how to build resilience into ML systems. We will discuss several types of production-specific risks and how these risks tend to manifest. These risks are common across many domains, but we will primarily use examples from our experience at Abnormal Security to demonstrate how we can detect, mitigate, and overcome these risks. ",How to design machine learning systems that are resilient to the kinds of problems that occur in production systems,-,,5.0,Performance optimization and efficiency,"Resilient ML systems, Handling production failures, Risk mitigation in ML
","Hi, I’m Dan Shiebler. I like math, history podcasts, fantasy novels, riding my bicycle, and traveling. I live in NYC.

Today I work as the Head of Machine Learning at Abnormal Security. I lead our team of 40+ detection engineers to build AI systems that fight cybercrime. We use a combination of foundational data engineering and advanced ML to detect and remediate cyberattacks. Our technology protects many of the world’s largest companies.

Previously, I managed the Web Ads Machine Learning team at Twitter. Before that I worked as a Staff ML Engineer at Twitter Cortex and a Senior Data Scientist at TrueMotion.

I’ve also spent some time in Academia. My PhD at the University of Oxford focused on applications of Category Theory to Machine Learning (advised by Jeremy Gibbons and Cezar Ionescu). Before that I worked as a Computer Vision Researcher at the Serre Lab.",https://www.youtube.com/watch?v=AqUrswRhcqg,Computer Software,,,AqUrswRhcqg
"Michael Haacke Concha, Diego Castillo Warnken",LATAM Airlines,"MLOps Lead, Staff Machine Learning Engineer",Revolutionizing the skies: Mlops case study of LATAM airlines,"This talk explores how LATAM Airlines leveraged MLOps to revolutionize their operations and achieve financial gain in the hundred of millions of dollars. By integrating machine learning models into their daily workflows and automating the deployment and management processes, LATAM Airlines was able to optimize tariffs, enhance customer experiences, and streamline maintenance operations. The talk will highlight key MLOps strategies employed, such as continuous integration and delivery of ML models, real-time data processing. Attendees will gain insights into the tangible benefits of MLOps, including cost savings, operational efficiencies, and revenue growth, showcasing how strategic ML operations can create substantial value in the airline industry.
"," You will acquire insight into how a scalable and decentralized tech team grows inside LATAM airlines, thanks to technology and organizational structure. also you will learn some of our successful use cases of our MLOps ecosystem.",,Applied Case Studies,2.0,Deployment and integration,"MLOps, Operational efficiency, 
Airline industry"," Michael Haacke Concha is the Lead Machine Learning Engineer of the centralized MLOps team at LATAM Airlines. He holds both a Bachelor's and a Master’s degree in Theoretical Physics from Pontificia Universidad Católica de Chile (PUC). Over his three years at LATAM Airlines, he developed an archival and retrieval system for black box data of the aircraft to support analytics. He then played a key role in building the framework for integrating the Iguazio MLOps platform within the company. In the past year, he has been leading the development of a new platform using Vertex GCP.

Prior to joining LATAM Airlines, Michael worked as a data scientist on the ATLAS experiment at the Large Hadron Collider (LHC), where he contributed to various studies, including the search for a long-lived Dark Photon and a Heavy Higgs.

Diego Castillo is a Consultant Machine Learning Engineer at Neuralworks, currently on assignment as Staff in LATAM Airlines, where he plays a pivotal role within the decentralized Data & AI Operations team. A graduate of the University of Chile with a degree in Electrical Engineering, Diego has excelled in cross-functional roles, driving the seamless integration of machine learning models into large-scale production environments. As a Staff Machine Learning Engineer at LATAM, he not only leads and mentors other MLEs but also shapes the technical direction across key business areas.
Throughout his career at LATAM Airlines, Diego has significantly impacted diverse domains, including Cargo, Customer Care and the App and Landing Page teams. He has more recently been supporting the migration of the MLOPS internal framework from Iguazio to Vertex GCP.
With a comprehensive expertise spanning the entire machine learning lifecycle, Diego brings a wealth of experience from previous roles, including Data Scientist, Backend Developer, and Data Engineer, making him a versatile leader in the AI space.",https://www.youtube.com/watch?v=h6X7Cbo_-ho,,"Exclusive insight to use cases in the aviation industry, working with a inhouse MLOps ecosystem with more than 60 different products with current developments team of more of 100 developers.
",MLOps & GenAI World 2024,h6X7Cbo_-ho
Dmitry  Petrov,DVC,CEO,Data Versioning in Generative AI: A Pathway to Cost-Effective ML,"For 5 years we have been building DVC and we know how data versioning helps teams. The evolving Generative AI workflows are different and require an evolution of versioning workflows to accomplish Generative AI goals. This new era thrives on vast amounts of unstructured data, which include everything from images, videos, and audio, to MRI scans, document scans, and plain text dialogues. This data, often scaling into billions of objects, together with the resource-consuming task of scoring models on expensive GPU hardware or using model APIs like ChatGPT, brings forth unique challenges in the field of data management and versioning.

In this talk, we will delve into data versioning in the context of generative AI. Our focus will be on strategies that assist businesses in minimizing their processing time and the volume of API calls to external models like ChatGPT, resulting in substantial cost savings. Furthermore, we will discuss effective methodologies for sharing datasets amongst ML researchers to promote seamless collaboration.

Lastly, we will examine the pivotal transformations generative AI has introduced to data versioning in the past year including annotations and embeddings versioning. Together, these insights will provide attendees with an in-depth understanding of the rapidly evolving data management landscape in the era of generative AI. ","1. How data management is different in generative AI environment compared to traditional ML
2. How to save const on compute and API calls using data versioning
3. Dataset sharing in the team as a way to improve collaboration
4. How to efficiently version annotation, embeddings and auto-labels together with data",,Advanced Technical/Research,6.0,"Model dev, training, arch.","Data versioning, generative AI, cost-effective ML
","Dmitry Petrov is the CEO and co-founder of Iterative.ai, working on building data-centric MLOps tools. He's an ex-data scientist at Microsoft with a Ph.D. in Computer Science and an active open-source contributor. He has written and open-sourced the first version of DVC.org (part of Iterative) – a data versioning and machine learning workflow management tool. He also implemented a wavelet-based image hashing algorithm (wHash) in the open-source library ImageHash for Python.",https://www.youtube.com/watch?v=1qLQ4VhgXzE,,"The talk brings together unique insights from creators of popular data versioning tool DVC with first-hand experience in navigating the challenges of data versioning in generative AI. We will focus on basic principles and user stories instead of DVC or other tools. Moreover, the talk will foster dialogue about recent changes in the field, providing fresh perspectives on the transformations generative AI has introduced to data versioning.
",MLOps & GenAI World 2023,1qLQ4VhgXzE
"Liz Lozinsky, Sara Ghaemi",TELUS,"Engineering Manager, Gen AI platform team, Senior Software Developer, Gen AI Platform TeaM",Fuel iX: An enterprise grade Gen AI platform,"Sharing how TELUS enabled Gen AI for everyone internally through Fuel iX to get the most value out of the latest advancements in generative AI, while ensuring flexibility, control, privacy, trust and joy!

TELUS has been making incredible strides in AI and we’re at the forefront of scaling generative AI for our team members and customers. We’ve developed a suite of internal generative AI platforms and tools to empower our team members to safely experiment with this technology, fostering a culture of innovation and trust. With over 24,000 team members already utilizing our AI-powered tools in ways we never imagined, it’s clear that the potential for generative AI to enhance productivity and efficiency is immense. By automating repetitive tasks and providing valuable assistance, our AI tools enable team members to focus on innovation and problem-solving, ultimately driving positive change and progress."," - Building out enterprise grade Gen AI platforms 
- The importance of responsible AI and ethical considerations in the development of Gen AI applications 
- TELUS's efforts in scaling generative AI for team members and customers
- The significant impact of AI tools in enhancing productivity and efficiency",,Applied Case Studies,4.0,Introduction to MLOps and GenAI,"Generative AI, enterprise-grade AI platforms, productivity and efficiency.






","Liz: Liz is a Developer Advocate and Engineering Manager on the Platform Engineering team at TELUS. With a background in software development and a BASc from the University of Waterloo in Electrical Engineering with an option in Management Science, Liz leads a talented team focused on democratizing Gen AI for all. Known for her creativity, positivity, and a hint of whimsy, Liz approaches every challenge with enthusiasm and a spirit of adventure! 

Sara: Sara is a Software Developer in the Gen AI Platform team at TELUS with background in both research and practical applications of software systems. She is one of the lead developers working on the Generative AI initiative at TELUS. She holds a Master’s degree in Software Engineering and Intelligent Systems from the University of Alberta for which she received the C.R. James Award for Best Master of Science Thesis award from the university. Sara is deeply passionate about leveraging her expertise to make technology more accessible and beneficial to all.",https://www.youtube.com/watch?v=bUmI6VDKdcM,"Telecommunications, Computer Software, Information Technology & Service
",,TMLS 2024,bUmI6VDKdcM
Tejas Chopra,Netflix,Senior Software Engineer,Memory Optimizations for Machine Learning,"As Machine Learning continues to forge its way into diverse industries and applications, optimizing computational resources, particularly memory, has become a critical aspect of effective model deployment. This session, ""Memory Optimizations for Machine Learning,"" aims to offer an exhaustive look into the specific memory requirements in Machine Learning tasks, including Large Language Models (LLMs), and the cutting-edge strategies to minimize memory consumption efficiently.

We'll begin by demystifying the memory footprint of typical Machine Learning data structures and algorithms, elucidating the nuances of memory allocation and deallocation during model training phases. The talk will then focus on memory-saving techniques such as data quantization, model pruning, and efficient mini-batch selection. These techniques offer the advantage of conserving memory resources without significant degradation in model performance.
A special emphasis will be placed on the memory footprint of LLMs during inferencing. LLMs, known for their immense size and complexity, pose unique challenges in terms of memory consumption during deployment. We will explore the factors contributing to the memory footprint of LLMs, such as model architecture, input sequence length, and vocabulary size. Additionally, we will discuss practical strategies to optimize memory usage during LLM inferencing, including techniques like model distillation, dynamic memory allocation, and efficient caching mechanisms.
By the end of this session, attendees will have a comprehensive understanding of memory optimization techniques for Machine Learning, with a particular focus on the challenges and solutions related to LLM inferencing."," By the end of this session, attendees will have a comprehensive understanding of memory optimization techniques for Machine Learning, including: pruning, quantization, distillation, etc. and where to apply them. They will also learn about how to implement these techniques using pytorch.",,Research or Advanced Technical,5.0,Performance optimization and efficiency,"Memory optimization, Pruning and quantization, Large Language Models (LLMs)"," Tejas Chopra is a Senior Software Engineer, working in the Data Storage Platform team at Netflix, where he is responsible for architecting storage solutions to support Netflix Studios and Netflix Streaming Platform. Prior to Netflix, Tejas was working on designing and implementing the storage infrastructure at Box, Inc. to support a cloud content management platform that scales to petabytes of storage & millions of users. Tejas has worked on distributed file systems & backend architectures, both in on-premise and cloud environments as part of several startups in his career. Tejas is an International Keynote Speaker and periodically conducts seminars on Micro services, NFTs, Software Development & Cloud Computing and has a Masters Degree in Electrical & Computer Engineering from Carnegie Mellon University, with a specialization in Computer Systems.",https://www.youtube.com/watch?v=ILR0F6Hjqq4,"Information Technology Services, Computer Software","This session is unique where it dives into the importance of Memory in training and inferencing ML/AI models, and shows the impact that a few lines of code can have on the performance of models. 

It not only includes theory but example code snippets to help out engineers to use these techniques in practice.",MLOps & GenAI World 2024,ILR0F6Hjqq4
Varun Kumar Khare,NimbleEdge,CEO,Hyper-scaling Real-time Personalization with Privacy via on-device computing,"The talk focusses on overcoming the de-facto trade-offs on privacy to personalization, latency to throughputs and scale to infrastructure costs. How over-reliance on cloud infrastructure has been a challenge for major applications hindering their revenue streams, complicated deployment cycles and session-aware ML capabilities. The talk will also be exploring the current state of edge computing infrastructure and the path ahead for MLOps.","Our experiences have shown ML teams to save over 50% of their cloud infra costs (amounting to $10-20M savings per year). The world still thinks of edge as limited to IoT while in reality, edge, especially with privacy, is a phenomenon for mobile and desktop apps. Given the proliferation of the edge computing, MLOps community can extract a lot of under-utilized compute capabilities on users' devices. What everyone thinks of as default trade-offs do not exist anymore and we as data scientists have technologies to build great user experiences without being intrusive.",,Business Strategy,4.0,Security and Privacy,"Edge computing, On-device computing, Real-time personalization
","Varun Khare, a distinguished alumnus of IIT Kanpur with a Bachelor's in Technology in Computer Science, has a rich research background that spans across marquee institutions.

He has conducted research at institutions such as the University of California, Berkeley, and at MPI for Brain Research in Germany where he worked on neural symbolic hybrids for program synthesis and built the first petabyte scale 3D segmentation for mapping brain tissues. 

Varun has also lead Federated Learning capabilities at Open-Mined and played a pivotal role in driving the open-source movement for secure and private Machine Learning.  Notably, his achievements there included pioneering the world's first open-source platform for privacy-aware Machine Learning on devices at OpenMined, in a joint venture with Facebook (Meta).. 

Merging his passion for Federated Learning with his enthusiasm for entrepreneurship and research, Varun founded NimbleEdge with the goal of revolutionizing the technological experiences of billions of users. As the Co-founder and CEO of NimbleEdge, he currently leads the organization in providing a platform that enables companies to leverage edge computing to create an on-device ML ecosystem, delivering hyper-personalized mobile app experiences at an incredible scale.
",https://www.youtube.com/watch?v=VX17JNON4nk,"Automotive, Banking & Financial Services, Computer Software, Food & Beverages, E-Commerce, Social media, Gaming
","I have previously shared such information in MLOps community and took workshops for open source community. These nuances around device diversity, personalization capabilities has not been shared to general industry previously. We have been critical in devising UN privacy enhancing techologies' discussion in the past and realized this information needs to propagated to practitioners from the state of the work happening in Academia.
",MLOps & GenAI World 2023,VX17JNON4nk
Kai  Luo,Loblaw Digital,Senior Applied Scientist,Recommender Systems at Loblaw Digital,"Loblaw Companies Ltd is a retailing conglomerate that is the largest grocery retailer in Canada. Its subsidiaries include over 20 supermarket banners, such as Real Canadian Superstore, No Frills, and T&T. E-commerce, particularly for grocery, has become a significant part of the business , and personalization use cases play an important role in that domain. In this talk, Kai will discuss challenges relating to modeling customers’ behaviors and grocery products’ latent representation, and how we iterate our system to solve these challenges.",,,Ignite Lighting Talk,,"Model dev, training, arch.","Recommender Systems, Customer Behavior Modeling, Grocery Product Representation","Kai works as a Senior Applied Scientist at Loblaw Digital, leading development  of the core recommender systems used in  personalization use cases across all lines of business. Prior to that, he completed a master’s degree at the University of Toronto, with a thesis relating to conversational recommendation.",https://www.youtube.com/watch?v=ZpmKD_m7pdQ,,,TMLS 2022,ZpmKD_m7pdQ
Fabiana Clemente,YData,Chief Data Officer,Synthetic Data: Generative AI for Enhanced Data Quality in the Era of Foundational Models,"A thought-provoking session that explores the transformative potential of synthetic data in the age of foundational language models. As Language Models (LMs) like GPT-3 and its successors continue to reshape the landscape of AI, the quality of training data becomes paramount. This session will deep dives into the synergy between synthetic data and foundational models exploring the impact of data quality in foundational models, the role of synthetic data, data augmentation, bias and data privacy. ","This session aims to provide a good understanding into core concepts such as foundational models, generative AI, and the process of synthetic data generation. The objective is to provide a comprehensive perspective of how synthetic data can enhance data quality and fuel the capabilities of Machine Learning models, such as LLMs. ",-,,5.0,Introduction to MLOps and GenAI,"Synthetic data, Foundational models, Generative AI
","Fabiana Clemente is cofounder and CDO of YData, combining data understanding, causality, and privacy as her main fields of work and research, with the mission of making data actionable for organizations. 
As an enthusiastic data practitioner she hosts the podcast When Machine Learning Meets Privacy and is a guest speaker on the Datacast and Privacy Please podcasts. She also speaks at conferences such as ODSC and PyData.",https://www.youtube.com/watch?v=HtiPffkyGrs,Information Technology & Service,,,HtiPffkyGrs
Bala Gopalakrishnan,Pelmorex Corp,Chief Data Officer,AI @ Scale to Understand Impact of Weather on Businesses,"Weather affects every facet of life. It changes our behaviour patterns of what we buy, what we choose to eat. We have one of the best historical weather data sets going back decades for every point in the globe. Correlating it with sales data and able to scale hundreds of thousands of models at product / store level, we are able to predict weather driven demand for short and long term to help decision makers in supply chain, man power planning and marketing ",Packaging ML in an end to end SAAS platform to solve a key industry problem,,Case Study,5.0,Deployment and integration,"Weather Data Analysis, Predictive Modeling, Supply Chain Optimization





","Bala is Chief Data Officer at Pelmorex Corp, the parent company of The Weather Network, MeteoMedia, El-Tiempo, Clima, WetterPlus, Pelmorex Data Solutions  brands.

Pelmorex is a leading player in Digital Advertising and Weather B2B/B2C solutions. The AI powered products of Pelmorex are used by B2B customers worldwide. 

Bala leads the Product, Engineering and technology talent that boasts of Canada's best GIS, AI/ML, Big Data Engineering and UX talents. 

Bala has a Master’s of Science in Computer Engineering from The University of Alabama, and an MBA from Ivey Business School. Prior to joining Pelmorex, Bala lead engineering and software teams at Johnson Controls (Fortune 100) and Eutech Cybernetics which was rated by Gartner as one of the cool companies in smart city solutions.  Bala was ranked in the top 50 global data analytics leaders in a survey by Corinium in 2022.",https://www.youtube.com/watch?v=1L3y-sFfeek,"Banking & Financial Services, Computer Software, Food & Beverages, Marketing & Advertising, Retail
","Solving a problem at scale at training hundreds of thousands of models , Explaining AI driven insights to ordinary business decision makers and C Suite, creating a new platform end to end that encompasses business intelligence 
",TMLS 2023,1L3y-sFfeek
Sophia  Yang,Mistral AI,Head of Developer Relations,Build with Mistral,"In the rapidly evolving landscape of Artificial Intelligence (AI), open source and openness AI have emerged as crucial factors in fostering innovation, transparency, and accountability. Mistral AI's release of the open-weight models has sparked significant adoption and demand, highlighting the importance of open-source and customization in building AI applications. This talk focuses on the Mistral AI model landscape, the benefits of open-source and customization, and the opportunities for building AI applications using Mistral models.",,,business strategy & advanced technical,1.0,Introduction to MLOps and GenAI,"Open-source, Mistral AI, Customization","Sophia Yang is the Head of Developer Relations at Mistral AI, where she leads developer education, developer ecosystem partnerships, and community engagement. She is passionate about the AI community and the open-source community, and she is committed to empower their growth and learning. She holds an M.S. in Computer Science, an M.S. in Statistics, and a Ph.D. in Educational Psychology from The University of Texas at Austin.",https://www.youtube.com/watch?v=_IM53bMowlQ,"Information Technology Services, Automotive, Banking & Financial Services, Computer Software, Enviromental Services, Food & Beverage, Hospital & Health Care, Insurance, Marketing & Advertising, Telecommunications",,MLOps & GenAI World 2024,_IM53bMowlQ
Graham Neubig,Carnegie Mellon University / All Hands AI,Associate Professor / Chief Scientist,The State-of-the-Art in Software Development Agents," One of the most exciting application areas of AI is software development, and gradually we are moving towards more autonomous development where AI agents can perform software development tasks end-to-end. In this talk I will describe the latest work on AI software developers, including research developments, evaluation methods, and challenges that agents currently face. I will also discuss some of the MLOps challenges involved in deploying these agents, including safety and efficiency concerns."," * Some of the latest developments in software-related AI
* Representative software products at different levels of autonomy
* AI modeling challenges involved in making highly accurate AI software engineers
* MLOps challenges involved in deploying these software AI agents",,business strategy & advanced technical,4.0,"Model dev, training, arch.","AI software developers, Autonomous software development, MLOps challenges","Graham Neubig is an associate professor at Carnegie Mellon University, focusing on machine learning methods for natural language processing, code generation, and AI agents. He is also co-founder and chief scientist at All Hands AI, a company building open-source software development agents.",https://www.youtube.com/watch?v=klrhZCxXzB0,"Information Technology Services, Computer Software","This is an important and widely-acknowledged topic, but I believe I can approach it from the perspective of someone who knows both cutting-edge research and the practicalities of making these systems actually work. I will try to discuss things in a way that is both accessible to non-technical audiences, and interesting to technical audiences.",MLOps & GenAI World 2024,klrhZCxXzB0
David Emerson,Vector Institute,Applied Machine Learning Scientist,Prompt Optimization and Parameter Efficient Fine-Tuning,"As the generalizability of large language models (LLMs) rapidly expands, prompting and prompt design have become an increasingly important field of study. For expressive LLMs, well constructed prompts can elicit remarkable performance on a wide variety of downstream tasks. However, there is significant variance in such performance depending on prompt structure, and manual optimization of prompts is often quite challenging. In this talk, we'll discuss several state-of-the-art prompt optimization techniques, including both discrete and continuous approaches. Continuous prompt optimization approaches fall under the more general category of parameter efficient fine-tuning (PEFT) methods. We'll briefly consider two such approaches in the form of Adapters and LoRA, the latter of which produces similar or better performance to full-model fine tuning on many tasks.","1. Motivations for why parameter-efficient fine tuning (PEFT) methods are important for large language models.

2. An in-depth understanding of current state-of-the-art approaches for PEFT.",,Technical,6.0,"Model dev, training, arch.","Large Language Models (LLMs), Prompt Optimization, Parameter Efficient Fine-Tuning (PEFT)","David Emerson is an Applied Scientist at the Vector Institute working on applied research and development in natural language processing (NLP) and federated learning. He obtained his PhD in Applied Mathematics at Tufts University in 2015 working on numerical methods for partial differential equations. Since graduating, he has focused on machine-learning research and worked in various applied research and development positions at companies in Toronto. His areas of interest include NLP, generative modeling, federated learning, and machine learning theory, especially optimization.",https://www.youtube.com/watch?v=VqIloIgqvvs,"Not industry specific
",,TMLS 2023,VqIloIgqvvs
Salina  Wu,Forethought,Senior Machine Learning Infrastructure Engineer,Avoid ML OOps with ML Ops: A modular approach to scaling Forethought’s E2E ML Platform,"As Machine Learning becomes more ubiquitous in business and product applications, the need for a cost-efficient, scalable, and automated infrastructure to support the end-to-end ML lifecyle becomes mission critical. However, a scalable and reusable ML Ops platform is often an afterthought in productionizing ML models, due to urgency of business needs and lack of resources or experience. A very common scenario is for ML Ops to be ad-hoc and de-centralized, with no good way to reproduce or automate ML processes. It can be challenging, especially for smaller teams, to identify and foresee specific ML Ops needs and understand how to address them. 

Forethought is an enterprise company building AI-powered customer experience (CX) solutions. Our products require training customer-specific language models and deploying them on low-latency, high-uptime endpoints. With ML at the heart of our business, our infrastructure supporting it is pivotal to our growth and success. At Forethought, we took a close look at our initial ML infrastructure, aiming to identify key areas of improvement and anticipate future requirements. Through a step-by-step approach, we gradually replaced our existing infrastructure with improved, modular components to arrive at a much more mature system. This case study will dive into which areas we identified as critical to replace as well as the steps we took to enhance them. In particular, we will look at the following: 

- Streamlining ML training and migrating to the Sagemaker training platform
- Achieving efficient model serving with Sagemaker Serverless and Multi-Model Endpoints
- Orchestrating our ML processes with automated pipelines on Dagster
- Centralizing ML feature engineering across our datalake using Spark
- Building intuitive model management tooling with Retool

Through this talk, we’ll show a real-world scenario of bringing a rudimentary v0 ML architecture to an enhanced v1 architecture. We will also share our plans and progress building towards our v2 vision, including automated re-training and LLM support. Key takeaways will include: 

- Understanding the different components of a solid ML infrastructure
- Identifying and proactively addressing bottlenecks and opportunities for growth in your ML lifecycle
- Learning how to improve and migrate your ML infrastructure in stages
- Understanding the goals and best practices of a stable end-to-end ML infrastructure","- Understanding the different components of a solid ML infrastructure
- Identifying and proactively addressing bottlenecks and opportunities for growth in your ML lifecycle
- Learning how to improve and migrate your ML infrastructure in stages
- Understanding the goals and best practices of a stable end-to-end ML infrastructure",-,,5.0,Deployment and integration,"ML infrastructure, Modular approach, Scaling
","Salina Wu is a Sr. Machine Learning Infrastructure engineer at Forethought.ai. She works closely with the Machine Learning team to build and maintain their end-to-end training, serving, and data infrastructures. She is particularly motivated by introducing new ways to improve efficiency and reduce cost across the ML space. When not at work, Salina enjoys surfing, pottery, and being in nature.",https://www.youtube.com/watch?v=iENEd59Rrqs,,,MLOps & GenAI World 2023,iENEd59Rrqs
Simba Khadder,"        
Featureform",Founder & CEO,We’re Doing RAG All Wrong—and How We Can Do So Much Better," Today's standard approach to RAG—embedding a user query, performing a nearest neighbor search on arbitrarily chunked text via a vector database, and fitting it into a prompt—is far from optimal. While it works, this technique misses the opportunity to truly optimize how much relevant information we can fit into the context window. The real challenge is squeezing the most useful data into the prompt, and there’s a wealth of research and techniques available that can significantly improve this process.

In this session, we’ll explore why the current approach is limiting and how we can leverage smarter strategies to optimize context, improve retrieval, and unlock the full potential of RAG.",,,Ignite Lighting Talk,,"Model dev, training, arch.","RAG optimization, Context window, Information retrieval","Simba Khadder is the Founder & CEO of Featureform. After leaving Google, Simba founded his first company, TritonML. His startup grew quickly and Simba and his team built ML infrastructure that handled over 100M monthly active users. He instilled his learnings into Featureform’s virtual feature store. Featureform turns your existing infrastructure into a Feature Store. He’s also an avid surfer, a mixed martial artist, a published astrophysicist for his work on finding Planet 9, and he ran the SF marathon in basketball shoes.",https://www.youtube.com/watch?v=DbMv1qhisgQ,Computer Software,Will get into more cutting edge techniques for RAG,MLOps & GenAI World 2024,DbMv1qhisgQ
Charles Frye,Modal Labs,AI Engineer,"How to Run Your Own LLMs, From Silicon to Service","In this talk, AI Engineer Charles Frye will discuss the stack for running your own LLM inference service. We'll cover: compute options like CPUs, GPUs, TPUs, & LPUs; model options like Qwen & LLaMA; inference server options like TensorRT-LLM, vLLM, & SGLang; and observability options like the OTel stack, LangSmith, W&B Weave, & Braintrust."," Everything about serving LLMs, from the latest and greatest open source software tooling to the fundamental principles that drive engineering constraints across the stack.",,case study & advanced technical,6.0,Deployment and integration,"LLM inference, Deployment stack, Observability tools","Charles teaches people to build data, ML, and AI applications. He got his PhD from the University of California, Berkeley, in 2020 for work on the geometry of neural network optimization. He has since worked as an educator and evangelist for neural network applications at Weights & Biases, Full Stack Deep Learning, and now Modal Labs.",https://www.youtube.com/watch?v=H0CHS0ZxMX0,Computer Software,"I've been in this space for 10 years, initially as a researcher, later as an educator and a consultant and now as an engineer and evangelist, so I have a ton of context to bring.",MLOps & GenAI World 2024,H0CHS0ZxMX0
Varun Raj Kompella,Sony AI,Senior Research Scientist,Outracing Champion Gran Turismo Drivers With Deep Reinforcement Learning,"Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms.",We demonstrate the possibilities and challenges of using deep RL techniques to control complex dynamical systems in domains such as Gran Turismo where agents must respect imprecisely defined human norms.,,Technical / Research,7.0,"Model dev, training, arch.",,"Varun Kompella is currently a senior research scientist at Sony AI. He earned his master’s of science degree in informatics with a specialization in graphics, vision and robotics from Institut Nationale Polytechnique de Grenoble (INRIA Grenoble), and a Ph.D degree from Università della Svizzera Italiana (IDSIA Lugano), Switzerland, working with Prof. Juergen Schmidhuber. In his thesis work he developed algorithms that use the slowness principle for driving exploration in reinforcement learning agents. After completing his Ph.D., he worked as a postdoctoral researcher at the Institute for Neural Computation (INI), Germany. His research contributions led to several patents, publications in peer-reviewed journals and conference proceedings.",https://www.youtube.com/watch?v=ucxCHwSLQSo,,,TMLS 2022,ucxCHwSLQSo
Patrick Marlow,Google,"Staff Engineer, Vertex Applied AI Incubator",Agentic AI: Unlocking Emergent Behavior in LLMs for Adaptive Workflow Automation,"Explore the emergent capabilities of ""agentic"" AI, where agents combine LLMs, reasoning loops, and tools to tackle complex workflows beyond the capabilities of LLMs alone. This session examines techniques for fostering this intelligence, enabling agents to adapt and self-direct their actions for unparalleled workflow automation. Attendees will leave with a deeper understanding of agentic AI and strategies for its impactful implementation.","A few things that attendees can take away from this session are:
- How agentic AI involves goal-oriented behavior, adaptability, and a degree of autonomy within AI systems.
- How AI systems can continuously learn and improve their workflow management capabilities
- How to explore practical techniques for implementing agentic AI within their workflows
- Best practices for system design and architecture of Agentic AI systems",,Applied Case Studies,4.0,Introduction to MLOps and GenAI,"Agentic AI, workflow automation, emergent behavior.






","Patrick is a Staff Engineer on the Vertex Applied AI Incubator team, where he focuses on building tooling and reusable assets to extend Google’s cutting-edge LLM technologies. He specializes in the Conversational AI ecosystem, working with products such as Vertex Agents, Vertex Search and Conversation, and Dialogflow CX. Previously, he was an AI Engineer in Google’s Professional Services Organization. Prior to Google, he was the Principal Data Architect at Levelset, a construction technology company, specializing in NLP Data Pipelines and OCR tooling. Patrick also worked as the Director of Engineering and Data Science for Amelia.ai, a Conversational AI company, delivering chat and voice bots to Fortune 500 clients across the Banking, Hospitality, Entertainment, and Retail verticals.

Patrick studied Electrical Engineering at the University of Texas at Austin.
He is the author and maintainer of several Open Source works including CXLint, an automatic linter for Dialogflow CX, and the Dialogflow CX Scripting API (SCRAPI) which is utilized by developers worldwide to supercharge bot building and analysis in Dialogflow CX.",https://www.youtube.com/watch?v=GwQi33fmexU,"Automotive, Banking & Financial Services, Computer Software, Environmental Services, Food & Beverages, Hospital & Health Care, Information Technology & Service, Insurance, Marketing & Advertising, Telecommunications, Other
",,TMLS 2024,GwQi33fmexU
Ihab Ilyas,Apple / University of Waterloo,"Professor, Cheriton School of Computer Science
Director, Head of Apple Knowledge Platform",Saga: Continuous Construction and Serving of Large Scale Knowledge Graphs,"In this talk I present Saga, an end-to-end platform for incremental and continuous construction of large scale knowledge graphs we built at Apple. Saga demonstrates the complexity of building such platform in industrial settings with strong consistency, latency, and coverage requirements. In the talk, I will discuss challenges around the following: building source adapters for ingesting heterogenous data sources; building entity linking and fusion pipelines for constructing coherent knowledge graphs that adhere to a common controlled vocabulary; updating the knowledge graphs with real-time streams; and finally, exposing the constructed knowledge via a variety of services. Graph services include: low-latency query answering; graph analytics; ML-biased entity disambiguation and semantic annotation; and other graph-embedding services to power multiple downstream applications. Saga is used in production at large scale to power a variety of user-facing knowledge features.",Complexity of building large scale knowledge graphs,,Technical,5.0,"Model dev, training, arch.",,"Ihab Ilyas is a professor in the Cheriton School of Computer Science and the NSERC-Thomson Reuters Research Chair on data quality at the University of Waterloo. He is currently on leave at Apple to lead the Apple Knowledge Platform team. His main research focuses on the areas of Data Science and data management, with special interest in data quality and integration, managing uncertain data, machine learning for data curation, and information extraction. Ihab is a co-founder of Tamr, a startup focusing on large-scale data integration, and the co-founder of inductiv (acquired by Apple), a Waterloo-based startup on using AI for structured data cleaning. He is an ACM Fellow and IEEE Fellow, a recipient of the Ontario Early Researcher Award, a Cheriton Faculty Fellowship, an NSERC Discovery Accelerator Award, and a Google Faculty Award. Ihab was an elected member of the VLDB Endowment board of trustees (2016-2021), elected SIGMOD vice chair (2016-2021), an associate editor of the ACM Transactions of Database Systems (2014-2020), and an associate editor of Foundations of Database Systems. He holds a PhD in computer science from Purdue University, West Lafayette.",https://www.youtube.com/watch?v=nsh8pkVsEAU,,,TMLS 2022,nsh8pkVsEAU
"George Matthew, Prerna Sharma, Mark Weber","Insight Partners, Antler, MIT Media Lab // Tectonic Ventures","Managing Director, General Partner, Fellow / Investor",Panel: The Current Investment Landscape. Opportunities & Challenges in ML/Gen AI,,,,Business Strategy,1.0,Business and stakeholder alignment,"Investment landscape, Opportunities, ML/GenAI","George Mathew is a Managing Director at Insight Partners, where he focuses on venture stage investments in AI, ML, Analytics, and Data companies as they are establishing product/market Fit.

He brings 20+ years of experience developing high-growth technology startups including most recently being CEO of Kespry. Prior to Kespry, George was President & COO of Alteryx where he scaled the company through its IPO (AYX). Previously he held senior leadership positions at SAP and salesforce.com. He has driven company strategy, led product management and development, and built sales and marketing teams.

George holds a Bachelor of Science in Neurobiology from Cornell University and a Masters in Business Administration from Duke University, where he was a Fuqua Scholar.
---

General Partner at Antler",https://www.youtube.com/watch?v=V5k4Mpbd0h4,,,MLOps & GenAI World 2024,V5k4Mpbd0h4
"Rafal Orlowski, Fabio Dutra Sarti",Scotiabank,"Director, Data Science, Sr. AI/ML Product Manager ",Launching Scotiabank's Customer Facing Chatbot for a Large Organization: From Cold Start Problem to Implementation,"As organizations grow in size and complexity they are increasingly leveraging AI to resolve customer inquiries.  The following talk outlines how Scotiabank built an in house chatbot solution from early strategic planning to launching to customers this year. First, the talk will highlight how a team of data scientists used data to prioritize intents and create a repository of training and testing utterances as a foundation for the NLU(Natural Language Understanding). Second, it will also show case the collaboration and engagement models between business , product, engineering, content, design , and accessibility to ensure that the chatbot delivers a dynamic conversational experience. Lastly, the talk will highlight how the data science team is leveraging NLP and ML to diagnose the health of the chatbot and identify new topics/data to train the chatbot on.",Examples of application of ML in healthcare; Impact of ML in healthcare technologies on patients; Potential biases in ML in Healthcare technologies,,Workshop,2.0,Deployment and integration,,"Rafal Orlowski is the Director of Data Science at Scotiabank on the Corporate Functions Analytics and AI/ML Solutions team  supporting various AI and ML initiatives across the bank and it’s subsidiaries. He been a part Scotiabank for over 5 years and has worked on a variety of projects in digital, fraud, AML and mobile banking. He has nearly 10 years of hands on experience in Data Science and holds a Masters from University of Toronto in Economics.

Fabio Dutra Sarti is the Sr. AI/ML Product Manager at Scotiabank on the Corporate Functions Analytics and AI/ML Solutions team supporting the development of the Scotiabank chatbot for the past 1.5 years. Prior to that, he spent 2 years scaling Juliet, Westjet’s virtual assistant, helping hundreds of thousands of customers. Fabio’s experience also includes launching a crypto currency exchange in Brazil and a real estate start-up in Boston. He holds a Master of Advanced Management degree from Yale School of Management",https://www.youtube.com/watch?v=7WR0mEcKn1c,,,TMLS 2022,7WR0mEcKn1c
Amber  Roberts,Arize AI,Machine Learning Engineer,Monitoring Unstructured Models in Production,"From images and video to natural language and audio, unstructured data coupled with machine learning can unlock deeper AI potential and ROI for many organizations and use cases. Embeddings are the core of how deep learning models represent structures and are fundamental to how the next generation of ML models work.

Join this talk to:

Troubleshoot a sentiment classification model in production

Learn about emerging techniques like UMAP to transform unstructured data into embeddings that can be more efficiently processed by ML models

Implement new technologies to monitor and improve models in production",,,Ignite Lighting Talk,,Performance optimization and efficiency,"Unstructured Data, Embeddings, Model Monitoring","Amber Roberts is a community-oriented Machine Learning Engineer at Arize AI, an ML observability company. Amber’s role at Arize looks to help teams across all industries build ML Observability into their productionalized AI environments. Previously, Amber was a product manager of AI at Splunk and the Head of Artificial Intelligence at Insight Data Science. A Carnegie Fellow, Amber has an MS in Astrophysics from the Universidad de Chile. When Amber isn’t expertly teaching ML observability best practices, you can find Amber playing with her two puppies, Rusty and Sully, on Florida's warm beaches. ",https://www.youtube.com/watch?v=q-PwsUfZGoI,,,TMLS 2022,q-PwsUfZGoI
"Meryem Arik, Hannes Hapke ","TitanML, Digits","Co-founder / CEO, Principal Machine Learning Engineer",Creating our own private OpenAI API,"Recent advancements in open-source large language models (LLMs) have positioned them as viable alternatives to proprietary models. However, the journey to deploying these open-source LLMs is fraught with challenges, particularly around infrastructure requirements and optimization strategies.

In this talk, Meryem Arik and Hannes Hapke will provide a detailed roadmap for startups and corporations aiming to deploy open-source LLMs effectively. Leveraging real-world examples, they will illustrate the practical steps and considerations essential for successful implementation. Additionally, they will share invaluable lessons learned from their own deployment experiences, offering attendees actionable insights to navigate the complexities of open-source LLM deployment."," Real world examples, lessons learned from deploying LLMs for 18+ months
Mixed deep technical insights from Meryem and team",,case study & advanced technical,4.0,Deployment and integration,"Open-source LLMs, Deployment, Optimization","Meryem is the Co-founder and CEO of TitanML. She is a prominent advocate of Women in AI and a TedX speaker.

Hannes Hapke is a principal machine learning engineer at Digits, where he develops innovative ways to use machine learning to boost productivity for business owners and accountants. Prior to joining Digits, Hannes solved machine learning infrastructure problems in various industries including healthcare, retail, recruiting, and renewable energies.

Hannes actively contributes to TensorFlow’s TFX Addons project, has co-authored machine learning publications including the book on ""Building Machine Learning Pipeline"" and ""Machine Learning Production Systems"" by O’Reilly Media, and presented state-of-the-art ML work at conferences like ODSC 2022, or O’Reilly’s TensorFlow World.",https://www.youtube.com/watch?v=NQXw8JA9QHM,"Banking & Financial Services, Information Technology Services",The speakers combine their expertise between hosting open source LLMs and deploying real world fintech application.,MLOps & GenAI World 2024,NQXw8JA9QHM
"Joe Reis, Jepson Taylor, Hugo Bowne-Anderson","Ternary Data, LLC, VEOX Inc, Independent Data and AI Scientist","""Recovering Data Scientist"" & Best-selling author (Joe), Former Chief AI Strategist DataRobot & Dataiku (Jepson)",Panel: Achieving Long-Term AI Growth: Real Problem Solving vs. Trend-Based Solutions,,,,Business Strategy,,Future trends,"Long-term growth, AI problem-solving, Trend-based solutions","Joe Reis is a ""recovering data scientist"" who's worked in a variety of data roles (analytics, machine learning, engineering, architecture, etc) since the early 2000s. Joe is the co-author of The Fundamentals of Data Engineering (O'Reilly, 2022).
----
Jepson is a popular speaker in the AI space having been invited to give AI talks to companies like Space X, Red Bull, Goldman Sachs, Amazon, and various branches of the US government. Jepson's applied career has covered semiconductor, quant finance, HR analytics, deep-learning startup, and AI platform companies. Jepson co-founded and sold his deep-learning company Zeff.ai to DataRobot in 2020 and later joined Dataiku as their Chief AI Strategist. Jepson is currently launching a new AI company focused on the next generation of AI called VEOX Inc.
-----
Hugo is an independent data and AI scientist, consultant, writer, educator & podcaster. His interests include promoting data & AI literacy/fluency, helping to spread data skills through organizations and society and lowering the barrier to entry for data science, analysis, and machine learning. Previously, he was Head of Developer Relations at Outerbounds, a company committed to building infrastructure that provides a solid foundation for machine learning applications of all shapes and sizes. He is also the host of the industry podcast Vanishing Gradients. He was previously Head of Marketing and Data Science Evangelism at Coiled, building solutions for scalable data science and machine learning in Python. Before this, he was at DataCamp, a data science training company educating over 4 million learners worldwide through interactive courses on the use of Python, R, SQL, Git, Bash and Spreadsheets in a data science context. While there, he spearheaded the development of over 25 courses in DataCamp’s Python curriculum, impacting over 250,000 learners worldwide through my own courses. He hosted and produced the data science podcast DataFramed, in which he used long-format interviews with working data scientists to delve into what actually happens in the space and what impact it can and does have.",https://www.youtube.com/watch?v=Swof2q5zTyI,,,MLOps & GenAI World 2024,Swof2q5zTyI
Muhammad  Mamdani,Unity Health Toronto - VP: Data Science and Advanced Analytics; Director: Temerty Centre for Artificial Intelligence Research and Education in Medicine of the University of Toronto; Professor - University of Toronto,Unity Health Toronto - VP: Data Science and Advanced Analytics; Director: Temerty Centre for Artificial Intelligence Research and Education in Medicine of the University of Toronto; Professor - University of Toronto,Saving Lives with ML: Applications and Learnings,"Machine learning (ML) has transformed numerous industries but its application in healthcare has been limited. ML applications are expected to permeate healthcare in the near future with a recent explosion in academic and commercial activity. The application of ML in healthcare, however, is complicated by a variety of factors including the significant variability in needs, healthcare settings and patients served in these settings, workflows, and available resources. This talk will present a case study of Unity Health Toronto and its journey in developing and deploying numerous ML solutions into clinical practice, including bridging public and private sector partnerships to spread innovations internationally. The talk will also present a novel Canadian academic centre dedicated to artificial intelligence (AI) in medicine - the Temerty Centre for Artificial Intelligence Research and Education in Medicine (T-CAIREM) at the University of Toronto. ","The successful application of ML in healthcare is multifaceted and highly dependent on end-user engagement. 
Innovative public-private partnerships are needed to spread ML applications globally. 
Multidisciplinary, collaborative efforts will fuel innovations in the development and application of ML in healthcare. ",,Case Study,3.0,Introduction to MLOps and GenAI,,"Dr. Mamdani is Vice President of Data Science and Advanced Analytics at Unity Health Toronto and Director of the University of Toronto Temerty Faculty of Medicine Centre for Artificial Intelligence Research and Education in Medicine (T-CAIREM). Dr. Mamdani’s team bridges advanced analytics including machine learning with clinical and management decision making to improve patient outcomes and hospital efficiency. Dr. Mamdani is also Professor in the Department of Medicine of the Temerty Faculty of Medicine, the Leslie Dan Faculty of Pharmacy, and the Institute of Health Policy, Management and Evaluation of the Dalla Lana Faculty of Public Health. He is also adjunct Senior Scientist at the Institute for Clinical Evaluative Sciences (ICES) and a Faculty Affiliate of the Vector Institute. In 2010, Dr. Mamdani was named among Canada’s Top 40 under 40. He has published over 500 studies in peer-reviewed medical journals. Dr. Mamdani obtained a Doctor of Pharmacy degree (PharmD) from the University of Michigan (Ann Arbor) and completed a fellowship in pharmacoeconomics and outcomes research at the Detroit Medical Center. During his fellowship, Dr. Mamdani obtained a Master of Arts degree in Economics from Wayne State University in Detroit, Michigan with a concentration in econometric theory. He then completed a Master of Public Health degree from Harvard University in 1998 with a concentration in quantitative methods.",https://www.youtube.com/watch?v=vWst8Wh9Er0,,,TMLS 2022,vWst8Wh9Er0
